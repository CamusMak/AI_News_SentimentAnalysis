index,url,output
1,https://www.forbes.com/sites/kenrickcai/2023/06/04/stable-diffusion-emad-mostaque-stability-ai-exaggeration/,"Emad Mostaque is the modern-day Renaissance man who kicked off the AI gold rush. The Oxford master’s degree holder is an award-winning hedge fund manager, a trusted confidant to the United Nations and the tech founder behind Stable Diffusion — the text-to-image generator that broke the internet last summer and, in his words, pressured OpenAI to launch ChatGPT, the bot that mainstreamed AI. Now he’s one of the faces of the generative AI wave and has secured more than $100 million to pursue his vision of building a truly open AI that he dreams will transform Hollywood, democratize education and vanquish PowerPoint. “Hopefully they’ll give me a Nobel Peace Prize for that,” he joked in a January interview with Forbes. At least, that’s the way that he tells the story. In reality, Mostaque has a bachelor’s degree, not a master’s degree from Oxford. The hedge fund’s banner year was followed by one so poor that it shut down months later. The U.N. hasn’t worked with him for years. And while Stable Diffusion was the main reason for his own startup Stability AI’s ascent to prominence, its source code was written by a different group of researchers. “Stability, as far as I know, did not even know about this thing when we created it,” Björn Ommer, the professor who led the research, told Forbes. “They jumped on this wagon only later on.” “What he is good at is taking other people’s work and putting his name on it, or doing stuff that you can’t check if it’s true.” These aren’t the only misleading stories Mostaque, 40, has told to maneuver himself to the forefront of what some are calling the greatest technological sea change since the internet — despite having no formal experience in the field of artificial intelligence. Interviews with 13 current and former employees and more than two dozen investors, collaborators and former colleagues, as well as pitch decks and internal documents, suggest his recent success has been bolstered by exaggeration and dubious claims. After Stable Diffusion went viral last summer, blue-chip venture capital firms Coatue Management and Lightspeed Venture Partners poured in $100 million, giving Mostaque’s London-based startup a $1 billion valuation. By October, Stable Diffusion had 10 million daily users, Mostaque told Bloomberg. In May, the White House named Stability alongside Microsoft and Nvidia as one of the seven “leading AI developers” which would collaborate on a landmark federal AI safety initiative. Mostaque recently dined with Amazon founder Jeff Bezos; reclusive Google cofounder Sergey Brin made a rare public appearance at Stability’s ritzy launch party in San Francisco last October. Mostaque’s vision for open-source AI has mesmerized other longtime technologists. “He’s probably the most visionary person I've ever met,” says Christian Cantrell, who left a two-decade career at Adobe to join Stability in October (he quit six months later and launched his own startup). More premier talent has followed since the cash injection last summer. Among the 140-person staff: a vice president of research and development who was a Nvidia director; another research head who came from Google Brain; and three Ph.D. students from Ommer’s lab. But to build buzz around Stability, Mostaque made an elaborate gambit supported by exaggerated claims and promises, overstating his role in several major AI projects and embellishing a quotidian transaction with the notoriously uncompromising Amazon into a “strategic partnership” with an 80% discount. AI researchers with whom Mostaque worked told Forbes he claimed credit he did not earn or deserve. And when pressed, Stability spokesperson Motez Bishara admitted to Forbes that Stability had no special deal with Amazon. Mostaque’s other mischaracterizations to investors include multiple fundraising decks seen by Forbes that presented the OECD, WHO and World Bank as Stability’s partners at the time — which all three organizations deny. Bishara said the company could not comment on the presentations “without knowing the exact version,” but that they were accompanied by additional data and documentation. Inside the company, wages and payroll taxes have been repeatedly delayed or unpaid, according to eight former employees, and last year the UK tax agency threatened to seize company assets. (“There were several issues that were expeditiously resolved,” Bishara said.) At the same time that workers faced payday uncertainties, Mostaque’s wife Zehra Qureshi, who was head of PR and later assumed a seat on the company’s board of directors, transferred tens of thousands of pounds out of the company’s bank account, per several sources and screenshots of financial transactions viewed by Forbes. Stability spokesperson Bishara said the spouses had been “making loans to and from the business” and that “any amounts owed from or to Mostaque and Qureshi were settled in full before the end of 2022.” In responding to a detailed list of questions, Mostaque shared a statement saying that Stability had not historically prioritized the “systems and processes” underpinning the fast-growing startup. “We recognize our flaws, and we are working to improve and resolve these issues in an effective and compassionate manner,” he wrote. AI experts and prospective investors have been privately expressing doubts about some of Mostaque’s claims for months now. Despite Silicon Valley’s sudden, insatiable appetite for AI startups, a number of venture capitalists told Forbes that the Stability founder has been struggling to raise hundreds of millions more in cash at a roughly $4 billion valuation. Mostaque publicly claimed last October that annualized revenue had surpassed $10 million, but insiders say sales have not improved (Bishara said the October number was “a fair assessment of anticipated revenues at the time,” and declined to comment on current revenue). “So many things don’t add up,” said one VC who rejected Mostaque’s funding overtures. In 2005, Mostaque graduated from Oxford with a bachelor’s degree, not a master’s degree as he’d later claim. (Responding to an inquiry from Forbes, Bishara said Mostaque intended to apply to receive an “Oxford MA,” which the university grants to alumni without any additional graduate-level coursework. He is now expected to obtain that degree in July.) Then he went into finance, joining Swiss fund manager Pictet. “He was very good at spinning a narrative,” said JP Smith, who hired Mostaque at Pictet and brought him over as a consultant at firm Ecstrat. In 2017, Mostaque joined hedge fund Capricorn, where Mostaque told Forbes he’d won an award for restructuring and running the struggling firm. “He was co-chief investment officer, but he didn’t pull the trigger on the investments,” clarified Damon Hoff, Capricorn’s cofounder. Hoff said the two-year run with the $330 million fund ended with its wind down in 2018 due to poor performance. Following a string of abandoned startups (including a crypto project centered on a digitized Quran), Mostaque founded Stability in 2019 as an AI-powered data hub that global agencies would use to make decisions about Covid-19. It launched with a July 2020 virtual event featuring talks by Stanford AI expert Fei-Fei Li and representatives from UNESCO, WHO and the World Bank. But the project failed to get off the ground and was scrapped about a year later. “Lots of people promised a lot and they didn’t come through,” Mostaque told Forbes in January. “One thing you learned from that is if you have a company with a huge press department, you can rebrand history in your interest.” The company’s focus shifted several more times. Early employees said they researched building a network of vending machine refrigerators around London that would be stocked with grab-and-go items, as well as a line of emotional support dog NFTs (Snoop Dogg was interested, employees recollect Mostaque claiming around the office; the rapper could not be reached for comment). When generative AI started exploding, Mostaque saw an opportunity. Through a variety of maneuvers and exaggerations, he would successfully position Stability as one of the leading unicorn AI companies of the moment. To get there, Mostaque began telling investors that Stability was assembling one of the world’s 10 biggest supercomputers. He branded himself to AI researchers as a beneficent ally, magnanimously willing to provide funding and lend use of Stability’s supercomputer to grassroots AI builders fighting the good fight against goliaths like Google and OpenAI. This supercomputer, Mostaque said, was built from thousands of Nvidia’s state-of-the-art GPUs and purchased with a stunning 80% discount from Amazon Web Services. Five fundraising pitch decks from May to August 2022 list AWS as a “strategic partner” or “partner.” “We talked to Amazon and said this will be the big thing,” Mostaque told Forbes from his bustling London headquarters in January. “They cut us an incredibly attractive deal — certain personal guarantees and other things, which I don’t particularly want to go into because she’ll be angry at me,” he explained, nodding to Zehra Qureshi, his wife and Stability’s then-head of PR. Qureshi declined to elaborate. But Bratin Saha, a vice president for the Seattle tech giant’s AI arm, told Forbes in January that Stability is “accessing AWS infrastructure no different than what our other customers do.” Three former Stability employees said that prior to its venture capital injection, Amazon had threatened to revoke the company’s access to some of its GPUs because it had racked up millions in bills that had gone unpaid for months. Asked for clarification, Stability conceded that the “incredibly attractive deal” Mostaque had claimed was actually the standard discount Amazon offers to anybody who makes a long-term commitment to lease computing power. “Any payment issues were managed in an orderly and communicative way with support from AWS,” Bishara said. AWS did not respond to multiple requests for additional comment. Stability’s pitch decks contained other exaggerations: In investor presentations from May and June 2022, Stability described AI image generator Midjourney as a part of its “ecosystem” claiming it had “co-created” the product and “organized” its user community. Midjourney founder David Holz told Forbes Mostaque gave a “very small” financial donation but otherwise had no connection with his organization. Got a tip about a story? Reach out to the authors, Kenrick Cai at kcai@forbes.com or kenrick.cai@protonmail.com, or Iain Martin at iain.martin@forbes.com. In addition, Mostaque directed his team to list groups like UNESCO, OECD, WHO and World Bank as partners in pitch decks, even though they were not involved in the company’s later evolution, according to four former employees. Bishara denied that Mostaque made this directive, but these organizations are indeed listed as “partners” in multiple fundraising decks as recent as August 2022, in which Mostaque also describes himself as the “UN Covid AI lead.” A UNESCO spokesperson said the UN agency had no association with Stability beyond the Covid-19 data initiative, which had ended well before last summer. The other three agencies said they had no record of official partnerships with the company. Asked about the claims in Stability’s pitch decks, Bishara said that all of Stability’s investor decks included investment memos and appendix documentation that contained more context on the Amazon deal and details of “our relationship with partners and more.” But two investors pitched by the company told Forbes they received no such additional information. In June 2022, Mostaque offered to provide Stability’s supercomputer to a group of German academics who had created an open-sourced image generator nicknamed Latent Diffusion. This model had launched seven months prior in collaboration with a New York City-based AI startup called Runway. But it was trained using only a few dozen Nvidia GPUs, according to Björn Ommer, the professor who led the research teams at Ludwig Maximilian University of Munich and Heidelberg University. For the researchers, who were facing shockingly high computing costs to do their work, the proposal seemed to them a no-brainer. The computing boost Stability provided dramatically improved Latent Diffusion’s performance. In August, the new model was launched as Stable Diffusion, a new name that referenced its benefactor. Stability issued a press release and Mostaque positioned himself in the public eye as chief evangelist for what he calls “the most popular open source software ever.” (Linux or Firefox might disagree.) “What he is good at is taking other people’s work and putting his name on it, or doing stuff that you can’t check if it’s true,” one former employee said of Mostaque. In a statement, Bishara said Mostaque is “quick to praise and attribute the work of collaborators” and “categorically denies these spurious claims and characterizations.” Within days of Stable Diffusion’s launch, Stability secured $100 million from leading tech investment firms Coatue and Lightspeed — eight times the amount of money Mostaque set out to raise, he declared in text messages to his earlier investors. Both firms declined requests for comment. “The investment thesis that we had is that we don’t know exactly what all the use cases will be, but we know that this technology is truly transformative and has reached a tipping point in terms of what it can do.” The round valued Stability at $1 billion though the company hadn’t yet generated much revenue. Stability’s fundraising decks at the time characterized Stable Diffusion as “our” model, with no mention of the original researchers. A press release announcing its funding said “Stability AI is the company behind Stable Diffusion” making no reference whatsoever to its creators. Ommer told Forbes he’d hoped to publicize his lab’s work, but his university’s entire press department was on vacation at the time. Bishara said that Stability has made “repeated public statements” crediting Ludwig Maximilian University and Runway on its website and on the Stable Diffusion’s GitHub page. Nevertheless, the original developers feel Mostaque misled the public in key communications. “One thing you learned from that is if you have a company with a huge press department, you can rebrand history in your interest,” Ommer said. In October, Stability claimed Runway had stolen its intellectual property by releasing a new version of Stable Diffusion. Runway cofounder Cristóbal Valenzuela snapped back that a copyright breach wasn’t possible because the tech was open source; Mostaque retracted a takedown request hours later. He later told Forbes that he was worried about the lack of guardrails in Runway’s version — though Stable Diffusion’s collaborators don’t buy the excuse. The incident, Ommer said, “pushed it too far over the edge.” Valenzuela was equally disillusioned. ""New people are coming into this field that we’ve been in for years, and really trying to own narratives that they should not,” he told Forbes in an interview last year (he declined a request for further comment). Both his lab and Runway ceased working with Stability. While Mostaque was touting Stability’s supercomputer and partnerships to investors and researchers, the company was facing a cash crunch. Wages and payroll taxes were repeatedly delayed or unpaid, according to seven current and former employees — in some cases for more than a month. Five of these sources said they personally experienced delayed payments between 2020 and 2023. Four of these people independently told Forbes that representatives of HM Revenue & Customs, the U.K. government tax collection agency, appeared at the company office and threatened to seize assets due to overdue taxes. Bishara said that delayed payments on taxes and employee salaries have been rectified. Eric Hallahan, a former intern, told Forbes he is still waiting for payment on an invoice he sent the company last August for 181 of the 300 hours he worked. Bishara said that the company has no record of missed salary payments “in the regular course of operations” since 2021, but conceded that some may have occurred under “extraneous circumstances”; in Hallahan’s case, he said Stability is looking into the invoice after being alerted to it in April. While staffers said they stressed over being paid last summer, tens of thousands of British pounds moved from Stability’s corporate account to the personal account of Qureshi, Mostaque’s wife, per screenshots of financial transactions obtained by Forbes. Bishara attributed the transactions to Stability’s “owner-managed startup” origins, which he said included the couple making loans to and from the company. “As the company grew and matured, a full reconciliation was done and any amounts owed from or to Mostaque and Qureshi were settled in full before the end of 2022 by the new, experienced finance team,” he told Forbes. Qureshi’s lawyers declined to answer questions but shared a statement in which she said she had provided “emotional and financial support” to her husband’s business since 2021. While Qureshi’s formal role at the company was head of PR, early employees told Forbes she had described herself as Stability’s chief operating officer — a title that also appeared on business cards. (Bishara said Qureshi never held an executive role and the cards were “created by a family friend for design purposes and were never used.”) After the company raised funding in September, Qureshi joined its board of directors. One current and four former employees who declined to be named for fear of retribution said Qureshi regularly scolded employees so harshly that she drove some to tears. Qureshi described her management style as “direct” in a statement shared through her lawyers. “Unfortunately it seems that my views or directions were taken personally by a few individuals, which was not my intention.” “Start to finish,” Mostaque told Forbes, he needed just six days to secure $100 million from leading investment firms Coatue and Lightspeed once Stable Diffusion went viral. Bishara said Qureshi left the company in late January to pursue personal endeavors and that she is no longer on the board. However, an organizational chart from earlier in May listed her as the “Head of Foundation,” at the top of the company hierarchy equal to Mostaque’s position. Qureshi, through counsel, shared a statement: “I recognised that the time had come for us to move in different directions and I stepped down from my role as Head of PR at the start of this year, and have also resigned from the Board. Emad and I have young children who need my focus, and I also intend to pursue other, personal projects, but I will continue to support my husband in his quest to build and grow Stability AI into a global leader in the field.” Venture capitalists historically spend months performing due diligence, a process that involves analyzing the market, vetting the founder and speaking to customers, to check for red flags before investing in a startup. But “start to finish,” Mostaque told Forbes, he needed just six days to secure $100 million from leading investment firms Coatue and Lightspeed once Stable Diffusion went viral. The extent of due diligence the firms performed is unclear given the speed of the investment. “The investment thesis that we had is that we don’t know exactly what all the use cases will be, but we know that this technology is truly transformative and has reached a tipping point in terms of what it can do,” Gaurav Gupta, the Lightspeed partner who led the investment, told Forbes in a January interview. Coatue and Lightspeed declined requests for further comment. Mostaque says Stability is building bespoke AI models for dozens of customers. But he told Forbes that he is only authorized to name two. The first is Eros Investments, an Indian holding company whose media arm was delisted from the New York Stock Exchange and recently settled a lawsuit alleging that it misled investors, though it did not admit wrongdoing. (Eros did not respond to multiple requests for comment.) The second: the African nation Malawi, where, Mostaque said on a recent podcast appearance, Stability is currently “deploying four million tablets to every child.” (Malawi’s government did not return requests for comment.) Less than two months after Stable Diffusion's public launch, Mostaque claimed that Stability's annualized revenue was higher than the “low tens of millions of dollars” that OpenAI was reportedly making at the time. Sources familiar with the matter said Stability’s ARR is now less than $10 million — and that it's far outpaced by the startup's burn rate. Like many AI startups raising vast amounts of cash right now, it will need more money to stay afloat. In January, Mostaque implied that the company was having no issues with fundraising: “We have been offered by many, many entities and we’ve said no,” he told Forbes. But three venture capitalists told Forbes he has been pitching them and other investors on raising a fresh $400 million for several months; they’d all passed. (Bishara declined to comment on revenue, but said the company has “significant” cash reserves remaining.) Stability is also facing a pair of lawsuits which accuse it of violating copyright law to train its technology. It filed a motion to dismiss one from a class action of artists on grounds that the artists failed to identify any specific instances of infringement. In response to the other, from Getty Images, it said Delaware — where the suit was filed — lacked jurisdiction and has moved to change the location to Northern California or dismiss the case outright. Both motions are pending court review. Bishara declined to comment on both suits. In an open letter last September, Democratic representative Anna Eshoo urged action in Washington against the open source nature of Stable Diffusion. The model, she wrote, had been used to generate images of “violently beaten Asian women” and “pornography, some of which portray real people.” Bishara said newer versions of Stable Diffusion filter data for “potentially unsafe content, helping to prevent users from generating harmful images in the first place.” AI research has not come easy for Stability — even on its flagship Stable Diffusion product. The last version of the model published by the original developers (released in October 2022) received three times as many downloads last month on Hugging Face, which hosts the models, as compared to the most popular version published in-house by Stability. And StableLM, its ChatGPT competitor, was released in April to a tiny fraction of Stable Diffusion's fanfare. Mostaque is unfazed. Stability has a seasoned technical leader to spearhead research: himself. He claims to have discovered a bespoke medical treatment for autism years ago by using AI to analyze existing scientific literature and build a knowledge graph of molecular compounds. (Bishara said the research was done privately and declined to elaborate further.) “I’m a good programmer,” Mostaque told Forbes in January. It all dates back to a gap year he said he took before Oxford to be a developer at software company Metaswitch, he continued. “I didn’t know how to program before that, so I taught myself over the summer — quite naturally actually,” he says. By his account, he submitted several pieces of code and made a personal plea to the company: “I want to be a programmer and you should pay me to be a programmer. They said sure.” “I can be quite convincing at times,” he says."
2,https://venturebeat.com/ai/stability-ceo-says-hes-sad-about-report-he-exaggerated-his-resume/,"Join top executives in San Francisco on July 11-12, to hear how leaders are integrating and optimizing AI investments for success. Learn More Emad Mostaque is the founder and CEO of Stability AI, a unicorn startup that exploded into the zeitgeist in October of 2022 with a billion-dollar valuation. It celebrated with what the New York Times called a “coming-out party” at the San Francisco Exploratorium that felt like a “return to pre-pandemic exuberance.” Stability AI’s claim to fame was its viral open-source AI text-to-image generator, Stable Diffusion, which quickly, along with another open-source option, Midjourney, seemed to supplant OpenAI’s DALL-E 2 as the generative image tools of choice. It also attracted the attention of legendary singer-songwriter Peter Gabriel, who recently partnered with Stability AI to launch a global AI animation challenge called Diffuse Together. Over the past year, Mostaque has also become something of a hype man in the world of open source AI, even taking his message to the U.S. Capital. But behind the scenes, Mostaque’s story is unraveling. A recent Forbes investigation revealed that he lied about his background, his achievements, and his partnerships. He is reported to have overstated his credentials, inflated his hedge fund experience, misled investors and customers, and exaggerated an Amazon deal. He also took credit for Stable Diffusion’s success, while downplaying the role of his co-founders and employees. Is the Stability AI party coming to an end? And how did Peter Gabriel get involved in this mess? We caught up with Mostaque after he hosted a live Twitch broadcast with Gabriel to announce the finalists of the Diffuse Together challenge — and after he published a blog post called “On Setting the Record Straight,” in which he detailed, with bullet points, the “countless false accusations and misrepresentations in this Forbes story.” He agreed to answer our questions and clear the air about the controversy. Here’s what he had to say. (Editor’s note: This interview has been edited for length and clarity.) Join us in San Francisco on July 11-12, where top executives will share how they have integrated and optimized AI investments for success and avoided common pitfalls. VentureBeat: How did you feel about the Forbes article? Emad Mostaque: It was really sad. I [had just] spent last Saturday with the new owners [of Forbes] discussing AI and media. I didn’t want to kill the article, I just asked they be accurate. They ignored our factual corrections. VB: Did you feel blindsided by the Forbes piece or did you know how it was being presented? I know you said there were inaccuracies, but did you have an idea of what they were going for? Mostaque: We issued factual corrections they ignored. They had huge numbers of false allegations and we knocked them down one by one. They did a two-hour interview with our head of research. We sent them the exact wording and asked them for details of leaked decks so we could give precise responses. It is not normal to build a 4,000 A100 cluster! We did that before funding! It was the 8th fastest in the world! And what was that about the Master’s degree? I forgot to send the £60 cheque in. And [as far as] Stable Diffusion, we showed them this — the two lead authors (Robin, Andreas) plus Dominik all work at Stability. And they say “the original researchers” as if they were different! It is actually unprofessional. VB: Are you saying that you did all the coursework for the MA but just didn’t send the check to get the diploma? Just wanted to be clear about what you meant. Mostaque: Yes. I don’t technically have a BA or MA because I missed the ceremony and didn’t send the form to get it sent [to me] by mail. VB: And it sounds like they had a fact-checker speak with you to go over everything before publication? Or did they fact-check over email? Mostaque: All via email over a month. They ignored several things in the final email. I am neuroatypical and very direct, [but] over a hundred interviews this is all they could find. They came and did three days with us, [with] full access, too. VB: You have mentioned your work on autism previously related to your family [Mostaque’s son is autistic]. I do think it’s too bad they did not mention that. Mostaque: They [did] and mock it basically: “Mostaque is unfazed. Stability has a seasoned technical leader to spearhead research: himself. He claims to have discovered a bespoke medical treatment for autism years ago by using AI to analyze existing scientific literature and build a knowledge graph of molecular compounds. (Bishara said the research was done privately and declined to elaborate further.)” Remember, David Ha is our head of research who is most respected in generative AI, with whom the reporters did a two-hour interview. He was head of Google Brain in Tokyo. VB: Can you say what kind of feedback you’ve gotten so far on the article? Mostaque: The response has been supportive. Those that don’t know me have their opinions. Those that do usually like me and know I know my stuff. I was an enterprise developer at age 18, at the top UK software company writing assembler and did computer science at Oxford having only got my first computer at seventeen. VB: You said earlier that you are neuroatypical and direct. Are you saying that you think people sometimes misinterpret your straightforwardness as hype or exaggeration? Mostaque: Yes, they do. VB: I’m also still unclear about the AWS thing: “But Bratin Saha, a vice president for the Seattle tech giant’s AI arm, told Forbes in January that Stability is “accessing AWS infrastructure no different than what our other customers do.” Are you saying that this isn’t the case? Mostaque: We have more GPUs than just about anyone in a unique structure, single spline, created before our funding round. The way we accessed at that time was EC2, a reserved dedicated instance — but that scale is typically only available to Meta, Apple, etc. On the Top 500 global list the cluster would have scored around 8th if we submitted it. VB: But just overall I’m wondering if you have any thoughts about why you or Stability AI is being focused on — in the Semafor piece and this one. I mean, Sam Altman is on a world tour…OpenAI has a lot of hype going on too Mostaque: We are not a normal company in the most [focused] area [of tech] in the world. In 18 months we built the only independent multimodal AI company except OpenAI, on 50x less spending. It’s right that we should get focus and scrutiny. [But] it is not right that we should have factual inaccuracies and slanted coverage. I love journalism and want to help journalists, so I am sad. VB: So what now? What’s your next step? Does this change your relationship with the media, does it change your outlook in any way? Mostaque: No, just build and do good stuff. Don’t talk to people who choose to ignore facts and try to do click bait. Move on. I have found financial journalists better than tech journalists in general. VB: How was it working with Peter Gabriel on the AI animation challenges? Mostaque: He’s great. He made a great statement on it. Solid guy. I’m helping him with his new concert tour. The videos are awesome. I’m working with him on a number of AI-related things to be announced. It’s going to be insane over the next year. This is why articles like Forbes are so saddening. VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Discover our Briefings. Want must read news straight to your inbox? © 2023 VentureBeat. All rights reserved."
3,https://news.crunchbase.com/ai-robotics/stability-ai-founder-emad-mostaque-false-claims,"Stability AI’s founder Emad Mostaque made exaggerated statements about both his own background and his generative AI startup, according to a Forbes report. Grow your revenue with all-in-one prospecting solutions powered by the leader in private-company data. The report, published over the weekend, claims Mostaque misled people — including investors — about receiving a master’s degree from Oxford University, as well as a deal with Amazon — which referred to it as a strategic partner — but was nothing more than a standard cloud computing leasing deal. Stability has also described the OECD, WHO and World Bank as partners to investors — something they all deny, per the report. The report also detailed issues related to payroll uncertainty, unpaid invoices, and monetary transactions between Stability and accounts controlled by Mostaque and his wife. A Stability spokesperson said those were actually loans the couple made to and from the company. This isn’t the first time the London-based startup has found itself in the headlines. Last fall, Stability AI locked up a $101 million raise led by Coatue, Lightspeed Venture Partners and O’Shaughnessy Ventures. The company did not release a valuation at the time, but Bloomberg reported the new cash infusion valued the company at around $1 billion. At the time, some AI researchers disputed the startup’s claims that it created the image generator Stable Diffusion — which was an open-source project developed by researchers.  It also was reported at that time the startup was looking to raise an additional $1 billion of capital at a multibillion-dollar valuation. Those funding talks have stalled, per Forbes. Stability is an AI-driven visual art startup. At the time of its fundraising, the company claimed its consumer-facing product DreamStudio had grown to more than a million registered users who have created more than 170 million images. Stability AI Newest Addition To Unicorn Stable Illustration: Dom Guzman Stay up to date with recent funding rounds, acquisitions, and more with the Crunchbase Daily. Find the right companies, identify the right contacts, and connect with decision-makers with an all-in-one prospecting solution. Editorial Partners: Verizon Media Tech About Crunchbase News Crunchbase News Data Methodology Terms of Service Crunchbase for Applications Featured Searches And Lists Create A Profile Sales Prospecting Guide Sales Prospecting Tools © 2023 Crunchbase Inc. All Rights Reserved."
4,https://www.foxnews.com/world/ai-drone-swarm-shows-military-might-questions-who-holds-power,"This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper. Naftali Bennett spoke exclusively with Fox News Digital about the benefits of AI and the need to set parameters for its use now. The new drone swarm test conducted by the U.S. and its allies last week shows some of the wider applications of artificial intelligence (AI) in military settings while also raising some potential issues about how multiple militaries will be able to cooperate. ""Just like coordination is needed to conduct classic, joint and coalition maneuvers and military operations, similar clear definitions of boundaries, tasks, responsibility and authority are needed to control and de-conflict drone swarms,"" retired Brig. Gen. Uri Engelhard, AI and cyber expert, member of the Israel Defense and Security Forum, told Fox News Digital. ""If planned and conducted properly, the deployment of drone swarms should not be more challenging than other military activities."" The joint AUKUS – Australia, United Kingdom and U.S. – military drill last week included the deployment of an AI-controlled swarm of drones that detected and tracked military targets ""in a real-time representative environment,"" according to a British government press release. CONGRESSIONAL LAWMAKERS AREN'T SURE HOW TO ADDRESS AI'S COMING IMPACT ON JOBS The AUKUS alliance handed control of a drone swarm to an AI program to test its observational capabilities. (UK Ministry of Defense handout) The test, conducted in southwest England, included a Challenger 2 tank, Warrior armored vehicle and Viking unmanned ground vehicle. The U.K. Defense Science and Technology Laboratory (DSTL) said the test helped the three allies reduce duplication efforts and ensure interoperability. ""Accelerating technological advances will deliver the operational advantages necessary to defeat current and future threats across the battlespace,"" U.K. Deputy Chief of Defense Staff Lt. Gen. Rob Magowan said in a press release about the trial.  ""We are committed to collaborating with partners to ensure that we achieve this while also promoting the responsible development and deployment of AI,"" he added. The swarm displayed impressive capabilities, and it raised significant questions about control – both the main AI that operates the swarm and the data that the swarm absorbs and disseminates. RESEARCHERS USE AI TO PREDICT CROPS IN AFRICA TO HELP ADDRESS FOOD CRISIS ""Obviously, he who controls the AI holds an advantage over other participants,"" Engelhard said. ""But similar to other complex systems, like the Iron Dome, only if all components operate in harmony can the system succeed."" ""No system is void of errors,"" he noted. ""Using AI will decrease errors. The task of remote controlling a swarm of drones is complex, and in order to minimize errors, a combination of AI and remote control should be employed."" An Israel Defense Forces officer analyzes visual information generated with the help of artificial intelligence. (IDF Spokesperson unit) ""The clearest advantage of AI-controlled drone swarms is that if programmed well in advance, the swarm can execute its tasks exceedingly well without the need for remote controlling,"" Engelhard said. The new trial shows the U.S. making great strides in areas that some countries have already explored, including Israel, but also rivals such as China, according to Matt McInnis, a senior fellow for the Institute for the Study of War's China Program, told Fox News Digital. ROBOTS COULD GO FULL ‘TERMINATOR’ AFTER SCIENTISTS CREATE REALISTIC, SELF-HEALING SKIN ""This is one of the areas that … with all the things that I saw that drone swarms could potentially bring to warfare, this type of very fast response to a changing situation during a conflict and being able to process many, many targets quickly is something that I think all major militaries are trying to get a handle on,"" McInnis said. ""I think using drones for this intelligence surveillance reconnaissance mission that the August test is looking at is, in many ways, trying to ensure that we are keeping up with where China and Israel and others are going and, frankly, to maintain and hopefully to establish and maintain a leading edge in this capability."" A military vehicle carrying a supersonic reconnaissance drone takes part a military parade in Beijing on Oct. 1, 2019. (Greg Baker/AFP via Getty Images) The focus for AI and drone technology thus far remains purely strategic, with little plans evident to give AI control of weapons or combat vehicles otherwise. The public’s reaction to such a choice became clear with the response to a report last week that the Air Force had run a simulation in which the AI drone allegedly tried to kill its operator and a communications tower because it deemed them as obstacles to fulfilling its goals. The Air Force had to clarify they did not actually hand over control of a weapon to an AI and no one actually died or was harmed during the test. Instead, the Air Force insisted that the comments had been taken out of context and ""were meant to be anecdotal."" WILL AI EVER BE SMART ENOUGH TO DECIPHER FEDERAL REGULATIONS? Iran showed video of a Shahed drone in flight on Feb. 24, 2023. (IRINN via AP) McInnis said the Chinese are aiming to do something along similar lines – in effect speeding up the ""targeting cycle,"" from observation, targeting, firing and then repeating the cycle.  ""Process in a war can become very quick, and it is something that I think most states are going to try to find ways to do because this is the key to maintaining a decision superiority in warfare,"" he said. ""It's certainly where the Chinese are trying to go, what they believe they can achieve [with] this type of decision superiority by bringing in artificial intelligence and machine learning into its entire intelligence and targeting process."" CLICK HERE TO GET THE FOX NEWS APP On the issue of who controls the AI and incoming data remains a chief concern that McInnis worries has escaped serious consideration so far, adding that there will remain ""enormous pressure"" to find a common operating platform, especially in organizations like NATO. ""My expectation is that, at least for our core allies certainly to press on all of NATO but maybe certain key players like France or Japan … those are all things that I expect we're going to try to make as interoperable as possible,"" he said. Peter Aitken is a Fox News Digital reporter with a focus on national and global news.  Get all the stories you need-to-know from the most powerful name in news delivered first thing every morning to your inbox You've successfully subscribed to this newsletter! This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper."
5,https://www.washingtonpost.com/technology/2023/06/05/chatgpt-hidden-cost-gpu-compute,"AI chatbots have a problem: They lose money on every chat. The enormous cost of running today’s large language models, which underpin tools like ChatGPT and Bard, is limiting their quality and threatening to throttle the global AI boom they’ve sparked. Their expense, and the limited availability of the computer chips they require, is also constraining which companies can afford to run them and pressuring even the world’s richest companies to turn chatbots into moneymakers sooner than they may be ready to. “The models being deployed right now, as impressive as they seem, are really not the best models available,” said Tom Goldstein, a computer science professor at the University of Maryland. “So as a result, the models you see have a lot of weaknesses” that might be avoidable if cost were no object — such as a propensity to spit out biased results or blatant falsehoods. What happens when ChatGPT lies about real people? The tech giants staking their future on AI rarely discuss the technology’s cost. OpenAI (the maker of ChatGPT), Microsoft and Google all declined to comment. But experts say it’s the most glaring obstacle to Big Tech’s vision of generative AI zipping its way across every industry, slicing head counts and boosting efficiency. The intensive computing AI requires is why OpenAI has held back its powerful new language model, GPT-4, from the free version of ChatGPT, which is still running a weaker GPT-3.5 model. ChatGPT’s underlying data set was last updated in September 2021, making it useless for researching or discussing recent events. And even those who pay $20 per month for GPT-4 can send only 25 messages every three hours because it’s so expensive to run. (It’s also much slower to respond.) Those costs may also be one reason Google has yet to build an AI chatbot into its flagship search engine, which fields billions of queries every day. When Google released its Bard chatbot in March, it opted not to use its largest language model. Dylan Patel, chief analyst at the semiconductor research firm SemiAnalysis, estimated that a single chat with ChatGPT could cost up to 1,000 times as much as a simple Google search. In a recent report on artificial intelligence, the Biden administration pinpointed the computational costs of generative AI as a national concern. The White House wrote that the technology is expected to “dramatically increase computational demands and the associated environmental impacts,” and that there’s an “urgent need” to design more sustainable systems. Even more than other forms of machine learning, generative AI requires dizzying amounts of computational power and specialized computer chips, known as GPUs, that only the wealthiest of companies can afford. The intensifying battle for access to those chips has helped to make their leading providers into tech giants in their own right, holding the keys to what has become the technology industry’s most prized asset. Why Nvidia is suddenly one of the most valuable companies in the world Silicon Valley came to dominate the internet economy in part by offering services like online search, email and social media to the world free, losing money initially but eventually turning hefty profits on personalized advertising. And ads are probably coming to AI chatbots. But analysts say ads alone probably won’t be enough to make cutting-edge AI tools profitable anytime soon. In the meantime, the companies offering AI models for consumer use must balance their desire to win market share with the financial losses they’re racking up. The search for more reliable AI also is likely to drive profits primarily to the chipmakers and cloud computing giants that already control much of the digital space — along with the chipmakers whose hardware they need to run the models. It’s no accident that the companies building the leading AI language models are either among the largest cloud computing providers, as with Google and Microsoft, or have close partnerships with them, as OpenAI does with Microsoft. Companies that buy those firms’ AI tools don’t realize they’re being locked into a heavily subsidized service that costs much more than what they’re currently paying, said Clem Delangue, CEO of Hugging Face, an open-source AI company. OpenAI CEO Sam Altman indirectly acknowledged the problem at a Senate hearing last month, when Sen. Jon Ossoff (D-Ga.) warned that if OpenAI were to try to make ChatGPT addictive in a way that harms kids, Congress “will look very harshly” on it. Altman said Ossoff needn’t worry: “We try to design systems that do not maximize for engagement. In fact, we’re so short on GPUs, the less people use our products, the better.” The expense of AI language models starts with developing and training them, which requires gargantuan amounts of data and software to identify patterns in language. AI companies also typically hire star researchers whose salaries can rival those of pro athletes. That presents an initial barrier to any company hoping to build its own model, though a few well-funded start-ups have succeeded — including Anthropic AI, which OpenAI alumni founded with financial backing from Google. Then, each query to a chatbot like ChatGPT, Microsoft’s Bing or Anthropic’s Claude is routed to data centers, where supercomputers crunch the models and perform numerous high-speed calculations at the same time — first, interpreting the user’s prompt, then working to predict the most plausible response, one “token,” or four-letter sequence, at a time. That sort of computational power requires GPUs, or graphics processing units, that were first made for video games but were found to be the only chips that could handle such heavy computer tasks as large language models. Currently, just one company, Nvidia, sells the best of those, for which it charges tens of thousands of dollars. Nvidia’s valuation recently rocketed to $1 trillion on the anticipated sales. The Taiwan-based company that manufactures many of those chips, TSMC, has likewise soared in value. “GPUs at this point are considerably harder to get than drugs,” Elon Musk, who recently purchased some 10,000 GPUs for his own AI start-up, told a May 23 Wall Street Journal summit. Those computing requirements also help to explain why OpenAI is no longer the nonprofit it was founded to be. Started in 2015 with the stated mission of developing AI “in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return,” by 2019, it had switched to a for-profit model to attract investors, including Microsoft, which pumped in $1 billion and became OpenAI’s exclusive computing provider. (Microsoft has since poured in $10 billion more and integrated OpenAI’s technology with Bing, Windows and other products.) Exactly how much chatbots like ChatGPT cost to run is a moving target, as companies work to make them more efficient. In December, not long after its launch, Altman estimated the cost of ChatGPT at “probably single-digits cents per chat.” That might not sound like much, until you multiply it by upward of 10 million users per day, as analysts have estimated. In February, SemiAnalysis calculated that ChatGPT was costing OpenAI some $700,000 per day in computing costs alone, based on the processing needed to run GPT-3.5, the default model at the time. Multiply those computing costs by the 100 million people per day who use Microsoft’s Bing search engine or the more than 1 billion who reportedly use Google, and one can begin to see why the tech giants are reluctant to make the best AI models available to the public. The new Bing told our reporter it ‘can feel or think things’ “This is not a sustainable equation for the democratization or wide availability of generative AI, the economy or the environment,” said Sid Sheth, founder and CEO of d-Matrix, a start-up working to build more efficient chips for AI. Google said in its February announcement of Bard that it would initially run on a “lightweight” version of the company’s LaMDA language model because it required “significantly less computing power, enabling us to scale to more users.” In other words, even a company as wealthy as Google wasn’t prepared to foot the bill of putting its most powerful AI technology into a free chatbot. Perspective: What Google’s new AI gets right, wrong and weird. The cost-cutting took a toll: Bard stumbled over basic facts in its launch demonstration, shearing $100 billion from the value of Google’s shares. Bing, for its part, went off the rails early on, prompting Microsoft to scale back both its personality and the number of questions users could ask it in a given conversation. Such errors, sometimes called “hallucinations,” have become a major concern with AI language models as both individuals and companies increasingly rely on them. Experts say they’re a function of the models’ basic design: They’re built to generate likely sequences of words, not true statements. Another Google chatbot, called Sparrow, was designed by the company’s DeepMind subsidiary to search the internet and cite its sources, with the goal of reducing falsehoods. But Google has not released that one so far. ChatGPT ‘hallucinates.’ Some researchers worry it isn’t fixable. Meanwhile, each of the major players is racing for ways to make AI language models cheaper. Running a query on OpenAI’s new, lightweight GPT-3.5 Turbo model costs less than one tenth as much as its top-of-the-line GPT-4. Google is making its own AI chips, which it claims are more efficient than Nvidia’s, as are start-ups like d-Matrix. And numerous start-ups are building on open-source language models, such as Meta’s LLaMA, so that they don’t have to pay OpenAI or Google to use theirs — even though those models don’t yet perform as well and may lack guardrails to prevent abuse. The push for smaller, cheaper models marks a sudden reversal for the industry, said Maryland’s Goldstein. “We spent the last four years just trying to make the biggest models we could,” he said. But that was when the goal was to publish research papers, not release AI chatbots to the public. “Now, just within the last few months, there’s been a complete turnaround in the community, and suddenly everyone’s trying to build the smallest model they can to control the costs.” For consumers, that could mean the days of unfettered access to powerful, general-purpose AI models are numbered. Microsoft is already experimenting with building advertisements into its AI-powered Bing results. At the Senate hearing, OpenAI’s Altman wouldn’t rule out doing the same, although he said he prefers a paid subscription model. Both companies say they’re confident the economics will eventually pencil out. Altman told the tech blog Stratechery in February, “There’s so much value here, it’s inconceivable to me that we can’t figure out how to ring the cash register on it.” Yet critics note that generative AI also comes with costs to society. “All this processing has implications for greenhouse gas emissions,” said Bhaskar Chakravorti, dean of global business at Tufts University’s Fletcher School. The computing requires energy that could be used for other purposes — including other computing tasks that are less trendy than AI language models. That “could even slow down the development and application of AI for other, more meaningful uses, such as in health care, drug discovery, cancer detection, etc.,” Chakravorti said. Based on estimates of ChatGPT’s usage and computing needs, data scientist Kasper Groes Albin Ludvigsen estimated that it may have used as much electricity in January as 175,000 people — the equivalent of a midsize city. For now, the tech giants are willing to lose money in a bid to win market share with their AI chatbots, Goldstein said. But if they can’t make them profitable? “Eventually you come to the end of the hype curve, and the only thing your investors are going to look at, at that point, is your bottom line.” Still, Goldstein predicted many people and companies will find generative AI tools hard to resist, even with all their flaws. “Even though it’s expensive,” he said, “it’s still far less expensive than human labor.” Nitasha Tiku contributed to this report."
6,https://www.theguardian.com/environment/2023/jun/06/climate-cardinals-ai-boost-artificial-intelligence,"Google-designed tools help 9,000 young Climate Cardinals volunteers who translate reports into more than 100 languages A network of young volunteers that translates climate information into dozens of languages is being boosted by new artificial intelligence tools designed by Google. Since founding Climate Cardinals three years ago to improve global climate literacy, Sophia Kianni, 21, has built a network of 9,000 young volunteers around the world who translate reports and content into more than 100 languages, including Swahili, Hebrew, Urdu, Mandarin and Hindi. Volunteers have translated 500,000 words since 2020. They work with professional networks including Respond Crisis Translation and Translators Without Borders for editing and proofreading to ensure translations are credible and accurate. By trialling Google Cloud’s new AI-powered Translation Hub platform, Climate Cardinals has translated an additional 800,000 words into more than 40 languages. “It’s crazy. The change in pace was immediate – we’ve created the same volume of output in the first three months of this partnership that we had done in our first two years of operation,” said Kianni, who is studying science, technology and society at Stanford University in the heart of Silicon Valley. She said Climate Cardinals was developing its own online translation portal using the generative AI tool ChatGPT so people could easily translate their own resources. English is the main language of international scientific communication, with 80% of scientific papers written in the language. A 2016 study found that languages were still a “major barrier” to the global transfer of scientific knowledge. Just 18% of the world’s population speaks English as a native or second language. “Education is an empowerment tool,” said Kianni, who first learned about the climate crisis in sixth grade. When she found out that temperatures in the Middle East were rising at more than twice the global average rate, she began verbally translating climate information into Farsi to educate her Iranian relatives who had previously been unaware of the climate emergency. They now had pro-environmental attitudes, she said, and supported environmental defenders in Iran who had been persecuted for standing up against the government. “Those who are being worst impacted by the climate crisis deserve to have access to the resources they need to really make sense of the disasters impacting their communities, so that they can use their voices to create this larger chorus of people calling for action,” said Kianni, who has served as the youngest ever adviser to the UN secretary general’s youth advisory group on climate change. “Africa is on the frontlines of the climate crisis despite barely contributing to it. People who are being disproportionately impacted by the climate crisis tend to be people of colour, and 80% of climate refugees are women, so it is 100% a social justice issue.” “Obviously, we have not solved the problem of climate translation but through our partnership with Google, we’ve created a tangible pipeline for providing an amazing capacity of translations to almost all of our partners,” added Kianni. She said Climate Cardinals’ youth members were still collating, formatting and delivering the automated translations. “The next step is about empowerment and making people understand how they can be part of the solution.”"
7,https://seekingalpha.com/article/4609485-ai-nvidia-is-taking-all-the-money,"Justin Sullivan/Getty Images News Justin Sullivan/Getty Images News The main thing Nvidia Corporation (NASDAQ:NVDA) makes are graphics processing units, or GPUs. But the “graphics” part is a bit of a misnomer. What GPUs do so very well is computationally expensive floating point math. This allows computers to have high-resolution monitors with high frame rates — graphics. It’s the most common use for GPUs. Around 2005, Nvidia realized that while graphics may be the most common thing that requires heavy duty floating point math, it is far from the only thing. So they began a long journey that now puts them at the center of all AI. They developed software that allowed people to do things with Nvidia GPUs besides graphics, beginning with CUDA in 2007. In 2012, they got the initial break they needed. The first pretty good image recognition AI, AlexNet, got built on Nvidia GPUs and software, and blew away rivals at the annual ImageNet competition. From that point on, Nvidia hardware and software became the AI research default for everyone except Alphabet Inc. (GOOG, GOOGL) aka Google. Since then, Nvidia has split their GPU development into two tracks. The ones that go into PCs and crypto mining rigs, and the data center GPUs. The PC GPUs are very expensive, topping out at around $1,600. The data center GPUs are $10,000-$15,000. I have seen them sell as high as $40,000. Nvidia gets something like a 75% gross margin on the data center GPUs, unheard of in hardware. But that’s what happens when you have what is substantially a monopoly in AI hardware and software. The other big thing that has happened since 2012 is that Nvidia GPUs and software have enabled explosive exponential growth in model size. Keep in mind, that is log scale on the Y-axis, so this is massive exponential growth in the shaded “Modern Era” area. (IEEE) Keep in mind, that is log scale on the Y-axis, so this is massive exponential growth in the shaded “Modern Era” area. (IEEE) In the years leading up to 2012, model size grew roughly at the Moore’s Law’s rate of doubling every two years. Then you see what happens after everyone adopts Nvidia GPU machine learning in 2012 — the line shoots up, doubling every 3-4 months. That chart ends before ChatGPT. The largest model there is AlphaGo, which is very good at playing the Chinese board game, Go. In 2021, the largest model was still playing games. Model size is important because the costs to build and run these things in production scale exponentially with model size. GPT-4 is somewhere in the range of 3-6 times as large as its predecessor, GPT-3.5. But OpenAI charges 15-60 times more for the GPT-4 API. What’s more, they are not yet offering the best version of GPT-4. Microsoft Corporation (MSFT) Azure, where all OpenAI is hosted, still doesn’t have enough Nvidia GPUs to pull that off, and what customer could afford it at scale anyway. They are holding back on other services as well for lack of GPUs. Let’s take a quick example. I asked ChatGPT to write a poem about the upcoming Fed meeting. You can read the exchange here. It’s a 3-sentence prompt, followed by a 28-line poem in response. Let’s see what that silly exchange would cost over at the OpenAI API: The last Google revealed Search metrics was 2012 when it was 1.2 trillion searches. I use 3 trillion as a lowball estimate of where that is today. (ChatGPT Plus, token calculator software, OpenAI API pricing) The last Google revealed Search metrics was 2012 when it was 1.2 trillion searches. I use 3 trillion as a lowball estimate of where that is today. (ChatGPT Plus, token calculator software, OpenAI API pricing) The reason for that huge price rise is Nvidia data center GPUs. The third column is the service that they still cannot offer for lack of GPUs. These trends set up when these very large models were still in the research phase. Building them is very expensive, but that pales in comparison to running them in production at scale. All of a sudden, the economics have changed, because we are now out of the research phase for many things. By defaulting to Nvidia hardware and software for a decade, and making model size grow very fast, everyone is backed into a corner now. Nvidia is taking all the money. And I mean from everyone: Microsoft quarterly reports Microsoft quarterly reports Microsoft was gaining operating leverage in their Intelligent Cloud segment for many years with rapid top line growth. That ended when they had to buy a lot of GPUs to put ChatGPT into production. Their cloud operating margin is down 4 quarters in a row now, and the reason is Nvidia’s 75% gross margin on those data center GPUs. Nvidia’s DGX H100 server (Nvidia) Nvidia’s DGX H100 server (Nvidia) When you price out an AI server like the Nvidia DGX H100 above, it’s shocking the extent to which Nvidia is taking all the money. Approximate guesses at what Nvidia pays for their 3rd-party components (Analyst research) Approximate guesses at what Nvidia pays for their 3rd-party components (Analyst research) That's some approximate sales costs that go into Nvidia’s DGX H100, their AI supercomputer building block. It's sort of the gold standard for AI servers now, and about 90% the materials by sales value are Nvidia. And that doesn’t even include their markup on the server. If you were making your own very high powered server, you could save money by skipping that markup, using a cheaper CPU than the most expensive like Nvidia does, or by using less memory and storage. You can save a lot of money eschewing Nvidia’s networking DPU hardware and substituting cheaper hardware from Broadcom Inc. (AVGO) or Mellanox (oh, wait, that’s also Nvidia), though it may cause bottlenecks if you do. But you are still stuck with something close to a $180k Nvidia bill for 8 H100 GPUs, and the 4 NVSwitches that connect them together. Nvidia is taking all the money because they have spent nearly two decades getting ready for this moment in 2023. But it puts a huge target on their backs. I don’t know that I have ever seen anything as ripe for disruption as Nvidia’s AI hardware dominance. But that does not mean it will happen any time soon. Their moat is that they have the only complete suite of hardware and software that has made it very easy for researchers to make them the default choice since 2012. But the default is costing everyone too much now that we have moved these very large models into production. So what is everyone doing about it? So far, three things: This is sort of a loose grouping of a few different kinds of hardware. The category began in 2015 when Google’s AI training needs exceeded the capabilities and supply of Nvidia GPUs of that vintage. That year they debuted the Tensor Processing Unit, or TPU, for internal use. Versions 2, 3 and 4 are available to rent on Google Cloud for up to a 40%-50% savings over cloud GPUs to do the same work. There are a few ways these things are designed, but they mostly do the same thing under the hood — fake that very computationally expensive floating point math with computationally cheap integer math. This leads to lower mathematical precision, but a lot of research shows that outside of scientific applications, most AI doesn’t need the very high level of precision that Nvidia GPUs bring. So it’s a bit of a cheat, but one that seems to work pretty well. Now we see AI accelerators from AMD/Xilinx (AMD), Qualcomm (QCOM), Intel Corporation (INTC), and others. Google Cloud has the TPUs. Along with hosting the Intel accelerator, AWS from Amazon.com, Inc. (AMZN) also has their own. Microsoft is reportedly building one for Azure, with the idea of putting all OpenAI on it, maybe with help from AMD. The cloud providers in this group have to tread lightly. On the one hand, they want to stop handing over margin to Nvidia. On the other hand, for the foreseeable future, they need to buy a lot of Nvidia GPUs, and Nvidia routinely has supply issues. It’s a delicate dance. The big thing holding the hardware back is the Nvidia software moat. We’ll talk about that below. You saw the fantastic growth in model size after 2012, doubling every 3-4 months. For years, that pattern was bigger and bigger. At OpenAI, for example: That was fine while everything was in the research phase, but now that these same very large models are moving into production, the costs are staggering. OpenAI still has not even been able to get the best version of the GPT-4 API into production because Azure doesn’t have enough GPUs for that yet. What I’ve been telling you is not a secret. I think everyone started seeing where this was headed last fall. “Bigger is better” doesn’t make sense in a commercial setting right now. All of a sudden, small is beautiful. This began the day ChatGPT was released. There were many companies, large and small, who had been working on natural language processing. ChatGPT was a slap to the face. It was bigger and better than what they had, and in one moment they all realized that they were way behind. Commence panic. Last year everyone saw Stability AI get a lot of traction with their open source Stable Diffusion image generator, so many decided to open source what they had and see what happened next. Facebook of Meta Platforms, Inc. (META) was one of them, open-sourcing their LLaMA language models that range in size up to 65 billion parameters about a third the size of GPT-3, and 9x-18x smaller than GPT-4. Then researchers at Stanford made a version, Alpaca, that would run on any hardware. And we were off. The pace of open source development can be pretty staggering when the community gets excited about something, and there are a huge number of applications being built on top of Alpaca and other open models. Others are working on making the models work better while keeping them small. Most importantly, these models can run on consumer hardware, PCs and phones, and they are free, as in beer. There is a chance that the dividing lines in foundational models will not be company versus company, but commercial versus open source. Google also noticed. The big news at I/O was that they announced a language model that was smaller than its predecessor, and also much better. This is the first time I can remember models getting smaller, not larger. The smallest of the 4 PaLM 2 models can run on PCs or phones. All of a sudden, small is beautiful. But GPT-4 remains the best language model, also the largest and most expensive to run. That favors Nvidia. But now many have set about the task of making smaller language models work better. Google is doing it in the training process. The open source people are fine tuning what is available to them, primarily the LLaMA/Alpaca models. So the threat to Nvidia here is much reduced GPU compute intensity for the same work, and more of it running on consumer hardware. The non-Nvidia AI software infrastructure is fragmented and has holes in it, and building a system around non-Nvidia hardware can run you into dead ends. Google remains the exception again, as their internal tools have all been built around TPU since 2015. They need to turn those into cloud services, and they have started that. The most important part of all this to Nvidia is not the hardware, which is all pretty impressive stuff and where the money comes from. The software they have been working on for nearly two decades now is their moat, because it is the combination of the two that make them so easy to choose. Then once you build on Nvidia hardware and software, you are sort of stuck with running it in production. For years, non-Nvidia researchers wrote software for their own purposes, and now the landscape is unsuited for a production environment. This is going to be the hardest piece to disrupting Nvidia. The best whack at it so far comes from Chris Lattner and is new startup, Modular. Lattner is sort of a legend in software circles. As a grad student, he wrote LLVM, which became the basis for many software compilers in wide use today. LLVM’s innovation was a modular structure that allowed it to be extended to any programming language and hardware platform. He led the team that built the Swift programming language at Apple Inc. (AAPL), followed by stints at Google, Tesla, Inc. (TSLA), and everyone’s favorite RISC-V company, SiFive. Modular has funding from Google in their A-round. One of the things Modular is working on is an inference engine, which runs models in production, that has a modular design like LLVM. It can be extended to any development framework, cloud or hardware. No matter how a model was built, it can be dropped into the Modular inference engine, and work on any hardware in any cloud. At least, this is what they are promising. This is exactly what is required to drain Nvidia’s moat, and likely Modular’s intention. This will be the hardest part of disrupting Nvidia. It’s Nvidia against the world: their own customers and their customers’ customers. Nvidia can only keep doing what they have been doing and resist complacency. Anyone who watched their recent presentation at Computex Taipei can attest to the fact that they are not in any way standing still. But they also need to be wary of how disruption happens. It’s usually not a direct challenge, like a rival GPU in this case. More often, it’s cheaper and less performant hardware. The classic case is IBM (IBM) and Intel. In the 1970s, IBM’s customers looked at the “microcomputers” being built around hardware from Intel and others and told them they were not interested. So IBM listened to their customers. But Intel silicon was good enough for hobbyists. Intel took the cash flows from that and reinvested it into making their CPUs better. When Visicalc, the first PC spreadsheet software came out, all of a sudden these microcomputers were good enough for business work, and IBM’s customers were interested. IBM became an Intel customer, their first big one. Then this happened: Data by YCharts This is a very roundabout way of saying that in addition to what they are doing, I think Nvidia has to make an AI accelerator to protect that flank eventually, even if it undercuts their margin and growth. If they don’t do it, someone else may do it for them. A wide variety of opinions (Seeking Alpha screenshot) A wide variety of opinions (Seeking Alpha screenshot) This is a topic of much discussion. My year-forward bull-case model with some generous assumptions still has them trading at around 50 years of year-forward earnings. The growth story is real — that’s in the model. But the Nvidia Corporation valuation has gotten out ahead of even a very bullish case. There is an analogy going around that Nvidia in 2023 looks a lot like Cisco Systems, Inc. (CSCO) in 1999-2000. The Cisco bulls were largely correct about their business prospects after the 2001 recession ended, with strong growth until the financial crisis. Let’s look at that period in the stock: Data by YCharts So, in the first place, valuations matter, and we all need to keep that in mind as the decade-plus of very loose financial conditions comes to an end, at least for now. Cisco has never gotten back to that 2000 peak. But I think the analogy falls down on closer analysis. Cisco was the market leader for sure, but they had vigorous competition. As of now, Nvidia has the field to themselves. That can only last for so long. How long? As of last week, it is literally a trillion dollar question. I also can’t help but notice that the Cisco chart looks very much like the Gartner Hype Cycle. Highlighted: generative AI, which is mostly what we are talking about here. (Gartner) Highlighted: generative AI, which is mostly what we are talking about here. (Gartner) This chart comes from July 2022, just before the Cambrian explosion of ChatGPT. Generative AI, highlighted, is near the ""Peak of Inflated Expectations."" Nvidia's price assumes this sort of growth will keep going, and the 5 considerable risks will not happen. I was a longtime Nvidia bull until it became a meme stock in 2020. It now operates under its own logic and rules, rules I’ll admit I don’t fully understand where 50 years of forward earnings make sense. Anyway, I prefer to invest during the “Trough of Disillusionment,” as Gartner calls it in the chart above. I bought Cisco in 2002. For now, I’ll just be watching closely. This is the most interesting conflict in business, and I can’t wait to see where it goes next. The pace of developments is spectacular. At Long View Capital we follow the trends that are forging the future of business and society, and how investors can take advantage of those trends. Long View Capital provides deep dives written in plain English, looking into the most important issues in tech, regulation, and macroeconomics, with targeted portfolios to inform investor decision-making. Risk is a fact of life, but not here. You can try Long View Capital free for two weeks. It’s like Costco free samples, except with deep dives and targeted portfolios instead of frozen pizza. This article was written by Confirmation Bias Is Your Enemy. Tech and macro. Deep analysis of long term sectoral trends, and the opportunities arising from them. I promise not to bore you. Author of Long View Capital, a Marketplace service for long-term investors. Risk Factors: I am also wrong sometimes. Analyst’s Disclosure: I/we have a beneficial long position in the shares of AAPL, MSFT, QCOM either through stock ownership, options, or other derivatives. I wrote this article myself, and it expresses my own opinions. I am not receiving compensation for it (other than from Seeking Alpha). I have no business relationship with any company whose stock is mentioned in this article. Seeking Alpha's Disclosure: Past performance is no guarantee of future results. No recommendation or advice is being given as to whether any investment is suitable for a particular investor. Any views or opinions expressed above may not reflect those of Seeking Alpha as a whole. Seeking Alpha is not a licensed securities dealer, broker or US investment adviser or investment bank. Our analysts are third party authors that include both professional investors and individual investors who may not be licensed or certified by any institute or regulatory body."
8,https://www.foxnews.com/tech/judges-forced-take-ai-rules-into-their-own-hands-as-lawmakers-slow-to-act-experts,"This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper. Center for AI Safety Director Dan Hendrycks explains concerns about how the rapid growth of artificial intelligence could impact society. Judges are likely to take concerns over artificial intelligence into their own hands and create their own rules for the tech in courtrooms, experts say. U.S. District Judge Brantley Starr of the Northern District of Texas may have been a pioneer last week when he required lawyers who appear in his courtroom to certify they did not use artificial intelligence programs, such as ChatGPT, to draft their filings without a human checking for accuracy. ""We're at least putting lawyers on notice, who might not otherwise be on notice, that they can't just trust those databases,"" Starr, a Trump appointed judge, told Reuters. ""They've got to actually verify it themselves through a traditional database."" Experts who spoke to Fox News Digital argued that the judge’s move to institute an AI pledge for lawyers is ""excellent"" and a plan of action that will likely repeat itself amid the tech race to build even more powerful AI platforms. TEXAS JUDGE SAYS NO AI IN COURTROOM UNLESS LAWYERS CERTIFY IT WAS VERIFIED BY HUMAN  AI guidance in courtrooms may be left to individual judges, experts told Fox News Digital. (iStock) ""I think this is an excellent way to ensure that AI is used properly,"" said Christopher Alexander, chief communications officer of Liberty Blockchain. ""The judge is simply using the old adage of ‘trust but verify.'"" ""The reasoning is likely that the risk for error or bias is too great,"" Alexander added. ""Legal research is significantly more complex than just punching numbers into a calculator."" Starr said he crafted the plan to show lawyers that AI can hallucinate and make up cases, with a statement on the court’s website warning that the chatbots don’t swear an oath to uphold the law like lawyers do. AI COST NEARLY 4,000 PEOPLE IN US THEIR JOBS, REPORT SAYS  ""These platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up – even quotes and citations,"" the statement said. ""Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle,"" the notice continued. Judges are likely to take concerns over artificial intelligence into their own hands and create their own rules for the tech in courtrooms, experts say. (Josep Lago/AFP via Getty Images) Phil Siegel, founder of CAPTRS (Center for Advanced Preparedness and Threat Response Simulation), a nonprofit focused on using simulation gaming and artificial intelligence to improve societal disaster preparedness, said the judge was prudent in his AI pledge requirement, adding that AI could take a role in the justice system in the future. ""At this point, this is a sensible position for a judge to take. Large language models are going to hallucinate because humans do also,"" Siegel said. ""It won’t take long, though, for more focused datasets and models to appear that solve this problem,"" he continued. ""In most specific fields like law, but also in architecture, finance, etc.""  He pointed to how in the field of law, a dataset could be created that gathers all case law and civil criminal laws by jurisdiction and is used to train an AI model. AI LIKENED TO GUN DEBATE AS COLLEGE STUDENTS STAND AT TECH CROSSROADS ""These databases can be built with citation markers that follow a certain convention scheme that will make it harder for a human or AI to either hallucinate or incorrectly cite,"" Siegel said. ""It will also need to have a good scheme to ensure that laws are coordinated with their jurisdictions. A citation might be real, but when it is from an irrelevant jurisdiction, it would not be usable in court. At the point that this dataset and trained AI is available, the ruling will become moot."" A Texas judge may have been a pioneer when he required lawyers in his courtroom to certify they did not use AI programs, such as ChatGPT, to draft their filings without a human checking for accuracy. (Getty Images) Aiden Buzzetti, president of the Bull Moose Project, a conservative nonprofit working ""to identify, train, and develop the next generation of America-First leaders,"" said Starr’s requirement is unsurprising due to the lack of legislation and guardrails surrounding AI. ""In the absence of proactive legislation to ensure the quality of AI-created products, it's completely understandable that individuals and institutions will create their own rules regarding the use of AI materials,"" Buzzetti said. ""This trend will probably increase the longer legislators ignore the risks involved in other professions."" OLDER GENERATIONS TRAIL NATION ON AI KNOW-HOW: POLL Starr’s plan comes after a judge in New York threatened to sanction a lawyer over using ChatGPT for a court briefing that cited phony cases. The Texas judge, however, said that incident did not weigh on his decision. Instead, he began crafting his AI rules during a panel on the technology at a conference hosted by the 5th Circuit U.S. Court of Appeals. TEACHERS TAKE AI CONCERNS INTO THEIR OWN HANDS AMID WARNING TECH POSES 'GREATEST THREAT' TO SCHOOLS Suzanne Nossel, CEO of PEN America, said in a statement that removing books from school libraries teaches students that they are dangerous. (iStock) Leaders in other fields have also taken concerns over AI and the lack of regulations around the powerful tech into their own hands, including teachers in the U.K. Eight educators penned a letter to the Times of London last month to warn that though AI could serve as a useful tool to students and teachers, the technology’s risks are considered schools’ ""greatest threat."" The educators are forming their own advisory board to hash out what AI components educators should ignore in their work. CLICK HERE TO GET THE FOX NEWS APP ""As leaders in state and independent schools, we regard AI as the greatest threat but also potentially the greatest benefit to our students, staff and schools,"" the coalition of teachers in the U.K. wrote in a letter to The Times. ""Schools are bewildered by the very fast rate of change in AI and seek secure guidance on the best way forward, but whose advice can we trust?"" Get a daily look at what’s developing in science and technology throughout the world. You've successfully subscribed to this newsletter! This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper."
9,https://www.healthcareitnews.com/news/unc-healths-cio-talks-generative-ai-work-epic-and-microsoft,"Brent Lamm, CIO at UNC Health, based in Morrisville, North Carolina Photo: UNC Health Brent Lamm is CIO at UNC Health, based in Morrisville, North Carolina. He has quite the seat for the artificial intelligence explosion in healthcare – he's sitting right in the middle of it. Lamm and peers at Stanford, the University of Wisconsin, and the University of California at San Diego are part of a pilot project by electronic health records giant Epic, testing Epic's large language model AI capabilities to enhance the provider experience. Lamm also is in the midst of some exciting work with Microsoft in the AI space, leveraging generative AI (the kind behind groundbreaking applications like ChatGPT) on a broad range of use cases to assist clinicians and other team members. Healthcare IT News sat down with Lamm to discuss these projects and his feelings on AI in healthcare as it grows faster than some might like. Q. You are working with Epic, along with Stanford, the University of Wisconsin and UC San Diego, to pilot its large language model AI capabilities to enhance the provider experience. What is the goal of this project? A. Our ultimate goals are to reduce clinician burnout and enhance the patient-provider relationship through the use of this new technology that assists with drafting responses to patient communications. Over the past decade, the implementation and adoption of digital solutions that enable patients to communicate with their care teams more easily between visits have brought tremendous benefits in terms of experience and engagement. However, a side effect has been an increased workload for clinicians, which has contributed to the current industrywide challenges for care team members in maintaining a healthy personal well-being. This situation only worsened with the pandemic, as the use of digital communication by patients has increased. We believe the generative AI capabilities Epic is working to develop can play a huge role in helping address these challenges by reducing the cognitive burden of drafting responses. This is similar to how spreadsheet technologies have helped business users dramatically reduce the burden of performing basic calculations for decades, allowing them to spend their time and energy focused on the core essence of the given problem or opportunity. By offloading the task of crafting a draft communication, the clinician will be able to better focus on the patient and their needs. The positive side effect of this more efficient process should also be an improvement in the patients' experience and their overall relationship with their care team. Being an early adopter of generative AI embedded into our Epic EHR allows our team to help drive this necessary transformation faster, while also building trust from our care teams that our providers helped make the end product more effective. We hope this initial pilot project will be the first of many Epic AI pilots that our team supports. We are excited about the potential benefits and efficiency that AI can offer all our care team members and our administrative Epic users. Q. More specifically, what will you and your team be doing with the LLM AI in-house to make the EHR experience better for providers? And how will you be getting your providers to understand the AI technology? A. Our initial work will expose this new capability to approximately 10 of our physicians to safely test this new functionality in a highly controlled environment. We have assembled a team of physicians from various specialties, including primary care, to be part of this initial effort. Their focus will be to evaluate the usability of the new functionality, particularly the accuracy of the communications drafted by the generative AI. One key metric we will be examining is how effectively this new technology reduces the time it takes for providers to finalize their communications. The amount of editing required will serve as an important measure of success. Our ultimate goal is to assist Epic in enhancing the technology to the point where only minimal edits are necessary. Another significant aspect of this work involves evaluating provider satisfaction regarding the tone and style of the draft messages. It is crucial that the generated content aligns with their expectations and preferred mode of communication with patients. We find this aspect of the work particularly promising from a technological and informatics standpoint. Furthermore, our team is enthusiastic about evaluating and contributing to the development of generative AI with regard to specialty-specific differences and nuances. Our aim is to guarantee that the potential of this technology is accessible and effective for all of our clinicians. Q. What will you be cautioning your providers about LLM AI? There's a lot of concern surrounding AI. A. We are very pleased to see Epic adopting a thoughtful and cautious approach in implementing this technology. One crucial aspect of our work is to properly train and educate the participating providers about this functionality. The primary caution is to ensure a proper and thorough review of the drafted message prior to hitting send. We know the current state of generative AI is far from perfect and can generate text that is inaccurate or may contain bias, which is commonly referred to as a hallucination. That issue will improve over time as the AI learns, but users must validate automatically generated drafts carefully. Our intention is to ensure that everyone clearly understands this technology is merely a tool to aid in drafting the initial message, with the providers retaining full control of the final response. In parallel with this effort, we have launched a formal program with leaders from across our organization to develop and implement UNC Health’s Responsible AI Framework. A committee of multi-disciplinary leaders and clinicians will use this framework to evaluate vendor-developed and homegrown technology solutions that use various forms of AI. The areas of consideration under the framework are fairness, transparency, accountability and trustworthiness. Looking ahead, as AI technology continues to advance, we firmly believe maintaining the perspective of AI as ""augmented intelligence"" is vital and should remain a core principle. Machine learning, large language models and other forms of AI have the potential to revolutionize healthcare for the better, but they should be used as tools to support our greatest asset: our people. Q. On another front, you're working with Microsoft on an AI project. Please elaborate about this project and its goals. A. Yes, we are excited to be working closely with Microsoft to leverage generative AI on a broader range of use cases to assist our clinicians and other team members. Much of this is still very early stage, but we already have a fully functional prototype of an internal general purpose chatbot, similar to the consumer-facing ChatGPT application. We believe that a local, enterprise solution like this has the potential to unlock the productivity and efficiency gains that generative AI chatbots can offer, while maintaining safety, security and privacy for our users. One of the first scenarios we are focusing on in leveraging this capability is to assist our care teams in accessing training and education resources more quickly and easily. Frontline care team members frequently reference these materials as protocols and workflows change, and we believe chatbot-style search can offer a far more efficient method for our users to quickly find the specific information they need to best navigate the technology and shift focus back to their patients. Q. What would you say to your CIO peers at other healthcare provider organizations about working with AI today? A. First and foremost, we must keep the patient and our care team members at the very heart of this work as it advances. The emerging technology has such tremendous potential to help improve health outcomes, reduce clinician burnout, and bend the cost curve across healthcare. However, we must not lose sight of our mission to improve the health and wellness of the patients and communities that we serve. While it may sound paradoxical, I would strongly encourage leaders across the industry to aggressively pursue use of this technology, but do so in a very careful and thoughtful manner. I also believe finding proven partners who share our values related to patient-centered care and protecting data and privacy is critically important. Given the nature of this technology and the significant resources required to make it work, I expect most organizations will need to align with strategic partners. Ensuring alignment of values is just as important, if not more so, than alignment of technology and architecture. I look forward to seeing how our peer colleagues and organizations embrace and drive improved outcomes with AI. If we do this right, the possibilities are truly amazing. Follow Bill's HIT coverage on LinkedIn: Bill Siwicki Email him: bsiwicki@himss.org Healthcare IT News is a HIMSS Media publication. © 2023 Healthcare IT News is a publication of HIMSS Media"
10,https://www.statnews.com/2023/06/05/ai-medical-records-carbon-health/,"Exclusive analysis of biotech, pharma, and the life sciences In-depth analysis of biotech, pharma, and the life sciences from some of the nation's most trusted and well-connected reporters in the industry with STAT+ reporters and leading industry experts in our STAT+ Conversations series hosted by STAT+, plus early-bird access and discounts to industry events around the country get delivered to your inbox to brief you on the most important industry news of the day like our CRISPR Trackr and Drug Pricing Policy Tracker In-depth analysis of biotech, pharma, and the life sciences from some of the nation's most trusted and well-connected reporters in the industry with STAT+ reporters and leading industry experts in our STAT+ Conversations series hosted by STAT+, plus early-bird access and discounts to industry events around the country get delivered to your inbox to brief you on the most important industry news of the day like our CRISPR Trackr and Drug Pricing Policy Tracker from some of the nation’s most trusted and well-connected journalists hosted by STAT+, plus early access and discounts to can’t-miss industry gatherings delivered to your inbox with the latest market-moving news and insights that help you stay up to date with the latest research and developments on the technologies, personalities, power brokers, and political forces driving changes in life science plus early access and discounts to industry gatherings delivered straight to your inbox with the latest industry news that help you stay up to date with industry research and developments for the latest news and insights in the world of life sciences, medicine, biotech, and pharma at exclusive live and virtual events hosted by STAT with subscriber-only newsletters delivered to your inbox daily with our premium data tools for the latest news and insights in the world of life sciences, medicine, biotech, and pharma at exclusive live and virtual events hosted by STAT with subscriber-only newsletters delivered to your inbox daily with our premium data tools By Mohana Ravindranath June 5, 2023 SAN FRANCISCO — Primary care tech startup Carbon Health is using artificial intelligence to listen in on patient appointments and automatically write up near-complete notes within minutes, directly in its own electronic health record software. Carbon — which has raised about $750 million in venture funding, including $100 million earlier this year — has been developing the AI-based health record software over the past few months and began using it on patients last month. About 200 of its 600 clinicians are already using it during in-person and virtual appointments, and Carbon is onboarding about 25 additional clinicians each day, executives told STAT. The company is hopeful that the technology could cut its own costs internally, and also become a product Carbon can sell to other provider groups. The speedy rollout hints at health tech companies’ impatience to squeeze cost and time savings out of large language models and generative AI — even when the technology is evolving in real time. The stakes for doing so, or not doing so, are higher as venture dollars dwindle and competition heats up between new, tech-driven providers desperate to attract new patients. Unlock this article by subscribing to STAT+ and enjoy your first 30 days free! Bay Area Correspondent Mohana Ravindranath is a Bay Area correspondent covering health tech at STAT. This name will appear with your comment There was an error saving your display name. Please check and try again. Reporting from the frontiers of health and medicine You've been selected! Subscribe to STAT+ for less than $2 per day Unlimited access to essential biotech, medicine, and life sciences journalism Subscribe to STAT+ for less than $2 per day Unlimited access to the health care news and insights you need Subscribe to STAT+ for less than $2 per day Unlimited access to essential biotech, medicine, and life sciences journalism Become a STAT+ subscriber today! Unlimited access to essential biotech, medicine, and life sciences journalism"
11,https://www.reuters.com/technology/elon-musk-says-he-learned-china-will-initiate-ai-regulations-2023-06-05/,"WASHINGTON, June 5 (Reuters) - The Chinese government will seek to initiate artificial intelligence regulations in its country, billionaire Elon Musk said on Monday after meeting with officials during his recent trip to China. Musk did not elaborate further and made his remarks in a Twitter Space with Democratic presidential candidate Robert F. Kennedy Jr. on Monday. ""It's worth noting that on my recent trip to China, I went to senior leadership there. I think we had some very productive discussions on artificial intelligence risks, and the need for some oversight and regulation,"" said Musk, owner of Twitter and Tesla Inc (TSLA.O) chief executive. ""And my understanding from those conversations is that China will be initiating AI regulation in China."" Reuters was not immediately able to reach Chinese officials for comment outside of normal business hours. Musk departed Shanghai on Thursday, wrapping up a two-day trip to China in which he met senior Chinese government officials including the highest-ranking vice premier. Musk met with China's foreign, commerce and industry ministers in Beijing. He also met with Chinese Vice Premier Ding Xuexiang on Wednesday, a source familiar with the matter said. China's cyberspace regulator unveiled draft measures in April for managing generative artificial intelligence services, saying it wants firms to submit security assessments to authorities before they launch their offerings to the public. Several governments are considering how to mitigate the dangers of the emerging technology, which has experienced a boom in investment and consumer popularity in recent months after the release of OpenAI's ChatGPT. In April, the Cyberspace Administration of China (CAC) said that China supports AI innovation and application and encourages use of safe and reliable software, tools and data resources, but content generated by generative AI had to be in line with the country's core socialist values. Providers will be responsible for the legitimacy of data used to train generative AI products and measures should be taken to prevent discrimination when designing algorithms and training data, it said. Our Standards: The Thomson Reuters Trust Principles. Kanishka Singh is a breaking news reporter for Reuters in Washington DC, who primarily covers US politics and national affairs in his current role. His past breaking news coverage has spanned across a range of topics like the Black Lives Matter movement; the US elections; the 2021 Capitol riots and their follow up probes; the Brexit deal; US-China trade tensions; the NATO withdrawal from Afghanistan; the COVID-19 pandemic; and a 2019 Supreme Court verdict on a religious dispute site in his native India. Taiwanese chipmaker TSMC said on Tuesday it expects its performance to be better in the second half of the year than the first. Reuters, the news and media division of Thomson Reuters, is the world’s largest multimedia news provider, reaching billions of people worldwide every day. Reuters provides business, financial, national and international news to professionals via desktop terminals, the world's media organizations, industry events and directly to consumers. Build the strongest argument relying on authoritative content, attorney-editor expertise, and industry defining technology. The most comprehensive solution to manage all your complex and ever-expanding tax and compliance needs. The industry leader for online information for tax, accounting and finance professionals. Access unmatched financial data, news and content in a highly-customised workflow experience on desktop, web and mobile. Browse an unrivalled portfolio of real-time and historical market data and insights from worldwide sources and experts. Screen for heightened risk individual and entities globally to help uncover hidden risks in business relationships and human networks. All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2023 Reuters. All rights reserved"
12,https://www.voanews.com/a/is-it-real-or-made-by-ai-europe-wants-a-label-as-it-fights-disinformation-/7123432.html,"The European Union is pushing online platforms like Google and Meta to step up the fight against false information by adding labels to text, photos and other content generated by artificial intelligence, a top official said Monday. EU Commission Vice President Vera Jourova said the ability of a new generation of AI chatbots to create complex content and visuals in seconds raises ""fresh challenges for the fight against disinformation."" Jourova said she asked Google, Meta, Microsoft, TikTok and other tech companies that have signed up to the 27-nation bloc's voluntary agreement on combating disinformation to dedicate efforts to tackling the AI problem. Online platforms that have integrated generative AI into their services, such as Microsoft's Bing search engine and Google's Bard chatbot, should build safeguards to prevent ""malicious actors"" from generating disinformation, Jourova said at a briefing in Brussels. Companies offering services that have the potential to spread AI-generated disinformation should roll out technology to ""recognize such content and clearly label this to users,"" she said. Jourova said EU regulations are aimed at protecting free speech, but when it comes to AI, ""I don't see any right for the machines to have the freedom of speech."" The swift rise of generative AI technology, which has the capability to produce human-like text, images and video, has amazed many and alarmed others with its potential to transform many aspects of daily life. Europe has taken a lead role in the global movement to regulate artificial intelligence with its AI Act, but the legislation still needs final approval and won't take effect for several years. Officials in the EU, which is bringing in a separate set of rules this year to safeguard people from harmful online content, are worried that they need to act faster to keep up with the rapid development of generative artificial intelligence. The voluntary commitments in the disinformation code will soon become legal obligations under the EU's Digital Services Act, which will force the biggest tech companies by the end of August to better police their platforms to protect users from hate speech, disinformation and other harmful material. Jourova said, however, that those companies should start labeling AI-generated content immediately. Most of those digital giants are already signed up to the EU code, which requires companies to measure their work on combating disinformation and issue regular reports on their progress. Twitter dropped out last month in what appeared to be the latest move by Elon Musk to loosen restrictions at the social media company after he bought it last year. The exit drew a stern rebuke, with Jourova calling it a mistake. ""Twitter has chosen the hard way. They chose confrontation,"" she said. ""Make no mistake, by leaving the code, Twitter has attracted a lot of attention and its actions and compliance with EU law will be scrutinized vigorously and urgently."""
13,https://apnews.com/article/artificial-intelligence-disinformation-europe-58a61973645ee0c36dddd1cbea81a42e,"LONDON (AP) — The European Union is pushing online platforms like Google and Meta to step up the fight against false information by adding labels to text, photos and other content generated by artificial intelligence, a top official said Monday. EU Commission Vice President Vera Jourova said the ability of a new generation of AI chatbots to create complex content and visuals in seconds raises “fresh challenges for the fight against disinformation.” She said she asked Google, Meta, Microsoft, TikTok and other tech companies that have signed up to the 27-nation bloc’s voluntary agreement on combating disinformation to work to tackle the AI problem. Online platforms that have integrated generative AI into their services, such as Microsoft’s Bing search engine and Google’s Bard chatbot, should build safeguards to prevent “malicious actors” from generating disinformation, Jourova said at a briefing in Brussels. Companies offering services that have the potential to spread AI-generated disinformation should roll out technology to “recognize such content and clearly label this to users,” she said. Google, Microsoft, Meta and TikTok did not respond immediately to requests for comment. Jourova said EU regulations are aimed at protecting free speech, but when it comes to AI, “I don’t see any right for the machines to have the freedom of speech.” The swift rise of generative AI technology, which has the capability to produce human-like text, images and video, has amazed many and alarmed others with its potential to transform many aspects of daily life. Europe has taken a lead role in the global movement to regulate artificial intelligence with its AI Act, but the legislation still needs final approval and won’t take effect for several years. Officials in the EU, which also is bringing in a separate set of rules this year to safeguard people from harmful online content, are worried that they need to act faster to keep up with the rapid development of generative AI. Recent examples of debunked deepfakes include a realistic picture of Pope Francis in a white puffy jacket and an image of billowing black smoke next to a building accompanied with a claim that it showed an explosion near the Pentagon. Politicians have even enlisted AI to warn about its dangers. Danish Prime Minister Mette Frederiksen used OpenAI’s ChatGPT to craft the opening of a speech to Parliament last week, saying it was written “with such conviction that few of us would believe that it was a robot — and not a human — behind it.” European and U.S. officials said last week that they’re drawing up a voluntary code of conduct for artificial intelligence that could be ready within weeks as a way to bridge the gap before the EU’s AI rules take effect. Similar voluntary commitments in the bloc’s disinformation code will become legal obligations by the end of August under the EU’s Digital Services Act, which will force the biggest tech companies to better police their platforms to protect users from hate speech, disinformation and other harmful material. Jourova said, however, that those companies should start labeling AI-generated content immediately. Most digital giants are already signed up to the EU disinformation code, which requires companies to measure their work on combating false information and issue regular reports on their progress. Twitter dropped out last month in what appeared to be the latest move by Elon Musk to loosen restrictions at the social media company after he bought it last year. The exit drew a stern rebuke, with Jourova calling it a mistake. “Twitter has chosen the hard way. They chose confrontation,” she said. “Make no mistake, by leaving the code, Twitter has attracted a lot of attention, and its actions and compliance with EU law will be scrutinized vigorously and urgently.” Twitter will face a major test later this month when European Commissioner Thierry Breton heads to its San Francisco headquarters with a team to carry out a “stress test,” meant to measure the platform’s ability to comply with the Digital Services Act. Breton, who’s in charge of digital policy, told reporters Monday that he also will visit other Silicon Valley tech companies including OpenAI, chipmaker Nvidia and Meta. AP reporter Jan M. Olsen contributed from Copenhagen, Denmark."
14,https://www.dw.com/en/eu-asks-big-tech-to-label-ai-generated-content/a-65830533,"The European Commission has called on online platforms to detect and label AI-generated content to tackle disinformation. Can the EU keep pace with tech developments? The European Union's executive body has asked tech platforms including Google, Facebook, YouTube and TikTok to detect photos, videos and text generated by artificial intelligence (AI)and clearly label them for users. It's part of the European Commission's bid to crack down on disinformation, which EU officials warn has been thriving since the start of Russia's war in Ukraine.   Now, Brussels fears generative AI technologies are creating even more fertile ground for the spread of fake news and phony information. ""Advanced chatbots like ChatGPT are capable of creating complex, seemingly well-substantiated content and visuals in a manner of seconds,"" European Commission Vice President Vera Jourova told reporters on Monday.   ""Image generators can create authentic-looking pictures of events that never occurred,"" Jourova said. ""Voice generation software can imitate the voice of a person based on a sample of a few seconds.""  Warning of widespread Russian disinformation in Central and Eastern Europe, Jourova said machines did not have ""any right"" to freedom of speech. She has tasked the 44 signatories of the European Union's code of practice against disinformation with helping users better identify AI-generated content.   ""The labeling should be done now — immediately,"" she said. There is no obligation for tech firms to comply with Brussels' latest request and no sanctions if they do not — because the code of practice is purely voluntary. Andrea Renda, a senior research fellow on digital economy with the Center for European Policy Studies, thinks there may also be technical barriers.  ""Nothing guarantees that they will be able to detect in real time that something is generated by AI,"" he told DW. Renda thinks most firms will work on a ""best-effort basis,"" but said the result would likely be ""far from 100%.""  But Jourova said she was reassured by Google CEO Sundar Pichai. ""I asked him: Did you develop fast enough technology to detect the AI production and label it so that the people can see that they do not read the text produced by real people? And his answer was: Yes, but we are developing, we are improving the technologies further,"" she said.  There is one glaring gap on the guest list of Brussels' anti-disinformation club: Twitter. In May, the platform owned by billionaire Elon Musk withdrew from the EU code of practice. Jourova was not impressed: By opting out, she said, Twitter ""chose confrontation.""  ""Twitter has attracted a lot of attention, and its actions and compliance with EU law will be scrutinized vigorously and urgently,"" Jourova told reporters on Monday. To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video In August, major content moderation obligations on large online platforms including Twitter will kick in under new EU legislation. The regulations, dubbed the Digital Services Act, will force companies to be more transparent about their algorithms, beef up processes to block the spread of harmful posts and ban targeted advertising based on sensitive data such as religion or sexual orientation.  Renda said the new rule book was ""groundbreaking."" Companies would face fines of up to 6% of global annual turnover if found to be in violation of the new legislation, and can even be banned from operating in the European Union.   That means that, though Twitter can dodge Brussels' latest request to immediately flag AI-generated images or videos, the platform will be forced to fall in line with broader EU rules later this year. Brussels is also brewing up other laws to regulate artificial intelligence, known as the AI Act. Under the plans, some uses of AI would be banned outright, such as ""social scoring"" and most facial recognition in public spaces. The proposals also foresee restricting AI in areas deemed ""high risk,"" including recruitment — where AI could lead to discrimination — or in public transport.   But those rules are still making their way through the lengthy EU legislative process and likely won't kick in for at least two years. Not to mention the fact they were drafted before the latest boom in generative AI. Now, Brussels is racing to catch up.  It's chasing several stopgap measures, including a new voluntary generative AI code of conduct and an ""AI pact,"" under which companies could opt in to respect future rules before they fully apply. To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video Catelijne Muller has been advising the European Union on AI legislation since 2017 and co-founded the research and policy group ""ALLAI."" She told DW that, despite what looks like a scramble to keep pace, Brussels is well-placed to regulate.   ""The latest developments that we hear so much about — ChatGPT and all the generative AI models and foundation models — these fall right in the middle of the legislative process for the AI Act. And they fall at a moment that legislators can still take them into account,"" she said.   Muller thinks the challenges thrown up by the latest AI tools are ""not inherently new.""  ""You still see the same problems with bias and discrimination, you still see the same problems with human oversight and agency, and with fundamental rights impacts,"" she said. Muller thinks lawmakers need not ""throw overboard what they had and write something completely new"" — instead, they can work to incorporate developments into existing proposals.  Edited by: Milan Gagnon"
15,https://www.cnbc.com/2023/06/05/apple-practical-approach-to-ai-no-bragging-just-features.html,"Help for Low Credit Scores All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best Credit Unions Best Mortgages for Small Down Payment Best Mortgages for No Down Payment Best Mortgages with No Origination Fee Best Mortgages for Average Credit Score Adjustable Rate Mortgages Affording a Mortgage Best Life Insurance Best Homeowners Insurance Best Renters Insurance Best Car Insurance All Credit Monitoring Best Credit Monitoring Services Best Identity Theft Protection How to Boost Your Credit Score Credit Repair Services All Personal Finance Best Budgeting Apps Best Expense Tracker Apps Best Money Transfer Apps Best Resale Apps and Sites Buy Now Pay Later (BNPL) Apps Best Debt Relief All Small Business Best Small Business Savings Accounts Best Small Business Checking Accounts Best Credit Cards for Small Business Best Small Business Loans Best Tax Software for Small Business Best Tax Software Best Tax Software for Small Businesses All Help for Low Credit Scores Best Credit Cards for Bad Credit Best Personal Loans for Bad Credit Best Debt Consolidation Loans for Bad Credit Personal Loans if You Don't Have Credit Best Credit Cards for Building Credit Personal Loans for 580 Credit Score or Lower Personal Loans for 670 Credit Score or Lower Best Mortgages for Bad Credit Best Hardship Loans How to Boost Your Credit Score Best IRA Accounts Best Roth IRA Accounts Best Investing Apps Best Free Stock Trading Platforms In this article On Monday during Apple's annual developer's conference, WWDC, the company subtly touted just how much work it's doing in state-of-the-art artificial intelligence and machine learning. As Microsoft, Google, and startups like OpenAI embraced cutting-edge machine learning technologies like chatbots and generative AI, Apple appeared to be sitting on the sidelines. But on Monday, Apple announced several significant AI features, including an improved iPhone autocorrect based on a machine learning program using a transformer language model, which is the same technology underpinning ChatGPT. It will even learn from how the user texts and types to get better, Apple said. ""In those moments where you just want to type a ducking word, well, the keyboard will learn it, too,"" said Craig Federighi, Apple's chief of software, joking about autocorrect's tendency to use the nonsensical word ""ducking"" to replace a common expletive. The biggest news on Monday was its fancy new augmented reality headset, Vision Pro, but Apple nonetheless showed how it's working on and paying attention to developments in state-of-the-art machine learning and artificial intelligence. OpenAI's ChatGPT may have hit over 100 million users in two months when it launched last year, but now Apple is taking the technology to improve a feature 1 billion iPhone owners use every day. Unlike its rivals, who are building bigger models with server farms, supercomputers, and terabytes of data, Apple wants AI models on its devices. The new autocorrect feature is particularly impressive because it's running on the iPhone, while models like ChatGPT require hundreds of expensive GPUs working in tandem. On-device AI bypasses a lot of the data privacy issues that cloud-based AI faces. When the model can be run on a phone, then Apple needs to collect less data in order to run it. It also ties in closely with Apple's control of its hardware stack, down to its own silicon chips. Apple packs new AI circuits and GPUs into its chips every year, and its control of the overall architecture allows it to adapt to changes and new techniques. Apple doesn't like to talk about ""artificial intelligence"" — it prefers the more academic phrase ""machine learning"" or simply talks about the feature the technology enables. Some of the other leading AI firms have leaders from academic backgrounds. That has led to an emphasis on showing your work, explaining how it might improve in the future, and documenting it so other people can study and build on it. Apple is a product company, and has been intensively secretive for decades. Instead of talking about the specific AI model, or the training data, or how it might improve in the future, Apple simply mentions the feature and says that there is cool technology working behind the scenes. One example of that on Monday was an improvement to AirPods Pro that automatically turns off noise cancelling when the user engages in conversation. Apple didn't frame it as a machine learning feature, but it's a difficult problem to solve, and the solution is based on AI models. In one of the most audacious features announced on Monday, Apple's new Digital Persona feature makes a 3D scan of the user's face and body and then can recreate what they look like virtually while videoconferencing with other people while wearing the Vision Pro headset. Apple also mentioned several other new features that used the company's skill in neural networks, such as the ability to identify fields to fill out in a PDF. One of the biggest cheers of the afternoon in Cupertino was for a machine learning feature that enables the iPhone to identify your pet — versus other cats or dogs — and put all the user's pet photos in a folder. Got a confidential news tip? We want to hear from you. Sign up for free newsletters and get more CNBC delivered to your inbox Get this delivered to your inbox, and more info about our products and services.  © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Data also provided by"
16,https://www.wired.com/story/apple-ghosts-the-generative-ai-revolution,"To revist this article, visit My Profile, then View saved stories. To revist this article, visit My Profile, then View saved stories. After years of anticipation and contributions from thousands of people, Apple Vision Pro made its debut yesterday, promising immersion in apps, games, movies, and the workplace. With more than 20 cameras, sensors, and microphones, two processing chips, and even an external battery you carry in your pocket, it’s packed full of world-class tech, but missing an element that seems to be everywhere else right now: Generative AI. Since the launch of OpenAI’s ChatGPT last fall, generative AI that creates text and imagery from simple prompts triggered calls for regulation and fear of an existential threat to humanity, and continues to play a role in ongoing Hollywood writers union strikes. It’s also led Big Tech companies to speed up AI deployments, but not at Apple. Yesterday Apple announced new features powered by its neural engine hardware—like call screening that transcribes the first few words of a voicemail live so you can decide whether to pick up a call—but there was no mention of generative AI during the two-hour Worldwide Developer Conference keynote address. The only thing that came close was an update to a feature in iOS 17 that suggests the next word you might want to use when typing on an iPhone keyboard.  Speculation about Apple’s future with generative models preceded WWDC, particularly in recent weeks after Apple posted a string of generative AI-related job ads, and a series of announcements by some of Apple’s biggest competitors. The lack of generative AI news at WWDC comes about a month after Google added conversational AI Bard for search and Workspace products, including an integration with image generator Adobe Firefly, and two weeks after Microsoft extended OpenAI’s ChatGPT and GPT-4 to Bing and Azure cloud offerings respectively. Matt Turck is an investor at FirstMark Capital, a firm that invests in a number of startups using generative AI in their products. He calls the lack of generative AI talk a savvy marketing choice by Apple. Several AI-powered features got announced, but the lack of generative AI mention gave Vision Pro more of the limelight. “The reality is that Apple is a bit behind others like Microsoft and Google in generative AI, so it smartly chose to position itself as running its own race in AI, as opposed to trying to play catch up with others,” he said in a direct message. Apple could use generative AI in a number of impactful ways, like giving the Siri voice assistant a more conversational feel, or helping Apple Pages compete with auto suggestions from Google Docs and Microsoft Word. AI that generates images can also hasten creation of the metaverse.  Apple appears focused on delivering high-end hardware instead of entering the highly convoluted and specialized generative AI arms race, says Micaela Mantegna, who last year lent her experience in games and AI ethics to red-teaming OpenAI’s text to image model DALL-E 2. She said in an email that she thinks that’s why Apple refers to its Vision Pro headset as a spatial computing device, and why CEO Tim Cook says Apple avoids the word metaverse, a term being tainted by Meta’s underperformance. She believes generative models in immersed environments like the metaverse make it possible for a company to learn what you like, what you love. So when you see that an interaction with a digital twin elicits a positive reaction in that environment, someday generative AI can alter what you see next and personalize the experience for the sake of monetization. Last month, Meta started testing generative AI for advertising. However, Mantegna says, Apple has built a reputation for prioritizing user privacy and security. In the WWDC presentation, Apple VP Mike Rockwell said the apps and websites that run in Vision Pro won’t have access to the eye-tracking data the device uses as a pointing mechanism for VR apps. Privacy advocates warn that overwhelming amounts of personal data collected by AR and VR headsets can power invasive experiences, learning your preferences like cookies do on the internet to sell ads or services. One single factor that may weigh heavier than any other is the poor reputation of the major sellers of augmented reality headsets. A December 2021 survey of people in the US found that people trust Facebook less than any other major tech company. It’s surprising Apple made little to no mention of generative AI this week, particularly considering how quickly Microsoft added ChatGPT to its products, says Jeremiah Ratican, an assistant professor at Lindenwood University and lead author of a proposed method for combining game engines like Unreal with image generating AI to create metaverse environments. Since Siri was the first widely accepted AI assistant, it’s strange Apple hasn’t emphasized this tech more. “It seems like perhaps Apple was caught off guard by the rapid adoption of ChatGPT and chose to focus on its investment in AR technology,” Ratican told WIRED in an email. “That could prove to be a smart move if their new headset catches on like their previous devices.” 📨 Understand AI advances with our Fast Forward newsletter 🎧 Our new podcast wants you to Have a Nice Future It’s the Age of Ozempic. Do we still need Weight Watchers? Temu is losing millions of dollars to send you cheap socks Sex workers are still hot for Twitter Spaces The security hole at the heart of ChatGPT and Bing Remembering GitHub’s office, a monument to tech culture 📷 Snap into spring with the Gear team’s picks for the best camera bags, fun instant cameras, and mirrorless cameras Omar L. Gallaga More From WIRED © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
17,https://arstechnica.com/information-technology/2023/06/at-apples-wwdc-keynote-ai-never-came-up-by-name-but-it-was-there/,"Front page layout Benj Edwards - Jun 5, 2023 10:09 pm UTC Amid notable new products like the Apple Silicon Mac Pro and the Apple Vision Pro revealed at Monday's WWDC 2023 keynote event, Apple presenters never once mentioned the term ""AI,"" a notable omission given that its competitors like Microsoft and Google have been heavily focusing on generative AI at the moment. Still, AI was a part of Apple's presentation, just by other names. While ""AI"" is a very ambiguous term these days, surrounded by both astounding advancements and extreme hype, Apple chose to avoid that association and instead focused on terms like ""machine learning"" and ""ML."" For example, during the iOS 17 demo, SVP of Software Engineering Craig Federighi talked about improvements to autocorrect and dictation: Autocorrect is powered by on-device machine learning, and over the years, we've continued to advance these models. The keyboard now leverages a transformer language model, which is state of the art for word prediction, making autocorrect more accurate than ever. And with the power of Apple Silicon, iPhone can run this model every time you tap a key. Notably, Apple mentioned the AI term ""transformer"" in an Apple keynote. The company specifically talked about a ""transformer language model,"" which means its AI model uses the transformer architecture that has been powering many recent generative AI innovations, such as the DALL-E image generator and the ChatGPT chatbot. A transformer model (a concept first introduced in 2017) is a type of neural network architecture used in natural language processing (NLP) that employs a self-attention mechanism, allowing it to prioritize different words or elements in a sequence. Its ability to process inputs in parallel has led to significant efficiency improvements and powered breakthroughs in NLP tasks such as translation, summarization, and question-answering. Apparently, Apple's new transformer model in iOS 17 allows sentence-level autocorrections that can finish either a word or an entire sentence when you press the space bar. It learns from your writing style as well, which guides its suggestions. All this on-device AI processing is fairly easy for Apple because of a special portion of Apple Silicon chips (and earlier Apple chips, starting with the A11 in 2017) called the Neural Engine, which is designed to accelerate machine learning applications. Apple also said that dictation ""gets a new transformer-based speech recognition model that leverages the Neural Engine to make dictation even more accurate."" During the keynote, Apple also mentioned ""machine learning"" several other times: while describing a new iPad lock screen feature (""When you select a Live Photo, we use an advanced machine learning model to synthesize additional frames""); iPadOS PDF features (""Thanks to new machine learning models, iPadOS can identify the fields in a PDF so you can use AutoFill to quickly fill them out with information like names, addresses, and emails from your contacts.""); an AirPods Adaptive Audio feature (""With Personalized Volume, we use machine learning to understand your listening preferences over time""); and an Apple Watch widget feature called Smart Stack (""Smart Stack uses machine learning to show you relevant information right when you need it""). Apple also debuted a new app called Journal that allows personal text and image journaling (kind of like an interactive diary), locked and encrypted on your iPhone. Apple said that AI plays a part, but it didn't use the term ""AI."" ""Using on-device machine learning, your iPhone can create personalized suggestions of moments to inspire your writing,"" Apple said. ""Suggestions will be intelligently curated from information on your iPhone, like your photos, location, music, workouts, and more. And you control what to include when you enable Suggestions and which ones to save to your Journal."" Finally, during the demo for the new Apple Vision Pro, the company revealed that the moving image of a user's eyes on the front of the goggles comes from a special 3D avatar created by scanning your face—and you guessed it, machine learning. Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox. Sign me up → CNMN Collection WIRED Media Group © 2023 Condé Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Ars Technica Addendum (effective 8/21/2018). Ars may earn compensation on sales from links on this site. Read our affiliate link policy.Your California Privacy Rights | Do Not Sell My Personal Information The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.Ad Choices"
18,https://www.reuters.com/technology/googles-ai-power-virtual-travel-agent-priceline-2023-06-06,"June 6 (Reuters) - Want a New York hotel near a Christmas market, a vegan restaurant, or another attraction? Look no further than artificial intelligence from Google (GOOGL.O) at Priceline as early as this summer, the companies told Reuters. The online travel agency, part of Booking Holdings (BKNG.O), aims to debut a more sophisticated chatbot for planning trips, as well as hotel suggestions that are like ""a personal concierge"" tailored to users, said Martin Brodbeck, Priceline's chief technology officer. ""You can easily find out that in Bryant Park there's a Christmas market that runs from early November all the way through the beginning of January when you're actually booking your hotel,"" he said. New tools from Google's cloud division give Priceline access to generative AI, like the technology behind ChatGPT that can draft text as if a human wrote it. The tools also extract information such as hotel prices from existing data to ensure accuracy. For Priceline, the embrace of novel technology may give it an edge over myriad sites that market travel options, Google among them. Google's capabilities, not competition, led to the partnership, Brodbeck said. For Google, drawing business through AI represents a potential way to close the gap with rivals Amazon (AMZN.O) and Microsoft (MSFT.O), against which it has long been a distant No.3 provider of cloud services like data storage. The ability to build applications atop technology that Google pioneered has attracted customers new and old, said Thomas Kurian, Google Cloud's CEO. ""There is a kind of a Cambrian moment happening now where there's an explosion of this technology,"" Kurian told Reuters, referring to the extraordinary prehistoric period when a wide array of new species emerged. He declined to answer how free corporate previews were impacting Google Cloud's profitability, but he said the company focused on cost efficiency and that with great products the ""business will take care of itself."" Among other uses, Google's AI will be generating coding suggestions for hundreds of software developers at Priceline, said Brodbeck. Priceline will adopt Google's search capabilities for employee intranets. And Google's AI will speed up marketing for trending destinations. ""You could have it create images like a beautiful beach, and you could marry that with great generative-AI copy,"" Brodbeck said. Our Standards: The Thomson Reuters Trust Principles. Jeffrey Dastin is a correspondent for Reuters based in San Francisco, where he reports on the technology industry and artificial intelligence. He joined Reuters in 2014, originally writing about airlines and travel from the New York bureau. Dastin graduated from Yale University with a degree in history. He was part of a team that examined lobbying by Amazon.com around the world, for which he won a SOPA Award in 2022. Taiwanese chipmaker TSMC said on Tuesday it expects its performance to be better in the second half of the year than the first. Reuters, the news and media division of Thomson Reuters, is the world’s largest multimedia news provider, reaching billions of people worldwide every day. Reuters provides business, financial, national and international news to professionals via desktop terminals, the world's media organizations, industry events and directly to consumers. Build the strongest argument relying on authoritative content, attorney-editor expertise, and industry defining technology. The most comprehensive solution to manage all your complex and ever-expanding tax and compliance needs. The industry leader for online information for tax, accounting and finance professionals. Access unmatched financial data, news and content in a highly-customised workflow experience on desktop, web and mobile. Browse an unrivalled portfolio of real-time and historical market data and insights from worldwide sources and experts. Screen for heightened risk individual and entities globally to help uncover hidden risks in business relationships and human networks. All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2023 Reuters. All rights reserved"
19,https://www.wired.com/story/fake-drake-will-ai-music-suck,"To revist this article, visit My Profile, then View saved stories. To revist this article, visit My Profile, then View saved stories. Michael Sayman has worked at Facebook, Google, Roblox, and Twitter. At 26, the software engineer has already published a memoir, App Kid. But until he began work on his latest project, he’d never built a website. “I made it in five hours over the weekend, out of frustration that there wasn’t anything like this,” he says. “Now there’s nearly a million streams on the site.” Sayman’s site is AI Hits. Ever since it launched in April, it’s been aggregating a controversial new musical … thing: songs created with artificial intelligence tools that mimic, with chilling accuracy, mainstream stars like Drake and Kanye West. The conversation around AI music has largely been frenzied and shot through with hand-wringing over What It All Means and What It All Portends. But Sayman is an AI optimist. So he built the Hot 100 for AI tunes. AI Hits sifts through the mushrooming detritus and ranks the tracks per their collective streams on the various platforms on which they’re posted, linking out to the tracks directly (unless, of course, they’ve already been taken down by the time you click). With its notable fealty to the real thing, the Drake-mimicking “Heart on My Sleeve” became the first “hit” of the AI music era, and various versions of it dominate AI Hits. (Sayman points out there’s even one featuring AI-generated Joe Biden vocals, for some reason.) AI versions of Ariana Grande, Travis Scott, Juice WRLD, SZA, and Lana Del Rey are all also represented on the chart. In conversation, Sayman uses the term “voice” to refer to the artist being mimicked, and he uses the term “artist” to refer to the username of whoever created the song. This use of nomenclature may seem small, but it is significant. It’s a step toward the creation of a shared lexicon around all this stuff. The landscape of AI music is endlessly, discursively messy, but as Sayman points out, we’re all present at the outset of a conversation that will unspool over years. “How do you search? Who are the creators? How do you attribute labels to them? What do those revenue splits look like?” he says. “And how does that even work, when you can make a hundred remixes of the same song?” That latter question, over the legality of the practice of AI music, is central. Spotify quickly took down “Heart on My Sleeve,” and UMG, Drake’s parent label, has pushed the company to purge thousands of other AI-made songs. On a recent podcast interview, Ice Cube urged Drake to directly sue the creator of “Heart on My Sleeve,” and he  has tweeted that he finds the idea of generating a song in the style of a dead artist without the approval of the artist’s estate to be “evil and demonic.” But when looking past AI’s potential for legal or ethical blunders, other artists, from pioneering musicians like Holly Herndon to legacy acts like the the Pet Shop Boys, are bullish on AI as a creative tool. It could even unfurl a whole new genre of music. Sayman believes AI can create a more democratic, open-ended music industry. “The record labels used to hold all the power—they were in charge of distribution, resources, production quality. We’ve seen social media replace distribution and discovery of music. Now we’re seeing AI expand production quality, so there’s opportunities for more people to get involved in the music creation process. More Drake singles! Instead of having two or three producers, he can have millions of producers working on those songs!” He laughs. “I’m half joking.” Sayman may be speaking off the cuff, but the futurist popstar Grimes has actually already embraced the idea of empowering an infinite number of nonprofessional musicians to make music with her voice. Via a site called Elf.tech, she’s handing her vocals to anyone for commercial use in exchange for 50 percent of the royalties. In a recent interview with the The New York Times, she reviewed some of the music that’s already been created via Elf.tech, and her sincerity in embracing the tracks provided a refreshing and much needed counterweight to the hysteria over AI. “What I like about the early AI stuff is that you can hear the technology very profoundly,” she said. “I think people will appreciate that more in five years, when they realize people only made stuff like this for a couple months.” “Heart on My Sleeve” was met in part by an almost visceral repulsion, and it’s possible that reaction is rooted in the horror of how easy it seemingly was to create a fake Drake. But as H. Drew Blackburn pointed out in a piece for Bloomberg, “Drake has been making music that sounds like AI Drake for years.” People are collectively obsessed with the theoretical ramifications of AI music. But maybe there’s another question everyone is forgetting to ask about all this content: Is it any good? “Most people don’t make good art by copying and pasting. What makes pop work is that it’s always changing and always responding. This is just a feedback loop between systems.” Marc Weidenbaum is a writer, sound artist, and professor at the Academy of Art University. “The hand-wringing [around AI music], it’s a strange thing to me,” he says. “We’ve been concerned with creating artificial life at least since the Golem.” Ultimately, for Weidenbaum, something like “Heart on My Sleeve” is a negative, not because of any apocalyptic overtones but because it’s, well, boring. He points to something like cybernetic music, in which an artist programs a machine to go off exploring and create sounds outside of the artist’s control. Aleatoric music operates similarly. “Unintended consequences are a feature, not a bug,” he says. “Brian Eno was interested in the idea of the composition being a garden, constantly changing.” But using AI to mimic Drake is a mind-numbingly straightforward thing to do. “Most people don’t make good art by copying and pasting,” Weidenbaum says. “What makes pop work is that it’s always changing and always responding. This is just a feedback loop between systems.” For Joey DeFrancesco, a musician and organizer with the Union of Musicians and Allied Workers, what’s alarming about AI is how the major labels may ultimately utilize it. “Any potentially interesting artistic uses of AI pale in comparison to the corporate domination of AI that will inevitably occur under our current music industry power structures,” DeFrancesco says. “The tech capitalist fantasy” in the music industry “has always been to cut out the artists entirely and remove the need for any royalty payments.” DeFrancesco points out that artists have successfully combated damages from emerging tech in the past: “Musicians in the 1940s went on strike to demand that the profits created by new vinyl record technologies be shared with musicians, and they won.” He also points to the ongoing Writers Guild of America strike, saying, “The big studios want free rein over AI so they can harvest writers' work then cut them out entirely. But writers are collectively saying no.” And DeFrancesco does have reason for concern. After kicking up a fuss over “Heart on My Sleeve,” UMG then moved quickly to embrace machine-learning technology by partnering with an AI company called Endel. All of which is a reminder that theorizing about worst-case scenarios ignores the bad-case scenarios that musicians and fans are currently facing. The pressing questions over AI music are human ones. Does its existence mean that musicians will be getting ripped off in new ways? And is it worth a listen? We don’t yet know the answer to either, but for the latter we do have a burgeoning resource in AI Hits. Currently, Sayman is working on improving the search functionality and on fielding requests from AI Hits’ many impassioned users. As for Sayman’s own AI listening habits, his current favorite is “Por Que,” an AI duet between Rihanna and Bad Bunny. Sayman is a native Spanish speaker, and his family is from Peru. “It’s kind of a Spanglish song,” he says, “but it’s funny—the Spanish doesn’t sound too good.” Apparently the AI that made the song was trained on a data set that didn’t have enough Spanish pumped in. He laughs. Whatever. To him, it’s still a banger. 📧 Get the best stories from WIRED’s iconic archive in your inbox 🎧 Our new podcast wants you to Have a Nice Future The explosive legacy of the pandemic hand sanitizer boom Scientists gave people psychedelics—then erased their memory I asked AI chatbots to help me shop. They all failed The race is on to crack an artist’s “test” signal from aliens The speedrunners trying to break Tears of the Kingdom ⛺ Embrace the new season with the Gear team’s best picks for best tents, umbrellas, and robot vacuums Jennifer M. Wood More From WIRED © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
20,https://www.politico.com/newsletters/digital-future-daily/2023/06/05/ai-isnt-just-what-the-west-does-00100264,"How the next wave of technology is upending the global economy and its power structures How the next wave of technology is upending the global economy and its power structures By signing up you agree to allow POLITICO to collect your user information and use it to better recommend content to you, send you email newsletters or updates from POLITICO, and share insights based on aggregated user information. You further agree to our privacy policy and terms of service. You can unsubscribe at any time and can contact us here. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. You will now start receiving email updates You are already subscribed Something went wrong By signing up you agree to allow POLITICO to collect your user information and use it to better recommend content to you, send you email newsletters or updates from POLITICO, and share insights based on aggregated user information. You further agree to our privacy policy and terms of service. You can unsubscribe at any time and can contact us here. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. By BEN SCHRECKINGER  06/05/2023 04:48 PM EDT With help from Derek Robertson The European Union flag and the American flag. | Jacquelyn Martin, File/AP Photo In the end, it could be multipolarity that brings about the singularity. As leaders from the U.S. and Europe struggled to coordinate their AI response in Sweden last week, the government of Abu Dhabi was toasting the recent climb of its own large language model to the top of a global performance ranking. The milestone — along with the decision a week prior to make the model open-source — highlights two trends complicating the transatlantic effort to rein in the development of artificial intelligence. One is the global decentralization of digital know-how. The other is a growing willingness from the rest of the world to buck the wishes of the Western alliance. As POLITICO reported last week, leaders from both sides of the Atlantic are looking for common ground between the (more permissive) U.S. and (more restrictive) European posture towards AI in hopes of presenting a joint proposal to the G7 this fall. Meanwhile, a letter released last week by AI experts compared the technology’s risks to those of nuclear weapons. It only took the USSR four years after Hiroshima to detonate its own bomb, with the technology spreading to roughly a half-dozen more nations from there. The barriers to the spread of AI are, to put it mildly, lower. That means policymakers in Washington, Brussels and London are at risk of fighting the last war — stuck in a moment when the most important consumer markets and the firms on the bleeding edge of technical innovation were largely confined to the U.S. and the European Union. In 2023, it’s becoming more important to account for developments in the rest of the world. Of course, there are the usual suspects to account for outside of this bloc: Russia’s state-owned Sberbank has invested heavily in AI, while China’s global share of AI patents grew from less than 5 percent in 1996, to more than a quarter in 2021, according to one estimate, putting it into contention as a global leader in the technology. But, it’s increasingly apparent that “Russia, China and the West” is not the whole story either when it comes to the global AI race. Late last month, Abu Dhabi’s state-run Technology Innovation Institute announced that it was making its Falcon 40B LLM — first released in March — open-source. The move coincided with the model’s ascension to the top of a performance ranking of about 100 open AI models maintained by New York-based AI firm Hugging Face. The institute, founded in 2020, touts its open-sourcing decision as a means of making AI more “inclusive.” DFD has written about the challenges of reining in open-source AI. When some of those open-source models are backed by sovereign states with a degree of geopolitical independence, that only compounds the challenge. Outside of the trans-Atlantic alliance, even a close ally like G7 member Japan views emerging digital technologies as an opportunity to chip away at U.S. tech dominance. That, too, promises to complicate coordinated AI regulation. One point of divergence between the allies is already clear. For example, in the U.S., in lieu of near-term rules tailored to AI, copyright law is considered a possible avenue for slowing the technology’s advance, because of its potential to restrict the use of copyrighted material for training AI models. In Japan, on the other hand, existing copyright laws are less restrictive. Domestic content creators have stepped up pressure on Japan’s government in recent weeks to address the use of copyrighted works to train AI. But a fact sheet released last month by the office of Prime Minister Fumio Kishida reaffirmed the legality of training generative AI models on copyrighted works. Kishida has said he wants Japan to lead global rule-making on AI through its chairmanship of the G7 this year. Even assuming the G7 can reach a meaningful consensus, the gap between that and effective global consensus is getting bigger, not smaller, at a time when AI advancement shows no signs of slowing down. DON’T MISS POLITICO’S HEALTH CARE SUMMIT: The Covid-19 pandemic helped spur innovation in health care, from the wide adoption of telemedicine, health apps and online pharmacies to mRNA vaccines. But what will the next health care innovations look like? Join POLITICO on Wednesday June 7 for our Health Care Summit to explore how tech and innovation are transforming care and the challenges ahead for access and delivery in the United States. REGISTER NOW. Big news over the weekend from the crypto world: Late Friday, House Republicans released the draft text of a bill that would radically overhaul the government’s approach to regulating cryptocurrencies, finally settling the push and pull between the Securities and Exchange Commission and the Commodity Futures Trading Commission. POLITICO’s Zachary Warmbrodt wrote in today’s edition of Morning Money that the proposal immediately became “the main event in Washington when it comes to digital asset policy.” The bill would give a large amount of regulatory authority to the CFTC over the SEC, in what’s seen as a big win for the crypto industry. Coinbase chief legal officer Paul Grewal tweeted that although “A comprehensive bill of this magnitude warrants an in-depth review… what we’re seeing so far is encouraging,” and Blockchain Association President Kristin Smith said in a statement that they “appreciate” the bill and “look forward” to reviewing it and providing feedback. (The SEC’s aggressive stance on crypto as of late has made it somewhat of an industry foe, to say the least.) In MM Zach points out that those good vibes are exactly what could give the bill a bumpy ride through the House: Consumer-watchdog Democrats have to vote on the industry-friendly, “innovation”-oriented bill, too, and as in the case of the Dodd-Frank Act, the tangled web of financial rulemaking could prevent it taking effect for years. (For an extremely detailed look at the bill, see this 200-plus-part Twitter thread from policy analyst Justin Slaughter.) — Derek Robertson A clash of philosophies over tech regulation is brewing in the European Union. POLITICO’s Mark Scott and Gian Volpicelli reported today for Pro subscribers on the growing discord between Margrethe Vestager, the bloc’s regulation-first European Commission executive vice president, and Thierry Breton, its more relatively laissez-faire Internal Market commissioner. Both officials are staking their claim to leadership on how the EU tackles AI, something that will be a major test of its privacy-first, consumer-oriented regulatory framework. Mark and Gian detail their dueling plans: Breton’s “AI Pact,” which would “outlaw some AI use cases, straitjacket some ‘high-risk’ applications, and impose rigorous checks on how Google, Microsoft and others develop the emerging technology,” all starting in 2025, and Vestager’s proposed code of conduct that would ambitiously expand the EU’s regulatory influence by partnering with the U.S. on rulemaking. Few details of Vestager’s plan are known as of yet, but as Gian and Mark write, “Which digital czar prevails matters. Europe hopes to complete its AI Act by the end of the year, with the rules coming into force likely by 2026. With such a gap before the rules become enforceable, EU officials acknowledge something is needed in the interim to address AI’s potential downsides.” — Derek Robertson Stay in touch with the whole team: Ben Schreckinger ([email protected]); Derek Robertson ([email protected]); Mohar Chatterjee ([email protected]); and Steve Heuser ([email protected]). Follow us @DigitalFuture on Twitter. If you’ve had this newsletter forwarded to you, you can sign up and read our mission statement at the links provided. GET READY FOR GLOBAL TECH DAY: Join POLITICO Live as we launch our first Global Tech Day alongside London Tech Week on Thursday, June 15. Register now for continuing updates and to be a part of this momentous and program-packed day! From the blockchain, to AI, and autonomous vehicles, technology is changing how power is exercised around the world, so who will write the rules? REGISTER HERE. © 2023 POLITICO LLC"
21,https://www.wired.com/story/chegg-embraced-ai-chatgpt-ate-its-lunch-anyway/,"To revist this article, visit My Profile, then View saved stories. To revist this article, visit My Profile, then View saved stories. Investors were surprised when the online education company Chegg last month revealed that ChatGPT was hurting subscriber growth—the company lost half of its market value overnight. But long before Chegg became an index case for the disruptive force of ChatGPT, its top brass had heard plenty of warnings about the threat and opportunity of generative AI.  For years, on afternoon walks outside Chegg’s Silicon Valley headquarters, former executives say they had discussed someday slashing costs by tapping AI programs to replace an army of instructors that answer student questions and draft flashcards. Matthew Ramirez, a product leader who left Chegg two years ago, says he even advised CEO Dan Rosensweig in 2020 that generative AI would be the bus that ran down Chegg if it didn’t prepare itself. Outside advisers flagged similar concerns. And just weeks after OpenAI launched ChatGPT last November, a source familiar with the exchange says, one Chegg executive had the bot write an email to Rosensweig urging him to develop a ChatGPT rival. What happened next shows a company that had tried to keep up with advances in AI getting blindsided by the rapid pace at which consumers have embraced experimental but capable tools such as ChatGPT—a position that many companies may find themselves in if the recent torrent of generative AI development continues. Interviews with two current and five former Chegg executives, along with three other former employees, indicate the company had considered the potential for AI to supplant its services but figured it would not manifest soon. The company’s chief operating officer, Nathan Schultz, says executives had bet in a recent five-year plan that an experience like ChatGPT wouldn’t be possible until at least 2025. And even after the bot’s debut, Chegg saw no cause for alarm, because data showed that the chatbot wasn’t luring away the 8 million paying subscribers to human-generated study guides and homework help. But the sirens went off in March, when OpenAI unleashed GPT-4, its most powerful model yet. It reignited the AI excitement just as students in the US and some other countries began taking spring exams. Undergraduates and high schoolers who might have paid Chegg as little as $16 a month for practice exams and term paper feedback quietly opted for ChatGPT instead—the free, fast, and cool new kid on the block. “We believe this is an existential change.” Chegg CFO Andy Brown would later describe the chatbot as vanishing 100,000 would-be subscribers “right around the fringes” of the subscription services that account for 90 percent of the company’s overall sales. Rosensweig says he had met with his friend Sam Altman, OpenAI’s CEO, for a couple of hours to discuss the future of education, and months later in mid-April, they announced a partnership to create CheggMate, an AI learning companion powered by GPT-4, Chegg’s own algorithms, and its repository of 100 million study questions built over previous years. The deal may have erected a barrier, but the bus came crashing through anyway. Two weeks later, on May 1, Rosensweig revealed the stunted growth to investors and said Chegg would not provide revenue forecasts for the second half of this year because the relationship students would have with ChatGPT when they return to school after the summer break was anyone's guess. “It was a little bit bewildering,” says Jeff Silber, an analyst who tracks Chegg for investment bank BMO. “It left a lot of people scratching their heads over how quickly ChatGPT took a hold.” Chegg shares plummeted 48 percent the day after Rosensweig’s bombshell and have not rebounded since. Last year, Chegg didn’t mention “artificial intelligence” a single time in its annual report to investors. This year, it came up nine times, mostly as a threat. “We have absolutely pivoted internally to focus our resources on CheggMate and AI,” Brown said at an investor event on May 18. “We believe this is an existential change.” Chegg is not the only education company suffering right now. ChatGPT-induced euphoria on Wall Street for shares of Microsoft, Nvidia, and other companies benefiting from relationships with OpenAI has been accompanied by a souring on ed-tech businesses such as Chegg, Duolingo, and Udemy. Some investors consider them to be threatened by ChatGPT’s ability to play teacher and tutor. None has taken the hit that Chegg has, or publicly warned that AI chatbots are chewing into its business, but if tech industry expectations for generative AI hold, it will be far from the last company to see its business undermined by text generators and chatbots. Chegg was founded in 2005 by college students to rent textbooks. Its founders quickly moved on to other ventures, but it has survived by repeatedly reshaping its business, riding out a pricing war with Amazon over textbook rentals a decade ago, and then shifting away from that money-losing business. “We made ourselves competitive long enough that we could transition,” says Ben Van Roo, a Chegg vice president at the time who now runs generative AI vendor Yurts AI. Chegg went on to make a series of acquisitions that created a highly profitable array of services, including language courses, months-long skills training for corporate workers, and a math problem solver. Students can save time, money, and perhaps their grades by “Chegging it” instead of hiring tutors, going deep into books, or knocking on professors’ doors. The idea that AI might change or challenge those services has been on Chegg executives’ minds for years. Since late 2018, it has used free, open-sourced models developed by OpenAI to help offer grammar and composition suggestions to students in a writing aid feature and to score the quality of internal documents, according to Ramirez, the former director of Chegg’s writing aid. “We could see whether a suggestion we gave actually made the writing more fluent, whether the text was better or worse with what we were suggesting,” says Ramirez, who now runs AI-based writing helpers Rephrasely and Paraphrase Tool. AI programs also help route subscribers’ academic questions to appropriate experts among Chegg’s over 150,000 contractors, most of them in India. The prospect of using AI to create the content students and other learners want isn't new to the company either. Since long before ChatGPT came on the scene, Brown has said the holy grail for Chegg has been generating content with algorithms to reduce tens of millions of dollars in labor and licensing costs. But OpenAI’s early models were not very fluent at text generation, and some Chegg leaders talked regularly about how difficult it would be to safely operate generative AI, according to three former employees. They feared students could goad a chatbot into silly and problematic responses that could tarnish Chegg’s reputation, while any instructional errors held huge academic consequences for users and liability questions for the company.  In subjects such as engineering, chemistry, and statistics, which drive significant traffic to Chegg but often involve diagrams, there was a sense that relying too heavily on AI to parse visual information was unreasonable, the former employees say. So the ethics of unleashing an imperfect product gave Chegg pause. “We knew generative was coming down the pike,” says one former executive. “Text analysis was easy to embrace in the short run.” In 2020, OpenAI’s GPT-3 model was released and made text generation much better. Some machine-learning leaders at Chegg wanted to get their hands on it, but one source says executives weren’t aggressive about securing access to the technology, which OpenAI did not open-source. Early this year, GPT-3’s successor was added to ChatGPT, and the centrality of generative AI to Chegg’s future became inarguable, carved as it was into the company's dented user growth. Chegg is now focused on proving with its in-house bot CheggMate that it’s possible to outcompete ChatGPT when it charges onto your turf. “We happened to be one of the industries that's facing it first, and that gives us a wonderful opportunity to understand it deeper and sooner and come on to the other side of it with unique and value-creating products for our consumers,” says Schultz, the COO. The company has marshaled all extra hands onto CheggMate and AI development, including by reassigning teams that worked on collecting more data from users to personalize services through more traditional means. Brown, the CFO, told investors last month that the company’s summer interns will be fully focused on CheggMate. But Chegg doesn’t have the best record of developing products from scratch and has previously leaned on acquisitions, leaving some former executives closely following CheggMate unsure of its prospects. The new service also doesn’t exactly ease ethical implications. Chegg has long faced allegations from colleges and universities that it enables cheating, as students secretly turn to its tools to complete homework and exams. Officially, Chegg bars dishonest use and carries out and supports integrity investigations, says Nina Huntemann, the company’s chief academic officer. But former Chegg data scientist Eric Wang worries that CheggMate and similar applications could spread the cheating habit. Students feel overwhelmed and pressed for time, and feel they are competing for scarce opportunities, he says. “All of these forces drive students who know better to make decisions that are in hindsight not great,” Wang says, suggesting that there could be better ways to support students and educators. Select users, along with Chegg’s subject matter experts and academic advisers, began testing CheggMate over the past couple of weeks, but it isn’t expected to publicly launch until next year. That means it won’t be ready for the US fall semester, when Chegg typically generates its greatest sales. Schultz says he’s proud of the company’s response to ChatGPT’s arrival. “We weren't going to react overnight and just throw something up on the site,” he says. “We have a responsibility to be thoughtful.” “We happened to be one of the industries facing ChatGPT first.” When a user types a query to CheggMate, it first attempts to categorize whether the request is for help understanding a concept, solving a particular problem, or concerning a particular subject, Schultz says. The system then tries to direct the question to the best resource, with the options including prompting GPT-4, having a human expert answer, or re-airing an old answer from Chegg’s database. CheggMate is designed to keep users engaged through positive reinforcement and pushing related content. “We could say, ‘Why don't you try this similar problem? Why don't you guess a step?’” says Huntemann, the chief academic officer. “Conversation allows us to extend the experience.” Chegg executives hope tuning their chatbot to education that way will make ChatGPT look less attractive as a homework helper. Pricing for CheggMate has not been determined; operating generative models is expensive, and those costs rise with usage. But two former employees say that having a human expert answer a question costs about $2. Generating a comparable response through GPT-4 possibly runs half a US cent, and having an expert edit it might cost $1 overall, they say, suggesting the economics could work out for Chegg. At the same time, competition is likely to intensify from ChatGPT itself, Microsoft, and Google’s generative AI-powered search features, or rivals developing their own AI tutors using OpenAI technology such as Quizlet, Brainly, and Khan Academy. That could force Chegg to spend more on marketing to stay relevant. Silber, the stock analyst, expects Chegg’s operating profit margins will suffer for some time. The recent ride has made Rosensweig, who has led Chegg since 2010, and his friend of 20 years, OpenAI’s Altman, into competitors and perhaps frenemies. They both have a hand now in shaping the next chapter of education. All of the people WIRED spoke to described Rosensweig, whose mom was a public school teacher, as someone who wants to see people have the chance to pull themselves up through educational opportunities. He hosts an online show called Going From Broke, in which he and a financial strategist help people with major money troubles bounce back. If Rosensweig was to rebuild Chegg for the generative AI age, ChatGPT suggests that show could be called Chegg Resurgence: Triumph Over Turbulence. 📨 Understand AI advances with our Fast Forward newsletter 🎧 Our new podcast wants you to Have a Nice Future It’s the Age of Ozempic. Do we still need Weight Watchers? Temu is losing millions of dollars to send you cheap socks Sex workers are still hot for Twitter Spaces The security hole at the heart of ChatGPT and Bing Remembering GitHub’s office, a monument to tech culture 📷 Snap into spring with the Gear team’s picks for the best camera bags, fun instant cameras, and mirrorless cameras Omar L. Gallaga More From WIRED © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
22,https://home.dartmouth.edu/news/2023/06/new-dartmouth-center-applies-ai-improve-health-outcomes,"Sign Me Up Center for Precision Health and Artificial Intelligence to bring data to patient care. Dartmouth has created a Center for Precision Health and Artificial Intelligence to spur interdisciplinary research that can better leverage—as well as more safely and ethically deploy—biomedical data in assessing and treating patients and improving their health care outcomes. The center is being launched with initial funding of $2 million from the Geisel School of Medicine and Dartmouth Cancer Center and is based in the Williamson Translational Research Building, a Dartmouth-owned building on the Dartmouth Hitchcock Medical Center campus in Lebanon, N.H. “Artificial intelligence is poised to play a transformative role in health care by delivering rapid and innovative solutions to real-world clinical challenges, improving patient outcomes, and creating better and more equitable access for all,” says President Philip J. Hanlon ’77. “This new center will help foster innovation and collaboration in these critically important fields.” Precision health is a holistic approach that aims to personalize health care by tailoring treatments and disease prevention strategies to a person’s unique biology—their genes, medical history, lifestyle, and environment. A wealth of biomedical data can be gathered through genomic sequencing, molecular testing, imaging techniques, and wearable monitoring devices, all of which have become more advanced, affordable, and broadly available over the past decade. It is truly a Dartmouth center with leaders and advisers from across the institution connecting clinicians and AI scientists. AI holds the key to extracting valuable insight from this deluge of data because it can sift through and analyze complex and heterogeneous information to identify trends and patterns and extract digital biomarkers that can guide clinically actionable decisions. Machine learning models trained on a host of different data sets can predict disease risk, enhance the accuracy of diagnoses, anticipate the course of an illness, and tailor treatment options best suited to the patient. CPHAI will be governed by the dean of Geisel and advised by a committee that will have representatives and stakeholders from Geisel, Dartmouth Cancer Center, Thayer School of Engineering, Arts and Sciences, and Dartmouth Health. “It is truly a Dartmouth center with leaders and advisers from across the institution connecting clinicians and AI scientists,” says Geisel Dean Duane Compton. By harnessing the power of AI and machine learning, CPHAI aims to create a toolbox of digital technologies that will empower providers to identify and deliver the most effective health care strategy for each patient. Researchers will work on projects such as developing AI-driven diagnostic tools, optimizing treatment strategies, and analyzing biomedical data to inform public health policies. AI models created through collaborations with radiologists and pathologists will be able to draw precise and complex inferences directly from medical images that complement the knowledge and experience of human imaging professionals and make diagnoses more reliable and efficient, reducing potential diagnostic errors. The center will also enable researchers to evaluate new digital tools they develop in clinical settings, paving the way for creating and building applications that can be integrated into health care systems after seeking FDA approval. “What makes CPHAI unique is its interdisciplinary and comprehensive approach to precision health and artificial intelligence, focusing not only on technological advancements but also on ethical and societal implications,” says Saeed Hassanpour, associate professor of biomedical data science, epidemiology, and computer science, who serves as the center’s inaugural director. The center, which will collaborate with the Dartmouth Ethics Institute, Neukom Institute for Computational Science, and the Wright Center for the Study of Computation and Just Communities, is committed to ensuring the ethical use of AI and fostering diversity and inclusion in the field, says Hassanpour. This commitment will help identify the limitations of AI, address issues related to biases in AI algorithms and datasets, improve transparency and privacy, and ensure equitable outcomes for all individuals, regardless of their background. CPHAI will also create new educational and training opportunities, attracting students and professionals interested in pursuing careers in AI and precision health, says Hassanpour. These opportunities will help develop a skilled workforce in the Upper Valley region, making it an attractive destination for technology and health care companies. Medical residents, postdocs, and students—both graduate and undergraduate—interested in working with artificial intelligence will also find unique opportunities for learning and research, Compton says. “We want every individual to reach their optimal health, which means both prevention and care medicine must come together. Precision health is a broader application than precision medicine,” says Compton. The new Dartmouth center has been in the works for several years and comes as the market for AI in health care is expected to grow tremendously—from just under $5 billion in 2020 to more than $45 billion in 2026. “Many of us at Dartmouth have been working in AI in the last several years. We have the talent, skills, experience, and material to develop and implement innovative AI-driven diagnostic tools,” says Arief Suriawinata, chair of pathology and laboratory medicine and a member of the center’s advisory committee. “The formation of CPHAI will foster intercampus and interdisciplinary collaborations, attract and retain top talent, and secure additional funding for our concerted efforts.” The technologies developed at CPHAI will help pathologists in triaging and screening cases, improve the diagnostic standard and quality, and optimize workflow, he says. Another member of the advisory committee, Jocelyn Chertoff, chair of radiology, also sees great promise for patients from the center’s work. “From management of administrative clinical tasks to computer-aided detection of cancers, radiologists are already using AI,“ Chertoff says. ”Tools based on deep learning algorithms promise to transform the practice by helping radiologists better interpret images, make the process of producing images from scanners more accurate and efficient, and improve a hospital’s overall workflow so that patients get the most timely care.” Also on the advisory committee are Steven Leach, director of the Dartmouth Cancer Center, Elizabeth F. Smith, dean of the Faculty of Arts and Sciences, Steven Bernstein, Dartmouth Health chief research officer, Michael Whitfield, chair of biomedical data science, and Charles Thomas Jr. ’79, chief of radiation oncology. Hassanpour expects that the center will actively engage with local and global communities to ensure their perspectives, concerns, and needs are considered in the development and application of AI technologies. This engagement will serve to build trust and awareness about the benefits and potential risks of AI in health care. “Overall, CPHAI’s presence in our region could lead to significant advancements in health care, education, and economic development, positioning the area as a leader in AI and precision health research,” he says. An FAQ on the CPHAI is also available. Harini Barath can be reached at harini.barath@dartmouth.edu. We inspire students to practice good global citizenship while strengthening their own communities. You can invest in our future leaders."
23,https://www.foreignaffairs.com/world/ground-rules-age-ai-warfare,"On March 14, a U.S. surveillance drone was on a routine mission in international airspace over the Black Sea when it was intercepted by two Russian fighter jets. For nearly half an hour, the jets harassed the American system, an MQ-9 Reaper drone, buzzing past and dumping fuel over its wings and sensors. One of the jets clipped the Reaper’s propeller, rendering it inoperable and forcing its American handlers to crash the drone into the sea. Not long after, Moscow awarded medals to the two Russian pilots involved in the incident. The Reaper’s every move—including its self-destruction after the collision—was overseen and directed by U.S. forces from a control room thousands of miles away. But what if the drone had not been piloted by humans at all, but by independent, artificially intelligent software? What if that software had perceived the Russian harassment as an attack? Given the breakneck speed of innovation in artificial intelligence (AI) and autonomous technologies, that scenario could soon become a reality. Traditional military systems and technologies come from a world where humans make onsite, or at least real-time, decisions over life and death. AI-enabled systems are less dependent on this human element; future autonomous systems may lack it entirely. This prospect not only raises thorny questions of accountability but also means there are no established protocols for when things go wrong. What if an American autonomous drone bombarded a target it was meant only to surveil? How would Washington reassure the other party that the incident was unintentional and would not reoccur? When the inevitable happens, and a partially or fully autonomous system is involved in an accident, states will need a mechanism they can turn to—a framework to guide the involved parties and provide them with potential off-ramps to avert unwanted conflict. The United States took a small step in this direction when it released a declaration in February that distilled its vision for responsible military use of AI and autonomous systems. The declaration included several sound proposals, including that AI should never be allowed to determine the use of nuclear weapons. But it did not offer precise guidelines for how states might regulate the behavior of AI systems, nor did it set up any channels through which states could quickly clear up any miscommunications. A more comprehensive framework, with more buy-in from other governments, is sorely needed. For inspiration, states could look to an underappreciated episode of the Cold War. In the 1970s, U.S. and Soviet leaders calmed rising tensions between their navies by setting rules for unplanned encounters on the high seas. Governments today should take a similar route through the uncharted waters of AI-driven warfare. They should agree on basic guidelines now, along with protocols to maximize transparency and minimize the risk of fatal miscalculation and miscommunication. Without such a foundational agreement, future one-off incidents involving AI-enabled and autonomous systems could too easily spin out of control. The loss of an American surveillance drone over the Black Sea in March was unsettling. The U.S. military has well-defined procedures for how to act if one of its crewed aircraft is shot down. But recent experience shows that standardized protocols do not necessarily extend to uncrewed aircraft. In one 2019 incident, Iran shot down a U.S. Navy drone over the Strait of Hormuz, setting off a chain reaction inside the Pentagon and the White House that nearly resulted in U.S. retaliatory strikes against Iran. U.S. forces were purportedly ten minutes from engaging their target when the strike was called off, according to then U.S. President Donald Trump. At the last moment, leaders in Washington had decided that a strike was disproportionate and opted instead for a cyberattack against Iranian intelligence and missile systems. The upside of remotely operated aircraft is that, when the circumstances are right, they can lower the risk of escalation rather than drive it up. This is partly because the loss of insentient machinery, no matter how expensive, is easier to stomach than the death of an aircrew. But that silver lining may dissipate as technologies evolve. Fully autonomous military systems do not yet exist, and the deployment of AI-enabled systems on the battlefield remains limited. Yet militaries worldwide are investing heavily in AI research and development. The U.S. Department of Defense alone has nearly 700 active AI projects. Among them are the U.S. Army’s Scarlet Dragon program, which has used AI to identify targets in live-fire exercises, and the U.S. Navy’s Task Force 59, which seeks to develop cost-effective, fully autonomous surveillance systems. The U.S. Air Force hopes to one day create swarming smart weapons capable of autonomously communicating with one another and sharing information on potential targets. The U.S. military is not the only innovator on this front. In April, Australia, the United Kingdom, and the United States conducted a joint trial in which a swarm of AI-enabled aerial and ground vehicles collaborated to detect and track targets. China is investing in an array of AI-powered underwater sensors, some of which are reportedly already in use in the South China Sea. The war in Ukraine has witnessed some of the first real uses of AI in direct conflict. Among other things, Ukrainian forces have used an AI software interface that consolidates commercial satellite data, thermal images of artillery fire, and other intelligence. The information is superimposed on digital maps that commanders on the ground can use to pick their targets. Encouraged by the benefits they already derive from AI-enabled systems, militaries will likely stay their current course and design future systems with growing degrees of autonomy. This push toward AI-enabled autonomy will certainly unlock strategic and tactical advantages, but they will come at a cost. Perhaps the greatest challenge is that humans who encounter an autonomous military system may be faced, in essence, with a black box. When confronted or targeted, they may have difficulty gauging the system’s intent and understanding its decision-making. This is partly a feature inherent in the technology because the algorithm at work often will not or cannot explain its “thought process” in terms humans can grasp. Adversaries, in turn, may have difficulty distinguishing intentional aggression from errant AI behavior, leaving them uncertain about how to react. Worse still, research suggests that the accidental use of force by an AI-enabled autonomous weapons system may elicit a more aggressive response than conventional human error: leaders in the targeted country may feel angered by the other side’s decision to delegate any lethal decision-making to a machine in the first place, and they may opt for a forceful reaction to indicate that displeasure. Some of the novel scenarios and the security risks they entail may differ not just from human error but also from the usual fog of war. Take a recent thought experiment conducted by an official in charge of the U.S. Air Force’s AI testing, in which an AI-enabled drone is trained to identify targets and destroy them on approval from a human operator. Each eliminated target equals a point, and the AI seeks to maximize a point-based score. It may conclude that its dependence on human approval for strikes limits its ability to accumulate points and may therefore decide to eliminate the operator. If the AI’s programming is tweaked to deduct points for killing the operator, the AI may instead resort to destroying the communication tower that relays the operator’s orders. What distinguishes this scenario from traditional human error or from a soldier going rogue is that the AI’s actions are neither accidental nor in violation of its programming. The behavior, although undesirable, is a feature, not a bug. This is a classic case of the “alignment problem”: it is challenging to develop and program AI such that its actions coincide exactly with human goals and values, and getting it wrong can have grave consequences. An added danger is the role that autonomous and AI-enabled systems could play in military standoffs and games of chicken. Human recklessness is mitigated by survival instinct, among other things, but that instinct might not come into play when autonomous systems are deployed without a human operator on board. Consider another scenario: a pair of fully autonomous aircraft from rival countries confront each other in the skies above contested territory. Both systems perceive the other as a threat and, since they are programmed for aggressiveness, engage in escalating maneuvers to assert dominance. Before long, one or both systems are damaged or downed unnecessarily, and the rival countries have a crisis on their hands. AI could also alter the domain of nuclear warfare, for better or worse. The speed of AI could allow for an incoming nuclear missile to be detected sooner, buying decision-makers valuable time to weigh their options. But when both sides are using AI, that same speed could add pressure to act fast (and think later) to avoid being outmaneuvered. AI-enabled nuclear deterrence would be a double-edged sword, too: autonomous nuclear-armed systems may make it harder for an attacker to take out all of a state’s nuclear defenses in one fell swoop, thus lowering incentives for a preemptive first strike. On the flipside, the complexity of AI-driven systems brings with it the risk of cascading, and potentially catastrophic, failures. The confluence of these risk factors makes determining the correct response to incidents involving AI-enabled and autonomous systems uniquely complex and context dependent. Given how hard it will be to grapple with such complexity on the fly, states need to build off-ramps from potential conflict ahead of time. Fortunately, in doing so, they can rely on blueprints from the past. In 2020, around 90 percent of U.S. reconnaissance flights over the Black Sea were intercepted by Russian jets, according to the U.S. military. NATO said it had intercepted Russian aircraft on over 300 occasions that same year. Such intercepts are not new; they are a modern version of the nineteenth-century practice of gunboat diplomacy. The term emerged to describe Western states’ tendency to use physical displays of naval assets to project power and intimidate other nations into complying with their demands. As technology advanced, the gunboats gave way to aircraft carriers, then to B-52 bombers, and later still to E-3 Sentry AWACS surveillance aircraft and other imposing innovations. The use—and the interception—of increasingly high-tech, AI-enabled systems is simply the latest iteration of such techno-tactics. As the name suggests, gunboat diplomacy is used to pursue diplomatic aims, not military ones. But given the tools involved, miscalculation and miscommunication can have dire consequences. States have long understood this, and they have, in the past, found ways to limit the danger of unintended escalation. One of the most effective of these mechanisms emerged during the Cold War. At the time, the Soviet Union objected to U.S. naval operations in waters it considered its own, such as the Black Sea and the Sea of Japan. It made its position clear by repeatedly and aggressively intercepting U.S. vessels, leading to a series of dangerous close calls. By the early 1970s, the United States and the Soviet Union had come to recognize that, as the scholar Sean Lynn-Jones wrote a few years later, “the risks of naval harassment undermine any justification for its continued unconstrained practice.” That mutual insight led, in 1972, to the U.S.-Soviet Incidents at Sea Agreement. It is challenging to develop AI with human values, and getting it wrong can have grave consequences. The INCSEA agreement, as it became known, covered any interaction between U.S. and Soviet military vessels on the high seas, from deliberate confrontations to unplanned encounters. It created notification protocols and information-sharing procedures designed to lower the risk of accidents and unintended conflict. As early as 1983, the U.S. Navy declared the agreement a success for having reduced the number of aggressive high-seas interactions even as the U.S. and Soviet navies had both expanded in size. Like other confidence-building measures, the agreement did not constrain military operations or force structures. It neither eradicated nor fundamentally transformed U.S.-Soviet competition in the naval domain. It did, however, make the rivalry more predictable and safer. The success of the INCSEA agreement paved the way for similar mechanisms on the high seas and beyond. The Soviet Union and, later on, Russia replicated the agreement with 11 NATO members and several countries in the Indo-Pacific. Additional U.S.-Soviet Union agreements created similar protocols for encounters on land and in the air. More recently, China and the United States have developed a nonbinding Code for Unplanned Encounters at Sea to which they and nearly 20 other states now adhere. There have even been discussions of extending similar mechanisms to outer space and cyberspace. To be sure, an agreement such as INCSEA technically applies whether a crew is on board or not, but it ultimately assumes that human operators are in control. The unique challenges presented by AI-enabled and autonomous systems demand more tailored solutions. Think of it as an INCSEA agreement for the age of AI: an Autonomous Incidents Agreement. The first hurdle for any such agreement is the difficulty parsing the meaning and intent behind an AI-enabled system’s behavior. Down the line, it may be possible to monitor and verify these systems’ internal workings and code, which could offer greater transparency about how they make their decisions. But as a stopgap measure, an Autonomous Incidents Agreement could start by regulating not AI code but AI behavior—setting rules and standards for expected conduct for both AI and autonomous systems and the militaries that use them. Elements of such an agreement could be as simple as requiring that autonomous and AI-enabled aircraft yield the right of way to nonautonomous aircraft (as Federal Aviation Administration rules already require of uncrewed commercial and recreational drones). The agreement could also require AI-enabled systems to stay at a certain distance from other entities. It might set notification and alert provisions to ensure transparency about who is deploying what. The private sector appears willing to at least somewhat self-regulate its AI development. Such provisions may seem obvious, but they would not be redundant. Outlining them in advance would set a baseline for expected behavior. Any actions by an AI system outside those parameters would be a cut-and-dried violation. Moreover, these parameters would make it easier to point out cases in which an AI-enabled system deviated from its expected behavior, even if it might not be technically feasible to determine a precise cause after the fact. The time for an Autonomous Incidents Agreement is ripe, given that AI is at an inflection point. On the one hand, the technology is maturing and increasingly suitable for military use, whether as part of wargaming exercises or in combat, such as in Ukraine. On the other hand, the exact outlines of future AI military systems—and the degree of disruption they will cause—remain uncertain and, by extension, somewhat malleable. States willing to take the initiative could build on existing momentum for stricter rules. The private sector appears willing to at least somewhat self-regulate its AI development. And in response to member state requests, the International Civil Aviation Organization is working on a model regulatory framework for uncrewed aircraft systems and has encouraged states to share existing regulations and best practices. An Autonomous Incidents Agreement would put these nascent efforts on solid footing. The need for clearer norms, for a baseline mechanism of responsibility and accountability, is as great as it is urgent. So is the need for a protocol for handling interstate skirmishes involving these cutting-edge systems. States should start preparing now, since the real question regarding such incidents is not whether they will occur, but when. Washington Needs an Endgame in Ukraine Why the Financial Crisis Took Economists By Surprise Iran’s Proxies Have Seized Power in Baghdad—and Are Gutting the State The War in Ukraine Has Become a Battle for the Russian Psyche Regulating AI Will Not Set America Back in the Technology Race How to Defend Against Supercharged Disinformation Get the Magazine From the publishers of Foreign Affairs Published by the Council on Foreign Relations ©2023 Council on Foreign Relations, Inc. All Rights Reserved."
24,https://www.foxnews.com/politics/house-democrat-bill-artificial-intelligence,"This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper. Harvey Castro talks about how AI cold be used in cold cases and the symbiotic relationship between AI and a detective. A new bill introduced in the House of Representatives on Monday is aimed at making sure American consumers know the difference between fantasy and reality online by cracking down on generative artificial intelligence technology.  Rep. Ritchie Torres, D-N.Y., is leading the effort on the AI Disclosure Act of 2023, which would force AI-generated content to include the disclaimer, ""Disclaimer: this output has been generated by artificial intelligence."" In a statement announcing the bill, Torres predicted that ""regulatory framework for managing the existential risks of AI will be one of the central challenges confronting Congress in the years and decades to come."" He noted risks in going too far with policing AI as well as not regulating it enough. BIDEN SAYS ARTIFICIAL INTELLIGENCE SCIENTISTS WORRIED ABOUT TECH OVERTAKING HUMAN THINKING AND PLANNING UNITED STATES - MARCH 29: Rep. Ritchie Torres, D-N.Y., attends the House Financial Services Committee hearing titled The Federal Regulators' Response to Recent Bank Failures, in Rayburn Building on Wednesday, March 29, 2023. ""The simplest place to start is disclosure. All generative AI – whether the content it generates is text or images, video or audio – should be required to disclose itself as AI,"" Torres said. ""Disclosure is by no means a magic bullet but it’s a commonsense starting point to what will surely be a long road to regulation."" His bill, if passed, would give the Federal Trade Commission oversight over the new rule.  WHAT ARE THE DANGERS OF AI? FIND OUT WHY PEOPLE ARE AFRAID OF ARTIFICIAL INTELLIGENCE And there appears to be an appetite on both sides of the aisle for promoting transparency in AI content. Rep. Nancy Mace, R-S.C., one of the GOP’s leading voices on AI in the House, said Torres bill was not the ""best solution"" but agreed that Americans need to be informed if the content they are viewing, particularly as the 2024 presidential cycle heats up, is real or fake.  UNITED STATES - FEBRUARY 7: Rep. Nancy Mace, R-S.C., is seen in the Capitol Visitor Center after a meeting of the House Republican Conference on Tuesday, February 7, 2023. (Tom Williams/CQ-Roll Call, Inc via Getty Images) ""AI has the ability to revolutionize the way we live and can be a valuable tool in our arsenal for national security. However, as we continue to witness the rapid advancements in artificial intelligence, it is crucial that we prioritize transparency and accountability,"" Mace told Fox News Digital."" ""The American people deserve to know when they are interacting with AI-generated content, especially in politics where there is an easy ability to manufacture content which can be used to mislead people,"" she added. GET READY FOR RIGHTWINGGPT AND LEFTWINGGPT ""While this particular bill may not be the best solution, by requiring a disclaimer for AI content, we empower users to make informed decisions about the information they consume."" ChatGPT artificial intelligence chatbot app logo on a cellphone screen.  (iStock) AI generated content has already had a test-run in the current election season. Former President Donald Trump shared an AI-made video parodying Florida Gov. Ron DeSantis’ 2024 announcement on Twitter Spaces.  CLICK HERE TO GET THE FOX NEWS APP Rather than the thousands of listeners who tuned in to hear DeSantis speak, Trump’s video included DeSantis with guests Dick Cheney, Adolph Hitler, the FBI, George Soros and others.  And just last month, an AI-made image depicting the Pentagon suffering an explosion went viral on the internet and even appeared to cause a brief dip in the stock market. Elizabeth Elkind is a politics reporter for Fox News Digital.  Get the latest updates from the 2024 campaign trail, exclusive interviews and more Fox News politics content. You've successfully subscribed to this newsletter! This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper."
25,https://www.cbsnews.com/minnesota/news/how-concerned-should-we-be-about-extinction-from-ai,"Watch CBS News By Jeff Wagner June 5, 2023 / 11:14 PM / CBS Minnesota MINNEAPOLIS – An ominous warning about the future of artificial intelligence, and the human race, has some people wondering what they should believe. Key minds that helped create AI, or artificial intelligence, are now worried it could lead to humanity's extinction at the pace it's developing. Concerns can drift away on an afternoon at the beach at Bde Maka Ska in Minneapolis.  ""I can kind of breathe,"" said Karin Coughlin as she approached the sand.  ""I get actually super present with myself,"" said Rumay Ali as he walked by with a friend. Naturally in a place of relaxation, we decided to ask them if they were worried about AI. ""I don't think it's like a flat yes or no for me,"" Coughlin said. She recently used Chat GPT, an AI chatbot, to help her develop a diet plan. ""I think to an extent there are some concerns because America is not the only country that's developing AI,"" Ali said. Perhaps the greatest concern involves humanity's future. The Center for AI Safety recently released the following statement: ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."" MORE: AI could be smarter than ""experts"" in 10 years, OpenAI CEO says Hundreds of AI developers and researchers cosigned the statement, including David Krueger, an assistant professor at the University of Cambridge in England. He's a member of the University's Computational and Biological Learning Lab. His research group focuses on Deep Learning, AI Alignment, and AI Safety. ""My focus is pretty squarely on preventing catastrophic outcomes and human extinction,"" Krueger said. What is the risk of extinction with AI?  ""We are very rapidly approaching a point at which it might actually be too late to take effective action or prevent the development and proliferation of extremely dangerous AI capability,"" he said. To put it bluntly, Krueger feels it's possible that humanity as we know it might not exist by the end of the century, if not sooner. The Center for AI Safety lists several risks associated with AI on its website. Weaponization is one of them.  ""AIs could be used by malicious actors to design novel bioweapons more lethal than natural pandemics,"" Dan Hendryks, the director for the Center for AI Safety, told CBS News. ""Alternatively, malicious actors could intentionally release rogue AI that actively attempt to harm humanity. If such an AI was intelligent or capable enough, it may pose significant risk to society as a whole."" Another risk is enfeeblement. That's when tasks and jobs are increasingly given to machines. That could make humans less useful or necessary for a functional society, while also becoming heavily dependent on machines. The thought of extinction can weigh heavy, from the average person to AI developers. ""It's a very emotionally difficult thing to confront, as actually both Geoffrey Hinton and Yoshua Bengio have remarked recently,"" Krueger said. Hinton and Bengio are considered two ""godfathers"" of AI. Both of them cosigned the warning statement. Hinton is a computer scientist who recently retired from Google. He said he spent 50 years trying to make models on computers that can learn in a way that a brain learns. ""My epiphany was a couple of months ago. I suddenly realized that maybe the computer models we have now are actually better than the brain. And if that's the case, then maybe quite soon they'll be better than us. So that the idea of superintelligence, instead of being something in the distant future might come much sooner than I expected,"" Hinton said. Fictional books and movies have long predicted a version of this outcome, playing on a fear of AI becoming smarter than humans and taking over the world. ""I'm nervous and I'm also aware that human beings are just afraid of the unknown. So I don't want fear to be how I engage with AI,"" Coughlin said. ""[AI is] a very, very powerful tool. And I think when a powerful tool is used in a wrong way, it can be very dangerous,"" Ali said, who thinks if extinction occurs, it won't be a direct result of AI, but rather those who control it. ""It's going to be because of greed, selfishness."" MORE: U of M grad's Whale Seeker named among world's top 10 AI projects for sustainable development University of St. Thomas Professor Dr. Mangeet Rege specializes in software engineering and data science. He feels the extinction warning is over the top, but he agrees more oversight is necessary. ""I believe there should not be a complete pause on AI research. There needs to be freedom in terms of what you can develop with AI, but there needs to be regulation on when you deploy it,"" Rege said. For Krueger, he feels trying to better understand the AI that's operating right now and how to control it should be prioritized. ""We don't need to rush to be making smarter, more powerful systems as people are still doing,"" Krueger said. ""There's a lot of ways that we can use the systems we have for socially beneficial applications, and I'd like to see a lot more focus on that."" What can the average person do to help prevent possible extinction from AI if they are concerned? It starts with understanding the risk and doing research. The next step would be raising awareness, said Krueger, specifically to those who hold power like government officials. ""There's a large political aspect to this problem, because we're talking about regulation, we're talking about international cooperation. And we need to make sure that politicians and leaders understand that this is a serious concern and something that is a priority for us,"" he said. ""We've seen what happens there with things like climate change that I think we don't want this to be another climate change. We want to be taking the steps now to make the world safe from advanced AI systems."" Click here to learn more about concerns related to AI and reducing the risks of extinction. Jeff Wagner joined the WCCO-TV team in November 2016 as a general assignment reporter, and now anchors WCCO's Saturday evening newscasts. Although he's new to Minnesota, he's called the Midwest home his entire life. First published on June 5, 2023 / 11:14 PM © 2023 CBS Broadcasting Inc. All Rights Reserved. ©2023 CBS Broadcasting Inc. All Rights Reserved."
26,https://www.theguardian.com/technology/2023/jun/05/ai-could-outwit-humans-in-two-years-says-uk-government-adviser,"Exclusive: party calls for developers without a licence to be barred from working on advanced AI tools The UK should bar technology developers from working on advanced artificial intelligence tools unless they have a licence to do so, Labour has said. Ministers should introduce much stricter rules around companies training their AI products on vast datasets of the kind used by OpenAI to build ChatGPT, Lucy Powell, Labour’s digital spokesperson, told the Guardian. Her comments come amid a rethink at the top of government over how to regulate the fast-moving world of AI, with the prime minister, Rishi Sunak, acknowledging it could pose an “existential” threat to humanity. One of the government’s advisers on artificial intelligence also said on Monday that humanity could have only two years before AI is able to outwit people, the latest in a series of stark warnings about the threat posed by the fast-developing technology. Powell said: “My real point of concern is the lack of any regulation of the large language models that can then be applied across a range of AI tools, whether that’s governing how they are built, how they are managed or how they are controlled.” She suggested AI should be licensed in a similar way to medicines or nuclear power, both of which are governed by arms-length governmental bodies. “That is the kind of model we should be thinking about, where you have to have a licence in order to build these models,” she said. “These seem to me to be the good examples of how this can be done.” The UK government published a white paper on AI two months ago, which detailed the opportunities the technology could bring, but said relatively little about how to regulate it. Since then, a range of developments, including advances in ChatGPT and a series of stark warnings from industry insiders, have caused a rethink at the top of government, with ministers now hastily updating their approach. This week Sunak will travel to Washington DC, where he will argue that the UK should be at the forefront of international efforts to write a new set of guidelines to govern the industry. Labour is also rushing to finalise its own policies on advanced technology. Powell, who will give a speech to industry insiders at the TechUK conference in London on 6 June, said she believed the disruption to the UK economy could be as drastic as the deindustrialisation of the 1970s and 1980s. Keir Starmer, the Labour leader, is expected to give a speech on the subject during London Tech Week next week. Starmer will hold a shadow cabinet meeting in one of Google’s UK offices next week, giving shadow ministers a chance to speak to some of the company’s top AI executives. Powell said that rather than banning certain technologies, as the EU has done with tools such as facial recognition, she thought the UK should focus on regulating the way in which they are developed. Products such as ChatGPT are built by training algorithms on vast banks of digital information. But experts warn that if those datasets contain biased or discriminatory data, the products themselves can show evidence of those biases. This could have a knock-on effect, for example, on employment practices if AI tools are used to help make hiring and firing decisions. Powell said: “Bias, discrimination, surveillance – this technology can have a lot of unintended consequences.” Sign up to First Edition Archie Bland and Nimo Omer take you through the top stories and what they mean, free every weekday morning after newsletter promotion She argued that by forcing developers to be more open about the data they are using, governments could help mitigate those risks. “This technology is moving so fast that it needs an active, interventionist government approach, rather than a laissez-faire one.” Matt Clifford, the chair of the Advanced Research and Invention Agency, which the government set up last year, said on Monday that AI was evolving much faster than most people realised. He said it could already be used to launch bioweapons or large-scale cyber-attacks, adding that humans could rapidly be surpassed by the technology they had created. Speaking to TalkTV’s Tom Newton Dunn, Clifford said: “It’s certainly true that if we try and create artificial intelligence that is more intelligent than humans and we don’t know how to control it, then that’s going to create a potential for all sorts of risks now and in the future. So I think there’s lots of different scenarios to worry about but I certainly think it’s right that it should be very high on the policymakers’ agendas.” Asked when that could happen, he added: “No one knows. There are a very broad range of predictions among AI experts. I think two years will be at the very most sort of bullish end of the spectrum.”"
27,https://www.csoonline.com/article/3698529/department-of-defense-ai-principles-have-a-place-in-the-ciso-s-playbook.html,"By Christopher Burgess Artificial intelligence has zoomed to the forefront of the public and professional discourse — as have expressions of fear that as AI advances, so does the likelihood that we will have created a variety of beasts that threaten our very existence. Within those fears also lay worries about the responsibilities of those who create the large language models (LLM) and engines that harvest the data that feed them to do so in an ethical manner. To be frank, I hadn’t given the matter much thought until I was triggered by a recent discussion around the need for “responsible and ethical AI” which occurred amidst the constant blast that AI is evil personified or conversely that it is some holy grail. I went away and began digging in and found the US Department of Defense (DoD) has a framework that it has used and shared publicly since early 2020 that comprises five principles that lay out what artificial intelligence should look like: Demonstrating a significant amount of prescience, Air Force Lt. General Jack Shanahan, then head of the Joint Artificial Intelligence Center (since integrated into the Chief Digital and Artificial Intelligence Office in 2022 led by Dr. Craig Martell, chief digital and artificial intelligence officer) noted in the context of the military’s use of AI in support of the warfighter, that “whether it does so positively or negatively depends on our approach to adoption and use. The complexity and the speed of warfare will change as we build an AI-ready force of the future. We owe it to the American people and our men and women in uniform to adopt AI ethics principles that reflect our nation's values of a free and open society."" In late 2021, the DoD published its Project Herald, which outlines the Defense Intelligence Digital Transformation Campaign Plan – 2022-2027. The plan embraces the aforementioned pillars of responsible AI and aligns perfectly with what every CISO should be addressing within their remit: people, process, and technology. So here we are in 2023, and the White House has joined in with a plethora of steps all designed to help foster the evolution of responsible AI, and not a moment too soon. In early May, the administration announced the creation of additional National AI Research Institutes (and $140 million to make it happen). The seven new institutes will join the 18 existing entities all focused on AI research.  The actions taken by the executive branch of the US government, coupled with its clear understanding that AI is a national security issue, should be easily translated by the CISO that AI is also a priority corporate security issue. How does this distill down to actionable elements which will assist the CISO who is looking at the ad copy being thrown over their transom from marketeers and trying to determine what exists and what is infamous vaporware? I submit that the CISO should take this DoD framework and run with it in their evaluation of what is being considered for inclusion in their network. No one wants to have a situation where AI-empowered tools move at machine speed, make decisions that on paper should protect the enterprise, yet end up creating consequences that may or may not be detectable. Embracing the ethical pillars of responsible AI as detailed by the DoD is not a heavy lift, though it may be an inconvenient one. All in the cybersecurity realm understand the threat that “convenience” is to security, and thus investment in “the need to absorb the inconvenience” will be one more task put upon the CISO’s already full plate. Copyright © 2023 IDG Communications, Inc. Copyright © 2023 IDG Communications, Inc."
28,https://www.forbes.com/sites/washingtonbytes/2023/06/06/to-safeguard-democracy-political-ads-should-disclose-use-of-ai/,"U.S. Representative Yvette Clarke (D-N.Y.) recently proposed a bill that would require disclosure of use of artificial intelligence (AI) in the creation of political advertisements. This is a timely bill that should garner bipartisan support and help safeguard our democracy. Recent advancements in language modeling, exemplified by the popularity of ChatGPT, and image generation, exemplified by Dall-E 2 and Midjourney, make it much easier to create texts or images that are intentionally misleading or false. Indeed, there are examples of people already using these technologies to spread fake political news in the U.S. and abroad. In early April a number of fake AI generated images of President Trump mug shots circulated online, and later that month the Republican National Committee (RNC) responded to President Biden’s re-election campaign with an AI-generated ad. In May there were accusations of AI being used in deceptive political ads in the lead up to Turkey’s elections. Why This Time is Different Fake news stories and doctored photos are not new phenomena. A well-known technique at fashion magazines is to digitally alter or “touch up” a celebrity’s appearance on a magazine cover. The goal is to drive magazine sales, and perhaps also drive publicity for the celebrity. Elections are a different matter that involve more consequential outcomes. Allegations of fake news were rampant during the 2016 and 2020 U.S. elections. Perhaps as a result, distrust of the media has increased. According to Pew Research, Republican voters have experienced a particularly large drop in trust in news organizations. While some of this may be due to some politicians talking about news outlets as “fake news” (whether actually fake or not) some is likely also due to some exposure to or experience with actual fake news stories. The decline in trust in news is troubling. As noted in recent remarks by President Obama, “over time, we lose our capacity to distinguish between fact, opinion and wholesale fiction. Or maybe we just stop caring.” Declining trust in news is coinciding with the advent of more sophisticated AI tools that can be used to create content that looks more and more realistic. So-called “deepfakes” are digitally altered photos or videos that replace one person’s likeness with somebody else’s. It has become much harder to spot these fakes. The tools to create these do not require much expertise, meaning that the barrier to entry is low for someone to create lots of hard to detect AI-manipulated or AI-generated content. Without regulation, this problem seems destined to get worse before it gets better. How Disclosure Can Help One solution is to require disclosure of the use of AI in political ads. In fact, the RNC did just this by including the disclaimer “built entirely with AI imagery” in its ad, suggesting there may be bi-partisan support for a bill on the disclosure of use of AI. Disclosing AI use should not be costly to advertisers. The technology to label content created by AI already exists, according to Hany Farid, a University of California computer scientist. In March, Google said it would include watermarks inside images created by its AI models. One issue that will need to be addressed is: what counts as “AI”? The current text of Representative Clarke’s proposed bill skirts this issue, describing it as “the use of artificial intelligence (generative AI).” Generative AI typically entails the use of image generation or large language modeling, but there isn’t, yet, a widely agreed definition. Hopefully implementation of the bill can run parallel to crafting a more precise definition of generative AI. In the words of New York University’s Julia Stoyanovich, “Until you try to regulate [Artificial Intelligence], you won’t learn how.” In other words, we have to start somewhere. Given the speed with which AI technology is advancing and the decline in trust in news, it is important to move quickly. Will disclosure of the use of AI in political ads matter to voters? It’s too early to tell. But, at least voters will have information about how the ad was created and can individually use that information to assess the advertisement. The U.S. Federal Election Commission (FEC) already requires certain disclaimers on political ads—though not yet about the use of AI—and handles enforcement through audits and investigation of complaints. The fact that the U.S. already requires various disclosures on political ads suggests we as a country believe that providing extra information to voters is an important safeguard for our democracy. Disclosing the use of AI would align current requirements with what is technologically feasible."
29,https://techcrunch.com/2023/06/06/eu-disinformation-code-generative-ai-labels/,"The European Union is leaning on signatories to its Code of Practice on Online Disinformation to label deepfakes and other AI-generated content. In remarks yesterday following a meeting with the 40+ signatories to the Code, the EU’s values and transparency commissioner, Vera Jourova, said those signed up to combat disinformation should put in place technology to recognize AI content and clearly label it to users. “The new AI technologies can be a force for good and offer new avenues for increased efficiency and creative expression. But, as always, we have to mention the dark side of this matter and they also present new risks and the potential for negative consequences for society,” she warned. “Also when it comes to the creation of and dissemination of disinformation. “Advanced chatbots like ChatGPT are capable of creating complex, seemingly well substantiated content and visuals in a matter of seconds. Image generators can create authentic looking pictures of events that never occurred. Voice generating software can imitate the voice of a person based on a sample of a few seconds. The new technologies raise fresh challenges for the fight against disinformation as well. So today I asked the signatories to create a dedicated and separate track within the code to discuss it.” ""When it comes to AI production, I don't see any right for the machines to have freedom of speech. Signatories of the EU Code of Practice against disinformation should put in place technology to recognise AI content and clearly label it to users."" — Vice-President @VeraJourova pic.twitter.com/yLVp79bqEH — European Commission (@EU_Commission) June 5, 2023 The current version of the Code, which the EU beefed up last summer — when it also confirmed it intends the voluntary instrument to become a mitigation measure that counts towards compliance with the (legally binding) Digital Services Act (DSA) — does not currently commit to identifying and labelling deepfakes. But the Commission is hoping to change that. The EU commissioner said it sees two main discussion angles for how to include mitigation measures for AI-generated content in the Code: One would focus on services that integrate generative AI, such as Microsoft’s New Bing or Google’s Bard AI-augmented search services — which should commit to building in “necessary safeguards that these services cannot be used by malicious actors to generate disinformation”. A second would commit signatories who have services with potential to disseminate AI-generated disinformation to put in place “technology to recognise such content and clearly label this to users”. Jourova said she had spoken with Google’s Sundar Pichai and been told Google has technology which can detect AI-generated text content but also that it is continuing to develop the tech to improve its capabilities. In further remarks during a press Q&A, the commissioner she said the EU wants labels for deepfakes and other AI generated content to be clear and fast — so normal users will immediately be able to understand that a piece of content they’re being presented with has been created by a machine, not a person. She also specified that the Commission wants to see platforms implementing labelling now — “immediately”. The DSA does include some provisions requiring very large online platforms (VLOPs) to label manipulated audio and imagery but Jourova said the idea to add labelling to the disinformation Code is that it can happen even sooner than the August 25 compliance deadline for VLOPs under the DSA. “I said many times that we have the main task to protect freedom of speech. But when it comes to the AI production, I don’t see any right for the machines to have freedom of speech. And so this is also coming back to the old good pillars of our law. And that’s why we want to work further on that also under the Code of Practice on the basis of this very fundamental idea,” she added. The Commission is also expecting to see action on reporting AI-generated disinformation risks next month — with Jourova saying relevant signatories should use the July reports to “inform the public about safeguards that they are putting in place to avoid the misuse of generative AI to spread disinformation”. The disinformation Code now has 44 signatories in all — which includes tech giants like Google, Facebook and Microsoft, as well as smaller adtech entities and civil society organizations — a tally that’s up from 34 who had signed to the commitments as of June 2022. However, late last month Twitter took the unusual step of withdrawing from the voluntary EU Code. EU warns Twitter over disinformation after Musk policy shifts found to boost Kremlin propaganda Other big issues Jourova noted she had raised with remaining signatories in yesterday’s meeting — urging them to take more action — included Russia’s war propaganda and pro-Kremlin disinformation; the need for “consistent” moderation and fact-checking; efforts on election security; and access to data for researchers. “There is still far too much dangerous disinformation content circulating on the platforms and too little capacities,” she warned, highlighting a long-standing complaint by the Commission that fact-checking initiatives are not comprehensively applied across content targeting all the languages spoken in EU Member States, including smaller nations. “Especially the center and eastern European countries are under permanent attack from especially Russian disinformation sources,” she added. “There is a lot to do. This is about capacities, this is about our knowledge, this is about our understanding of the language. And also understanding of the reasons why in some Member States there is the feeding ground or the soil prepared for absorption of big portion of disinformation.” Access for researchers is still insufficient, she also emphasized — urging platforms to step up their efforts on data for research. Jourova also added a few words of warning about the path chosen by Elon Musk — suggesting Twitter has put itself in the EU’s enforcement crosshairs, as a designated VLOP under the DSA. The DSA puts a legal requirement on VLOPs to assess and mitigate societal risks like disinformation so Twitter is inviting censure and sanction by flipping the bird at the EU’s Code (fines under the DSA can scale up to 6% of global annual turnover). “From August this year, our structures, which will play the role of the enforcers of the DSA will look into Twitter’s performance whether they are compliant, whether they are taking necessary measures to mitigate the risks and to take action against… especially illegal content,” she further warned. “The European Union is not the place where we want to see the imported Californian law,” she added. “We said it many times and that’s why I also want to come back and appreciate the cooperation with the… former people working in Twitter, who collaborated with us [for] several years already on Code of Conduct against hate speech and Code of Practice [on disinformation] as well. So we are sorry about that. I think that Twitter had very knowledgeable and determined people who understood that there must be some responsibility, much increased responsibility on the site of the platforms like Twitter.” Asked whether Twitter’s Community Notes approach — which crowdsources (so essentially outsources) fact-checking to Twitter users if enough people weigh in to add a consensus of context to disputed tweets — might be sufficient on its own to comply with legal requirements to tackle disinformation under the DSA, Jourova said it will be up to the Commission enforcers to assess whether or not they are compliant. However she pointed to Twitter’s withdrawal from the Code as a significant step in the wrong direction, adding: “The Code of Practice is going to be recognised as the very serious and trustworthy mitigating measure against the harmful content.” Elon Musk takes Twitter out of the EU’s Disinformation Code of Practice"
30,https://www.fool.com/investing/2023/06/06/smart-investing-how-id-allocate-1000-to-3-promisin/,"Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. You’re reading a free article with opinions that may differ from The Motley Fool’s Premium Investing Services. Become a Motley Fool member today to get instant access to our top analyst recommendations, in-depth research, investing resources, and more. Learn More Investing in artificial intelligence (AI) can be as straightforward or as complex as you want. You could spend hours digging into each company's product line and determining its potential uses, or just pick the popular stocks that are consensus winners. However, I think there's an intelligent way to play the AI investment wave: Purchase shares of a few companies spread throughout the AI supply chain. That way, your bets are spread across the board, giving you the best chance of success. If you've got $1,000 and are itching to get some AI exposure, here's how I'd do it. While AI is highly complex, the value chain required to create the software is quite simple. As with most technological investments, there is a software and hardware component. On the hardware side, data centers filled with thousands of GPUs (graphics processing units) and other computing devices are needed to help process calculations and train AI models. On the software side, data sets are used to train AI models, which can then be integrated into software and sold as an AI product from which the end user benefits. The three companies I'm choosing as promising AI stocks are Taiwan Semiconductor Manufacturing (TSM), Alphabet (GOOG 1.12%) (GOOGL 1.07%), and CrowdStrike (CRWD 1.82%). By investing in this trio, you'll cover every corner of AI mentioned above and gain some exposure to other industries. Taiwan Semiconductor supplies the chips to AMD and Nvidia, whose products then go into data centers. Because TSMC is an independent supplier, its chips go into many competing products, even if they are sourced from the same supplier. With Taiwan Semiconductor producing some of the most potent chips globally with an innovative product pipeline, it makes for a strong AI story. Alphabet is perhaps the most exposed and unexposed company on this list of AI stocks. While that may seem contradictory, it makes sense when you look at the business. Alphabet is exposed to the AI space through its massive AI toolkit, a library of data sets, as well as its cloud computing division, Google Cloud. CEO Sundar Pichai said Alphabet would be an ""AI-first"" company in 2016 and has spent a lot of capital hiring engineers to power this vision. Alphabet is striving to be an AI powerhouse and has much riding on this technology. However, 78% of Alphabet's revenue comes from advertisement sales, so AI isn't affecting the company's revenue streams. But, if Alphabet hits a home run on its AI products, it may shift the balance of the business to be more AI and less advertising. CrowdStrike represents a consumer-facing application of AI, as it uses the technology to evolve its cyber security platform continuously. This gives its clients the best chance to repel attacks, as it's sourcing the data from trillions of signals weekly. With AI becoming a huge selling point, plus the need for increased cybersecurity protection in today's environment of increasing attacks, CrowdStrike has a large runway in front of it. These three companies tell compelling stories, but how should investors split their $1,000? Of the three, CrowdStrike is likely the most risky. The company is just now reaching profitability and has a long way to go before achieving the levels mature software companies have attained. Taiwan Semiconductor is also tricky, as it is probably the least risky investment on this list due to its low valuation and wide usage. TSM PS Ratio data by YCharts However, an invasion from China would send this investment to a potential 100% loss. I think the likelihood of this is low, but it's a real risk investors must weigh. Alphabet is in the middle, and the stock is at a reasonable valuation (27 times earnings). But there are questions about how well it's executing in the AI space. I think these fears are way overblown. With that in mind, I think deploying $450 to Alphabet, $350 to Taiwan Semiconductor, and $200 to CrowdStrike would be a wise way to spread out the money. That way, if the riskier investments don't work out, the most likely winner still has the majority of the capital. However, if CrowdStrike explodes higher, that small investment basis will grow to a nice lump of money. That's how I'd invest $1,000 into three AI stocks. While multiple companies can be plugged into this trio's slot, the mindset of spreading out investments across all parts of AI should always be considered. Suzanne Frey, an executive at Alphabet, is a member of The Motley Fool’s board of directors. Keithen Drury has positions in Alphabet, CrowdStrike, and Taiwan Semiconductor Manufacturing. The Motley Fool has positions in and recommends Advanced Micro Devices, Alphabet, CrowdStrike, Nvidia, and Taiwan Semiconductor Manufacturing. The Motley Fool has a disclosure policy. *Average returns of all recommendations since inception. Cost basis and return based on previous market day close. Invest better with The Motley Fool. Get stock recommendations, portfolio guidance, and more from The Motley Fool's premium services. Making the world smarter, happier, and richer. Market data powered by Xignite."
31,https://www.ft.com/content/6cc9c9d5-7006-43c9-8cb0-147a74031402,"We use cookies and other data for a number of reasons, such as keeping FT Sites reliable and secure, personalising content and ads, providing social media features and to analyse how our Sites are used. George Steer in London We’ll send you a myFT Daily Digest email rounding up the latest US equities news every morning. Retail investors have increased their exposure to technology stocks after missing out on the market rally driven this year by artificial intelligence, with many having preferred cash or low-risk money market funds. Net purchases of US stocks by retail investors hit almost $1.5bn on May 30 and 31, the highest daily figures in three months, data from VandaTrack shows. Tech stocks were among the main beneficiaries, at the end of a month in which retail interest in AI-associated companies began to broaden and benefit the likes of Palantir, Marvell Technology and UiPath, as well as bigger names like Nvidia and Microsoft. After weeks on the sidelines, individual investors are “starting to chase the tech rally”, said Marco Iachini, VandaTrack’s vice-president. “Fear of missing out looks to be kicking in.” Sticky inflation, a crisis of confidence in the US regional banking sector and uncertainty in the run-up to last week’s debt ceiling agreement meant smaller investors had cut their equity allocation in favour of bonds and low-risk money market funds offering yields comparable to those on stocks, according to Barclays analysts. However, VandaTrack said retail purchases of money market ETFs had declined in recent weeks as “mom and pop investors” had begun shifting into riskier assets, lured by the Nasdaq Composite’s 27 per cent gain so far this year. Vanguard’s Information Technology ETF and Fidelity’s MSCI Information Technology Index recorded chunky inflows last week and have risen to their highest levels in more than a year, suggesting a “re-emergence of the US retail impulse into tech”, said analysts at JPMorgan. The bank said “younger” retail investors who tend to invest in individual stocks appear to be leading the charge, with appetite for exchange-traded individual equity options having “increased markedly” in recent weeks. Reddit’s popular Wall Street Bets is, meanwhile, awash with comments from members who missed out on the tech rally or are baulking at how high some shares have risen. A widely shared meme shows Nvidia literally carrying the US stock market on its back. “Been sitting out the market for a bit but seeing these AI gains is killing me,” said one user. Retail investors have not always been so risk-averse. Shares in struggling video games retailer GameStop rose more than 2,000 per cent in a month in 2021, at the height of the meme stock craze. But so far the only GameStop-like stock attracting retail capital is C3.ai, a US-based software provider whose shares have jumped 195 per cent this year. It fell 14 per cent on Thursday, however, after revenue guidance for the 2024 fiscal year came in well below analysts’ expectations. Nvidia, by comparison, has risen 170 per cent in the year to date and now trades at a price-to-earnings ratio of 190, up from 47 at the start of November. Equity valuations and “a turnround in risk sentiment around AI-related stocks” were now the “key downside risk” to the wider US stock market, said Peter Garnry, head of quantitative strategies at Saxo Bank. Until last week, the gap between retail flows and the stock market’s bumper performance implied institutional players such as hedge funds and systematic traders were the primary source of demand for AI stocks, VandaTrack said. Typifying many professional investors’ optimism, Geir Lode, head of global equities at Federated Hermes, describes AI as “the next supercharged growth area”. Analysts at Morgan Stanley said they were “believers in the AI revolution” but that the technology would not be able to “stop or even cushion” a wider earnings recession later this year."
32,https://www.usf.edu/news/2023/usf-to-enhance-skillset-of-soaring-tech-workforce-with-new-ai-certificate-program.aspx,"University of South Florida Tampa Bay has become a magnet for tech start-ups – an industry growing more quickly than the talent pool, especially in artificial intelligence (AI). In response to this demand, the University of South Florida has launched a graduate certificate program in AI for working professionals in the technology sector interested in enhancing their skillset. “As Tampa keeps growing in economic development and becoming a bigger technology hub, USF had the foresight to put together this program to upscale the current workforce, not just here, but everywhere,” said Distinguished University Professor Sudeep Sarkar, chair of the Department of Computer Science and Engineering. Participants in the Artificial Intelligence Graduate Certificate are required to take four online courses that provide detailed information about crucial domains in AI, how to modify AI tools and strengthen programs, such as ChatGPT. The certificate also serves as a pathway for those interested in enrolling in the USF Master of Science in Data Intelligence, which launches in the fall. According to Dice, Tampa is among the fastest-growing U.S. tech hubs for IT talent with an average salary of $120,900. The Tampa Bay Economic Development Council reports the number of businesses in the IT industry in the region has increased 27 percent during the past five years, growing from 13,400 businesses to nearly 17,000. It's projected the industry will continue to grow in Tampa with more than 3,700 jobs added by 2027. “Artificial Intelligence is an emerging field that will have a transformational impact on business,” said Craig J. Richard, president and CEO of the Tampa Bay Economic Development Council. “It’s wonderful that USF is being proactive in offering this additional training to our local tech professionals. This certificate program will strengthen our community’s competitiveness as we build our tech talent pipeline further to meet the demand for these new specialized roles.” The Artificial Intelligence Graduate Certificate launches in the fall. An informational session is scheduled for June 9. Registration information is available here. The deadline to register for the course is Aug. 1. More information can be found here. Return to article listing Research and Innovation, Student Success, University News College of Engineering, MyUSF Learn more about USF's journey to Preeminence by viewing Newsroom articles from past years. June 1, 2023 May 27, 2023 May 24, 2023 May 23, 2023 More USF in the News 4202 E. Fowler AvenueTampa, FL 33620, USA813-974-2011 Copyright © 2023, University of South Florida. All rights reserved. This website is maintained by USF News."
33,https://techcrunch.com/2023/06/05/sensi-ai-and-flint-capital-speak-on-developing-and-deploying-ai-solutions-in-healthcare/,"Remotely monitoring patients without violating their privacy is a challenging task. But one co-founder believes that she’s cracked the code. On a recent episode of TechCrunch Live, TC’s weekly event designed to help founders build better venture-backed businesses, Romi Gubes, the CEO of Sensi.AI, spoke about how she built a company that uses audio-based AI software to detect and predict anomalies that can impact the health of those receiving in-home care. Romi, a software engineer by training who’s worked at Fortune 500 companies including Cisco, Dell and Vonage, says that she was inspired to found Sensi.AI after an episode of abuse in her daughter’s daycare center. “It was one of the things in life that really changes your life,” she said. “And I wanted to leverage my technological background in order to help those vulnerable individuals be safe in any kind of care environment.” That turned her on to the massive shortage of in-house care professionals in the U.S., as well as the effects that “aging in place” without the proper infrastructural support can have. “As most of you know, as time goes by, there are more and more older adults and less than less younger people that can potentially take care of them,” Romi said. “Very soon, I understood how big the pain in the senior care industry is.” Image Credits: Sensi.AI Sensi.AI, founded in 2018, grew rather quickly, scaling today to 70 employees across two countries — the U.S. and Israel — and to customers in 37 states serving thousands of individuals. Along the way, Sensi.AI raised $25 million from investors including Sergey Gribov, a general partner at Flint Capital and a board member at Sensi.AI, who joined for the TC Live discussion. Bolstered by the pandemic, the market for remote care monitoring solutions is quite large. So how did Sensi.AI manage to stand out from the crowd? Romi attributes it to the company’s differentiated technology, which uses a combination of AI and audio monitoring to detect key events in and around patients’ environments. Sensi.AI spent years collecting data from the field to train its AI system. To date, the company has captured more than 10 million caregiver interactions from tens of thousands of people throughout the U.S., Romi claims. “For example, we know to detect if a caregiver has a specific problem with transitioning the older adult from bed to the chair, where this is a huge risk factor for both of them, actually,” she explained. “We’re more focused on the prevention layer in order to really allow professionals to act before something’s happening.” But what about privacy — both the privacy of the patients and of the caregivers? Romi pointed out that Sensi.AI doesn’t use cameras for monitoring, unlike some of its competitors. On top of that, the system is compliant with HIPAA — the major medical records privacy bill in the U.S. — and anonymizes data so that the audio data isn’t tied to any individual being monitored. That contributed to Sensi.AI’s funding success as well, according to Gribov. But the pandemic arguably played a larger role. “When the pandemic hit, many caregivers weren’t able to get to the homes of the older adult and really serve them, and older adults stayed at home by themselves,” Romi said. “And this is where the need for solutions such as Sensi was very, very clear.” Image Credits: Sensi.AI One might assume that Sensi.AI’s grand ambition is to replace care workers entirely. But Romi asserts that this isn’t the case. In fact, she thinks it isn’t feasible from a technical standpoint — and won’t be for the foreseeable future. She hopes, rather, that Sensi.AI can grow into a care tool that clinicians — and even parents of older adults — can use to keep track of what’s going on in the home of a vulnerable patient. “We can make their work much more efficient, and to get them make better decisions,” Romi said."
34,https://www.fool.com/investing/2023/06/06/3-ai-chip-stocks-much-cheaper-than-nvidia/,"Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. You’re reading a free article with opinions that may differ from The Motley Fool’s Premium Investing Services. Become a Motley Fool member today to get instant access to our top analyst recommendations, in-depth research, investing resources, and more. Learn More There has been a lot of talk about artificial intelligence and the impact it will have on humanity and the economy ever since the release of ChatGPT last November. But that enthusiasm began to translate into eye-opening financial results with Nvidia's (NVDA -0.40%) blowout guidance on May 24 -- roughly six months later. Nvidia's guidance seemed to validate the recent bullishness for semiconductors and cloud platforms generally, propelling many such stocks significantly higher. However, for some, Nvidia may now look at tad pricey, at over 200 times earnings and 35 times sales. Image source: Getty Images. Fortunately, there are other data center-oriented chip stocks that will benefit handsomely from the AI trend, and they come at a cheaper valuation -- like the following three stocks. I group these two companies together because they have some overlap in terms of the products they offer, including Ethernet switching, optical component interfaces, storage controllers, and customized application-specific integrated chips (ASICs). All of these chips will be essential in building out next-generation data centers geared for AI applications. On the company's recent conference call with analysts, Marvell (MRVL -3.57%) CEO Matt Murphy said: ... [L]eading cloud data centers connect thousands of these systems in a single cluster to provide maximum scalability for their customers, with each of these systems capable of driving tens of terabits of network traffic... And in order to create the largest possible cluster sizes at data center scale, these connections need to be able to operate over increasingly long distances. These clusters require a staggering amount of high-bandwidth connectivity, all of which needs to be provided at ultra-low latency and high reliability and within a reasonable power envelope. In other words, along with GPU-based computing systems, networking and switching requirements are also about to skyrocket, which will play into the hands of Marvell's PAM4 digital signal processing (DSP) interfaces, data center interconnect, and Teralynx 10 Ethernet switching platforms. Meanwhile, Broadcom's (AVGO -1.20%) current open architecture Tomahawk and Jericho chips for merchant switching and routing, currently deployed in data centers and telecom platforms, can be easily adapted and used for the intense networking needs of AI data centers. In addition to these routing, switching, and optical interconnect platforms, both companies also compete in the arena of custom ASICs, which many large cloud companies are using with their custom-designed AI accelerators. JPMorgan Chase analyst Harlan Sur believes this will be a fast-growing market, with Broadcom having a leading 35% share, and Marvell having roughly 15% ASIC share. Yet, Sur sees both as poised to benefit from custom AI design wins in the years ahead. Overall, both companies recently stated they see their AI-related revenue doubling this year, with another doubling next year. Broadcom notes AI-related products represent about 15% of its semiconductor business today, but remember, Broadcom also has a software segment that accounts for about 22% of revenues, so AI chips account for a little less than that 15% overall. Meanwhile, for Marvell, AI-related products only accounted for $200 million of revenue last year out of $5.9 billion in revenue, or about 3.3%; however, Marvell obviously sees very strong growth here. Moreover, management isn't accounting for the knock-on benefits for its large storage control portfolio in its AI growth projections, which should benefit from increased data usage. Thus, the doubling in 2023 and 2024 AI revenue could understate the effect of AI on Marvell's business. Broadcom is the more mature and profitable company, trading at just 18 times forward earnings with a dividend of 2.3%. Meanwhile, Marvell is smaller, at just a $51.8 billion market cap to Broadcom's $338 billion. Marvell isn't currently profitable on a generally accepted accounting principles (GAAP) basis, but is free cash flow profitable, due to high stock-based compensation. The stock trades at 40 times this year's earnings expectations, and about 25 times 2025 analyst estimates, as analysts expect profitability to inflect upwards. Meanwhile, Microchip Technology (MCHP -2.50%) also isn't the first chip stock many think about when they mention AI. This could be due to the company's microcontroller and analog focus, which are very diverse businesses beyond data center infrastructure. However, Microchip's solutions are important to the data center, and management has identified the data center as one of six big megatrends it cited and attacked as part of its Microchip 3.0 strategy launched two years ago. In fact, Microchip's data center segment is the largest of the six megatrends cited, at 17.5% of sales, up from 14.2% just two years ago. Overall, Microchip's high-growth megatrend segments have grown from 34% to 45% of the business in just two years, growing at two times the rate of the overall company. While not every Microchip data center product plays a role in AI, many do, including its PCIe switches, which connect servers and individual chips to each other as opposed to whole-rack systems like Ethernet does. Other AI-related products include smart memory interfaces, data center interconnect Ethernet products, power and silicon carbide chips, and others. All should benefit from increasing compute intensity, and Microchip's focus on sustainability and electrification should help mitigate the heat generated by massive AI clusters going forward, too. Microchip is the cheapest of the three stocks, trading at just 11.5 this year's earnings estimates. While its AI impact may be more muted than the other two, Microchip's low valuation and increasing cash returns give it a lower hurdle to clear. Meanwhile, the rest of its portfolio is geared heavily toward industrial, IoT, and automotive chips, which should also have resilient growth as the electrification and automation trends sustain through this decade. JPMorgan Chase is an advertising partner of The Ascent, a Motley Fool company. Billy Duberstein has positions in Broadcom and Microchip Technology. His clients may own shares of the companies mentioned. The Motley Fool has positions in and recommends JPMorgan Chase and Nvidia. The Motley Fool recommends Broadcom and Marvell Technology. The Motley Fool has a disclosure policy. *Average returns of all recommendations since inception. Cost basis and return based on previous market day close. Invest better with The Motley Fool. Get stock recommendations, portfolio guidance, and more from The Motley Fool's premium services. Making the world smarter, happier, and richer. Market data powered by Xignite."
35,https://www.fool.com/investing/2023/06/06/my-top-hidden-gem-ai-stock-to-buy-in-june-deere/,"Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. You’re reading a free article with opinions that may differ from The Motley Fool’s Premium Investing Services. Become a Motley Fool member today to get instant access to our top analyst recommendations, in-depth research, investing resources, and more. Learn More A lot of the investment discussion surrounding artificial intelligence (AI) and automation has focused on the semiconductor industry and the tech sector. But the industrial sector is flush with opportunities -- particularly through companies that are using AI and automation to shake up old ways, save their customers money, and drive efficiency.  Deere (DE -2.00%) has spent decades investing in these burgeoning trends. And in many ways, Deere stock is an AI play that's hiding in plain sight. At the CES conference in 2022, Deere wowed investors when it unveiled its autonomous tractor. But Deere's automation offering is so much more than that product alone. Here's a look at some of the ways Deere is automating farm operations, and why the company is poised to lead technological adoption in the agriculture industry. Image source: Deere. During his talk at CES 2022, Deere Vice President of Automation and Autonomy Jorge Heraud discussed a few of the ways that Deere's tractors help farmers with the planting process. One way is by automating the path of the tractor so the farmer can plant straight rows. Another is by automating seed placement to maximize crop yield and the number of seeds that can be planted in a given acreage. By putting cameras on sprayers, Deere helps farmers detect crops versus weeds. The sprayers then use just the right amount of product needed to support crop growth or get rid of weeds. Heraud said that this spraying technology saves farmers 80% on product costs, which improves farm sustainability. Yet another feature is using cameras during harvest time to monitor crop conditions and then using AI to automatically adjust settings to improve harvesting efficiency. This planting example gives you an idea of Deere's big-picture strategy and the importance that AI and automation will have in making further improvements. The initial purpose of tractors was to replace oxen and horses and make work easier. Much of the growth throughout Deere's rich history has centered around making tractors stronger and more versatile to replace physical tasks. But many of the improvements in today's tractors stem from data-driven insights that were previously unattainable. In this vein, today's tractors are integral in providing the brains and the brawn on a farm. The value proposition for Deere is giving farmers the tools they need to do the most with the resource they have. This could mean saving money on fertilizer, maximizing production, avoiding unnecessary fuel expenses, and saving farmers precious time by automating mundane tasks. Part of what makes the Deere investment thesis compelling is that the company is producing record results while also sporting ultra-long-term potential to leverage AI and automation in the agriculture, construction, and forestry industries for decades to come. Deere is on pace for a record fiscal 2023 following a record 2022 and a record 2021. The company's fiscal 2023 guidance of $9.25 billion to $9.5 billion in net income is staggering when you consider it's more than the company made in the three years from fiscal 2018 to fiscal 2020.  Deere stock is currently enjoying a declining price-to-earnings (P/E) ratio -- signaling that profit growth is outpacing the stock price's appreciation. DE PE Ratio data by YCharts Granted, Deere is a cyclical stock. So its P/E ratio tends to be low during growth periods and higher during downturns. Still, Deere's 12.5 P/E ratio indicates that profits could be cut in half and Deere would still sport a P/E ratio close to the S&P 500 average of 22.2.  Part of the reason Deere has a low P/E ratio is that it uses so much of its excess earnings to buy back stock. Like Apple, Deere has leveraged the two-pronged benefit of growing earnings and reducing its outstanding share count -- which permanently boosts earnings per share by decreasing the denominator of the equation. Deere reduced its shares outstanding by 6.4% in the last three years and 23.4% in the last 10 years. Passive income-orientated investors may prefer to see excess earnings go toward paying more dividends. But it's actually better for long-term investors if a company buys back its stock instead of paying dividends if it proves it can get shareholders a better return on that capital over time. Given that Deere stock has outperformed the S&P 500 and the Nasdaq Composite over the last three, five, seven, and 10 years, it's safe to say that investors have been handsomely rewarded by its decision to invest in research and development and buy back stock. Deere is one of the few legacy companies that blends present-day success with long-term upside. For example, there are plenty of oil and gas companies that booked record years in 2022. But those companies are undergoing an energy transition and will need to supplement profits from fossil fuels with cleaner alternatives in the future. On the flip side, there are many unproven companies that promise paradigm-shifting technologies but lack the profits to fuel that growth with cash and must instead rely on capital markets for dry powder. Deere has the unique advantage of being the industry leader in agriculture machinery. This was not only one of civilization's first industries, but it continues to be an industry civilization simply cannot live without. If you're interested in AI and automation but have a low to moderate risk tolerance, you may be more interested in companies like Deere that have a track record for organic growth and the long-term tailwinds that can drive decades of value creation instead of a riskier company whose AI prospects are feast or famine. Daniel Foelber has the following options: long January 2024 $400 calls on Deere. The Motley Fool has positions in and recommends Apple. The Motley Fool recommends Deere. The Motley Fool has a disclosure policy. *Average returns of all recommendations since inception. Cost basis and return based on previous market day close. Invest better with The Motley Fool. Get stock recommendations, portfolio guidance, and more from The Motley Fool's premium services. Making the world smarter, happier, and richer. Market data powered by Xignite."
36,https://www.theatlantic.com/magazine/archive/2023/07/generative-ai-human-culture-philosophy/674165/,"We need a cultural and philosophical movement to meet the rise of artificial superintelligence. On July 13, 1833, during a visit to the Cabinet of Natural History at the Jardin des Plantes, in Paris, Ralph Waldo Emerson had an epiphany. Peering at the museum’s specimens—butterflies, hunks of amber and marble, carved seashells—he felt overwhelmed by the interconnectedness of nature, and humankind’s place within it. Check out more from this issue and find your next story to read. The experience inspired him to write “The Uses of Natural History,” and to articulate a philosophy that put naturalism at the center of intellectual life in a technologically chaotic age—guiding him, along with the collective of writers and radical thinkers known as transcendentalists, to a new spiritual belief system. Through empirical observation of the natural world, Emerson believed, anyone could become “a definer and map-maker of the latitudes and longitudes of our condition”—finding agency, individuality, and wonder in a mechanized age. America was crackling with invention in those years, and everything seemed to be speeding up as a result. Factories and sugar mills popped up like dandelions, steamships raced to and from American ports, locomotives tore across the land, the telegraph connected people as never before, and the first photograph was taken, forever altering humanity’s view of itself. The national mood was a mix of exuberance, anxiety, and dread. From the June 2018 issue: Henry A. Kissinger on AI and how the Enlightenment ends The flash of vision Emerson experienced in Paris was not a rejection of change but a way of reimagining human potential as the world seemed to spin off its axis. Emerson’s reaction to the technological renaissance of the 19th century is worth revisiting as we contemplate the great technological revolution of our own century: the rise of artificial superintelligence. Even before its recent leaps, artificial intelligence has for years roiled the informational seas in which we swim. Early disturbances arose from the ranking algorithms that have come to define the modern web—that is, the opaque code that tells Google which results to show you, and that organizes and personalizes your feeds on social platforms like Facebook, Instagram, and TikTok by slurping up data about you as a way to assess what to spit back out. Now imagine this same internet infrastructure but with programs that communicate with a veneer of authority on any subject, with the ability to generate sophisticated, original text, audio, and video, and the power to mimic individuals in a manner so convincing that people will not know what is real. These self-teaching AI models are being designed to become better at what they do with every single interaction. But they also sometimes hallucinate, and manipulate, and fabricate. And you cannot predict what they’ll do or why they’ll do it. If Google’s search engine is the modern-day Library of Alexandria, the new AI will be a mercurial prophet. From the May 2018 issue: The era of fake video begins Generative artificial intelligence is advancing with unbelievable speed, and will be applied across nearly every discipline and industry. Tech giants—including Alphabet (which owns Google), Amazon, Meta (which owns Facebook), and Microsoft—are locked in a race to weave AI into existing products, such as maps, email, social platforms, and photo software. The technocultural norms and habits that have seized us during the triple revolution of the internet, smartphones, and the social web are themselves in need of a thorough correction. Too many people have allowed these technologies to simply wash over them. We would be wise to rectify the errors of the recent past, but also to anticipate—and proactively shape—what the far more radical technology now emerging will mean for our lives, and how it will come to remake our civilization. Corporations that stand to profit off this new technology are already memorizing the platitudes necessary to wave away the critics. They’ll use sunny jargon like “human augmentation” and “human-centered artificial intelligence.” But these terms are as shallow as they are abstract. What’s coming stands to dwarf every technological creation in living memory: the internet, the personal computer, the atom bomb. It may well be the most consequential technology in all of human history. People are notoriously terrible at predicting the future, and often slow to recognize a revolution—even when it is already under way. But the span of time between when new technology emerges and when standards and norms are hardened is often short. The Wild West, in other words, only lasts for so long. Eventually, the railroads standardize time; incandescent bulbs beat out arc lamps; the dream of the open web dies. The window for effecting change in the realm of AI is still open. Yet many of those who have worked longest to establish guardrails for this new technology are despairing that the window is nearly closed. Generative AI, just like search engines, telephones, and locomotives before it, will allow us to do things with levels of efficiency so profound, it will seem like magic. We may see whole categories of labor, and in some cases entire industries, wiped away with startling speed. The utopians among us will view this revolution as an opportunity to outsource busywork to machines for the higher purpose of human self-actualization. This new magic could indeed create more time to be spent on matters more deserving of our attention—deeper quests for knowledge, faster routes to scientific discovery, extra time for leisure and with loved ones. It may also lead to widespread unemployment and the loss of professional confidence as a more competent AI looks over our shoulder. Annie Lowrey: Before AI takes over, make plans to give everyone money Government officials, along with other well-intentioned leaders, are groping toward ethical principles for artificial intelligence—see, for example, the White House’s “Blueprint for an AI Bill of Rights.” (Despite the clunky title, the intention is for principles that will protect human rights, though the question of civil rights for machines will eventually arise.) These efforts are necessary but not enough to meet the moment. We should know by now that neither the government’s understanding of new technologies nor self-regulation by tech behemoths can adequately keep pace with the speed of technological change or Silicon Valley’s capacity to seek profit and scale at the expense of societal and democratic health. What defines this next phase of human history must begin with the individual. Just as the Industrial Revolution sparked transcendentalism in the U.S. and romanticism in Europe—both movements that challenged conformity and prioritized truth, nature, and individualism—today we need a cultural and philosophical revolution of our own. This new movement should prioritize humans above machines and reimagine human relationships with nature and with technology, while still advancing what this technology can do at its best. Artificial intelligence will, unquestionably, help us make miraculous, lifesaving discoveries. The danger lies in outsourcing our humanity to this technology without discipline, especially as it eclipses us in apperception. We need a human renaissance in the age of intelligent machines. In the face of world-altering invention, with the power of today’s tech barons so concentrated, it can seem as though ordinary people have no hope of influencing the machines that will soon be cognitively superior to us all. But there is tremendous power in defining ideals, even if they ultimately remain out of reach. Considering all that is at stake, we have to at least try. From the June 2023 issue: Never give artificial intelligence the nuclear codes Transparency should be a core tenet in the new human exchange of ideas—people ought to disclose whenever an artificial intelligence is present or has been used in communication. This ground rule could prompt discipline in creating more-human (and human-only) spaces, as well as a less anonymous web. Any journalist can tell you that anonymity should be used only as a last resort and in rare scenarios for the public good. We would benefit from cultural norms that expect people to assert not just their opinions but their actual names too. Now is the time, as well, to recommit to making deeper connections with other people. Live videochat can collapse time and distance, but such technologies are a poor substitute for face-to-face communication, especially in settings where creative collaboration or learning is paramount. The pandemic made this painfully clear. Relationships cannot and should not be sustained in the digital realm alone, especially as AI further erodes our understanding of what is real. Tapping a “Like” button is not friendship; it’s a data point. And a conversation with an artificial intelligence is one-sided—an illusion of connection. Someday soon, a child may not have just one AI “friend,” but more AI friends than human ones. These companions will not only be built to surveil the humans who use them; they will be tied inexorably to commerce—meaning that they will be designed to encourage engagement and profit. Such incentives warp what relationships ought to be. Writers of fiction—Fyodor Dostoyevsky, Rod Serling, José Saramago—have for generations warned of doppelgängers that might sap our humanity by stealing a person’s likeness. Our new world is a wormhole to that uncanny valley. Whereas the first algorithmic revolution involved using people’s personal data to reorder the world for them, the next will involve our personal data being used not just to splinter our shared sense of reality, but to invent synthetic replicas. The profit-minded music-studio exec will thrill to the notion of an AI-generated voice with AI-generated songs, not attached to a human with intellectual-property rights. Artists, writers, and musicians should anticipate widespread impostor efforts and fight against them. So should all of us. One computer scientist recently told me she’s planning to create a secret code word that only she and her elderly parents know, so that if they ever hear her voice on the other end of the phone pleading for help or money, they’ll know whether it’s been generated by an AI trained on her publicly available lectures to sound exactly like her and scam them. Today’s elementary-school children are already learning not to trust that anything they see or hear through a screen is real. But they deserve a modern technological and informational environment built on Enlightenment values: reason, human autonomy, and the respectful exchange of ideas. Not everything should be recorded or shared; there is individual freedom in embracing ephemerality. More human interactions should take place only between the people involved; privacy is key to preserving our humanity. Finally, a more existential consideration requires our attention, and that is the degree to which the pursuit of knowledge orients us inward or outward. The artificial intelligence of the near future will supercharge our empirical abilities, but it may also dampen our curiosity. We are at risk of becoming so enamored of the synthetic worlds that we create—all data sets, duplicates, and feedback loops—that we cease to peer into the unknown with any degree of true wonder or originality. We should trust human ingenuity and creative intuition, and resist overreliance on tools that dull the wisdom of our own aesthetics and intellect. Emerson once wrote that Isaac Newton “used the same wit to weigh the moon that he used to buckle his shoes.” Newton, I’ll point out, also used that wit to invent a reflecting telescope, the beginnings of a powerful technology that has allowed humankind to squint at the origins of the universe. But the spirit of Emerson’s idea remains crucial: Observing the world, taking it in using our senses, is an essential exercise on the path to knowledge. We can and should layer on technological tools that will aid us in this endeavor, but never at the expense of seeing, feeling, and ultimately knowing for ourselves. A future in which overconfident machines seem to hold the answers to all of life’s cosmic questions is not only dangerously misguided, but takes away that which makes us human. In an age of anger, and snap reactions, and seemingly all-knowing AI, we should put more emphasis on contemplation as a way of being. We should embrace an unfinished state of thinking, the constant work of challenging our preconceived notions, seeking out those with whom we disagree, and sometimes still not knowing. We are mortal beings, driven to know more than we ever will or ever can. The passage of time has the capacity to erase human knowledge: Whole languages disappear; explorers lose their feel for crossing the oceans by gazing at the stars. Technology continually reshapes our intellectual capacities. What remains is the fact that we are on this planet to seek knowledge, truth, and beauty—and that we only get so much time to do it. As a small child in Concord, Massachusetts, I could see Emerson’s home from my bedroom window. Recently, I went back for a visit. Emerson’s house has always captured my imagination. He lived there for 47 years until his death, in 1882. Today, it is maintained by his descendants and a small staff dedicated to his legacy. The house is some 200 years old, and shows its age in creaks and stains. But it also possesses a quality that is extraordinarily rare for a structure of such historic importance: 141 years after his death, Emerson’s house still feels like his. His books are on the shelves. One of his hats hangs on a hook by the door. The original William Morris wallpaper is bright green in the carriage entryway. A rendering of Francesco Salviati’s The Three Fates, holding the thread of destiny, stands watch over the mantel in his study. This is the room in which Emerson wrote Nature. The table where he sat to write it is still there, next to the fireplace. From the October 1883 issue: Ralph Waldo Emerson’s ‘Historic Notes of Life and Letters in Massachusetts’ Standing in Emerson’s study, I thought about how no technology is as good as going to the place, whatever the destination. No book, no photograph, no television broadcast, no tweet, no meme, no augmented reality, no hologram, no AI-generated blueprint or fever dream can replace what we as humans experience. This is why you make the trip, you cross the ocean, you watch the sunset, you hear the crickets, you notice the phase of the moon. It is why you touch the arm of the person beside you as you laugh. And it is why you stand in awe at the Jardin des Plantes, floored by the universe as it reveals its hidden code to you. This article appears in the July/August 2023 print edition with the headline “In Defense of Humanity.” When you buy a book using a link on this page, we receive a commission. Thank you for supporting The Atlantic."
37,https://news.sky.com/story/ai-could-help-produce-deadly-weapons-that-kill-humans-in-two-years-time-rishi-sunaks-adviser-warns-12897366,"Mr Clifford acknowledged that the prediction of computers surpassing human intelligence within two years was at the ""bullish end of the spectrum"", but said AI systems are improving rapidly and becoming increasingly capable. Tuesday 6 June 2023 12:03, UK Artificial Intelligence (AI) could have the power to be behind advances that ""kill many humans"" in just two years' time, Rishi Sunak's adviser has warned. Matt Clifford expressed concern over the lack of global regulation for AI producers and said that if left unregulated, they could become ""very powerful"" and difficult for humans to control, creating significant risks in the short term. He made the comments in a TalkTV interview, citing the potential for AI to create dangerous cyber and biological weapons that could lead to many deaths. Such concerns have been shared by countless experts in the field, as evidenced by a letter published last week, which urged for increased attention and action towards mitigating the risks of AI on par with pandemics or nuclear war. The letter rejecting the harmful use of artificial intelligence was signed by top executives from leading companies like Google DeepMind and Anthropic. Geoffrey Hinton, popularly known as the ""godfather of AI"", also endorsed the letter, warning that if AI falls into the wrong hands, it could be catastrophic for humanity. Mr Clifford, who is the chairman of the Advanced Research and Invention Agency (ARIA), is currently advising the prime minister on the development of the government's Foundation Model Taskforce, which focuses on investigating AI language models such as ChatGPT and Google Bard. ""I think there are lots of different types of risks with AI and often in the industry we talk about near-term and long-term risks, and the near-term risks are actually pretty scary,"" Mr Clifford told TalkTV. ""You can use AI today to create new recipes for bioweapons or to launch large-scale cyber attacks. These are bad things. ""The kind of existential risk that I think the letter writers were talking about is... about what happens once we effectively create a new species, an intelligence that is greater than humans."" Mr Clifford acknowledged that the prediction of computers surpassing human intelligence within two years was at the ""bullish end of the spectrum"", but said that AI systems are improving rapidly and becoming increasingly capable. During an appearance on the First Edition programme on Monday, he was asked what percentage he would give on the chance humanity could be wiped out by AI, replying: ""I think it is not zero."" He continued: ""If we go back to things like the bioweapons or cyber (attacks), you can have really very dangerous threats to humans that could kill many humans - not all humans - simply from where we would expect models to be in two years' time. ""I think the thing to focus on now is how do we make sure that we know how to control these models because right now we don't."" Please use Chrome browser for a more accessible video player The tech expert added that AI production needed to be regulated on a global scale - not only by national governments. The warnings on AI come as apps using the technology have gone viral, with users sharing fake images of celebrities and politicians, while students use ChatPGT and other ""language learning models"" to generate university-grade essays. Read more:Terminator blamed for public concerns over AIAbout ducking time: Apple to tweak iPhone autocorrect functionMeet Ai-Da, the world's first humanoid robot AI is also being used in a positive way - such as performing life-saving tasks including algorithms analysing medical images from X-rays to ultrasounds, thus helping doctors to identify and diagnose diseases such as cancer and heart conditions more accurately and quickly. If harnessed in the right way, Mr Clifford said AI could be a force for good. ""You can imagine AI curing diseases, making the economy more productive, helping us get to a carbon-neutral economy,"" he said. Be the first to get Breaking News Install the Sky News app for free But the Labour Party has been urging ministers to bar technology developers from working on advanced AI tools unless they have been granted a licence. Shadow digital secretary Lucy Powell, who is set to speak at TechUK's conference today, said AI should be licensed in a similar way to medicines or nuclear power. ""That is the kind of model we should be thinking about, where you have to have a licence in order to build these models,"" she told The Guardian. © 2023 Sky UK"
38,https://www.cnbc.com/2023/06/06/brace-for-disruption-but-ai-wont-eliminate-jobs-completely-singapore-dpm.html,"Help for Low Credit Scores All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best Credit Unions Best Mortgages for Small Down Payment Best Mortgages for No Down Payment Best Mortgages with No Origination Fee Best Mortgages for Average Credit Score Adjustable Rate Mortgages Affording a Mortgage Best Life Insurance Best Homeowners Insurance Best Renters Insurance Best Car Insurance All Credit Monitoring Best Credit Monitoring Services Best Identity Theft Protection How to Boost Your Credit Score Credit Repair Services All Personal Finance Best Budgeting Apps Best Expense Tracker Apps Best Money Transfer Apps Best Resale Apps and Sites Buy Now Pay Later (BNPL) Apps Best Debt Relief All Small Business Best Small Business Savings Accounts Best Small Business Checking Accounts Best Credit Cards for Small Business Best Small Business Loans Best Tax Software for Small Business Best Tax Software Best Tax Software for Small Businesses All Help for Low Credit Scores Best Credit Cards for Bad Credit Best Personal Loans for Bad Credit Best Debt Consolidation Loans for Bad Credit Personal Loans if You Don't Have Credit Best Credit Cards for Building Credit Personal Loans for 580 Credit Score or Lower Personal Loans for 670 Credit Score or Lower Best Mortgages for Bad Credit Best Hardship Loans How to Boost Your Credit Score Best IRA Accounts Best Roth IRA Accounts Best Investing Apps Best Free Stock Trading Platforms Singapore's Deputy Prime Minister Lawrence Wong said he expects technology — including artificial intelligence — to disrupt the labor market, but it won't eliminate jobs completely. In fact, technology can make humans more productive and create more jobs, he said at the Asia Tech x Singapore summit on Tuesday. ""I do not believe we will end up with a jobless future, a dystopian jobless future where machines take over humans for everything and humans become upset because technology can replace some tasks,"" said Wong. The summit gathers government officials, executives from global companies and consumers over four days to discuss the role of technology in the future. Wong, who is also the city-state's finance minister, said what will change is ""the nature of jobs in blue and white-collar occupations"" and warned that the ""pace of change will accelerate, the scale of disruptions will increase with time."" He added: ""It can also make us more productive in performing other tasks. And by doing so, it will create new tasks and new jobs."" TSMC or Samsung? One chipmaker is the better play on A.I., geopolitics and earnings, analyst says These are the cheapest tech stocks in the S&P 500 How much of A.I. is just hype? A bull and a bear share their tips on how to invest AI has become the new buzzword in the business world, after chatbot ChatGPT went viral following its release in November. The AI-powered chatbot, which can generate humanlike responses to users' prompts, reached 100 million users in just two months after its launch. Some researchers and analysts have even suggested it could lead to human extinction and replace jobs. These experts, including Sam Altman, CEO of ChatGPT owner OpenAI, as well as executives from Google's AI arm DeepMind and Microsoft, also called for global priority to reduce the risks associated with AI. The International Monetary Fund's first deputy managing director, Gita Gopinath, has also warned of ""substantial disruptions"" in labor markets and ""very large"" risks arising from generative AI, according to a Financial Times report. She also called on governments to introduce regulations to govern the technology. Singapore's Wong said workers need to learn to adjust and adapt amid the impending disruptions stemming from AI. ""Understandably, this will create anxiety among those who are less able to adjust and adapt. All of us will do more to help workers refresh and update their skills so that they can stay competitive and relevant in an increasingly digital world,"" he said. ""We cannot leave this for markets to take care of themselves. Neither can we say this is just the responsibility of employers alone,"" stressed Wong. He added that regulators need to implement ""comprehensive support"" in the form of job matching and skills development. ""This will require concerted and proactive efforts on the part of governments, industry and skills training providers,"" said Wong. Singapore launched AI Verify, the world's first testing toolkit, to help companies objectively assess and verify whether their AI products are responsible and meet the international principles, said Wong. The country will continue to work with the industry on pilot projects and drive the development of AI testing standards, he added. Got a confidential news tip? We want to hear from you. Sign up for free newsletters and get more CNBC delivered to your inbox Get this delivered to your inbox, and more info about our products and services.  © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Data also provided by"
39,https://www.nytimes.com/2023/06/03/business/who-is-liable-for-ai-creations.html,"Tools like ChatGPT could open a new line of questions around tech products and harmful content. Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Ephrat Livni, Sarah Kessler and Ravi Mattu A string of challenges to Section 230 — the law that shields online platforms from liability for user-generated content — have failed over the last several weeks. Most recently, the Supreme Court declined on Tuesday to review a suit about exploitative content on Reddit. But the debate over what responsibility tech companies have for harmful content is far from settled — and generative artificial intelligence tools like the ChatGPT chatbot could open a new line of questions. Does Section 230 apply to generative A.I.? The law’s 1996 drafters told DealBook that it does not. “We set out to protect hosting,” said Senator Ron Wyden, Democrat of Oregon. Platforms are immune only to suits about material created by others, not their own work. “If you are partly complicit in content creation, you don’t get the shield,” agreed Chris Cox, a former Republican representative from California. But they admit that these distinctions, which once seemed simple, are already becoming more difficult to make. What about A.I. search engines? Typically, search engines are considered vehicles for information rather than content creators, and search companies have benefited from Section 230 protection. Chatbots generate content, and they are most likely beyond protection. But tech giants like Microsoft and Google are integrating chat and search, complicating matters. “If some search engines start to look more like chat output, the lines will be blurred,” Wyden said. A deadly recipe? Generative A.I. tools have already been used to make intentionally harmful content. And hallucinations — the falsehoods that generative A.I. tools create (like court cases that never existed) — are a significant problem. If a user prompts an A.I. for cocktail instructions and it offers a poisonous concoction, the algorithm operator’s liability is obvious, said Eric Goldman, a law professor at Santa Clara University and a Section 230 expert. But most situations won’t be that clear-cut, and that poses a risk, Goldman said. He fears that anger over immunity for social media platforms threatens nuanced debate about the next generation of tech development. “The blossoming of A.I. comes at one of the most precarious times amid a maturing tech backlash,” Goldman said. “We need some kind of immunity for people who make the tools,” he added. “Without it, we’re never going to see the full potential of A.I.” — Ephrat Livni Elon Musk receives a hero’s welcome in China. The Tesla chief was hailed on Chinese social media as a “global idol” during his visit this week to the country, where he met with government ministers and visited the Tesla’s Shanghai factory. Musk reportedly also had kind words for his hosts: Government readouts of his meetings with Beijing ministers said he had described the U.S. and Chinese economies as “conjoined twins” and opposed political efforts to decouple them. Billy Joel is movin’ out (of Madison Square Garden). The singer announced this week that he will finish a 10-year stay at Madison Square Garden in July 2024. The series of more than 100 shows crossed its $200 million threshold in March. James Dolan, the C.E.O of the Garden’s parent company, said the run had “made history” for both the venue and the music industry. Or, perhaps more simply: Joel was a big shot. “Ketatations” in the workplace. Some executives have embraced the anesthetic ketamine to improve professional performance or foster team bonding. “We put them on yoga mats in the room, we have a prescription from a doctor, and we have a 45-minute experience together,” Kaia Roman, who has led “ketatations” (ketamine + meditation) since the pandemic, told Bloomberg. Others prefer a more aggressive way to relax: Mark Zuckerberg recently completed “the Murph Challenge,” which consists of a mile run, 100 pull-ups, 200 push-ups, 300 squats and another mile run — all while wearing a 20-pound vest. He said he had done it in 40 minutes. General inflation slowed for the 10th straight month in April, but many companies are still raising prices. Why? Some economists blame “greedflation,” “excuseflation” or a “price-price spiral,” whereby businesses use inflationary events like the pandemic, the Ukraine war and soaring energy prices as an excuse to make big price increases that more than cover their higher costs. The idea is that customers are more accepting of price increases when they know inflation is historically high, so companies are taking the opportunity to raise prices as much as they can. But not everyone is convinced, and some point to a host of other postpandemic economic trends as the real culprit. Here are two views. Greedflation is to blame. Despite expectations that net profit margins would decline this year, they have increased at the average company in the S&P 500, according to data from FactSet. “What we see in many cases is that volumes are going down, while prices are going up and profit margins are going up,” said Isabella Weber, a professor at the University of Massachusetts Amherst, who pioneered the theory. She pointed to Starbucks as an extreme example of what she calls “sellers’ inflation.” In 2020, when the pandemic shut down demand for coffee shops, basic supply-and-demand laws suggested that Starbucks would lower the price of coffee to entice people back to its stores. Instead, Weber said, “prices actually were going up.” Last month, the Federal Reserve Bank of Kansas City said corporate profits had contributed to inflation in 2021, though their contribution fell in 2022, which is consistent with what happened in previous economic recoveries. Greedflation is not to blame. Customers who benefited from stimulus checks, low interest rates, investment gains and other factors were in a good financial position coming out of the pandemic. Their willingness to spend more is what’s mostly fueling inflation, some analysts say. “It seems to me that many telling the profit story forget that households have to actually spend money for the story to hold,” David Beckworth, a senior research fellow at the right-leaning Mercatus Center at George Mason University and a former economist for the Treasury Department, told The Times this week. “And once you look at the huge surge in spending, it becomes inescapable to me where the causality lies.” In any case, conditions for greedflation could be waning. Supply chain disruptions and other inflationary pressures are easing, making it harder for companies to blame inflation elsewhere for raising prices. “Some firms are claiming ‘general inflation pressures’ as being behind their price increases,” said Paul Donovan, the chief economist at UBS, “but that is far less convincing, and consumers are less willing to accept it.” Weber warns, however, that another inflation-causing crisis could pop up at any time, and “firms have now learned this playbook.” “Ted Lasso,” the saccharine story of an apparently clueless American who is appointed to run a British soccer team, ended this week. And while Lasso’s journey from barely knowing the rules to turning a group of misfits into a top team is not particularly realistic, management experts say some of his coaching strategies really are. DealBook has picked out four management lessons from the fictional coach from Kansas that might apply to the real world. Warning: They contain spoilers. The outsider sees things that others do not. Lasso is initially portrayed as a naïve bumpkin with little understanding of the sport, his team or the country he’s living in. But that is the foundation of his success, said Allyson Stewart-Allen, C.E.O. of International Marketing Partners and an expert on cross-cultural management. “He brings a lack of self-consciousness in wanting to ask questions others might think are facile,” she told DealBook, adding that her American clients who are expanding in Europe do exactly what Lasso does. “Ask lots of questions, be open to new ideas, and experiment.” Culture trumps strategy. Any team that is on the same wavelength is obviously more likely to thrive, a view famously championed by management thinkers like Peter Drucker. Lasso’s first task was to understand the culture of the organization he has taken over and then mold it in his image. Once he achieved this, he shifted to identifying the strengths and weaknesses of his players and coaches, and figuring out how to motivate them. That should be the goal of every good manager. Serious strategic change takes time. Most executives, especially those who run public companies, are under immense pressure to deliver quickly. Sometimes that’s justified, but sometimes boards can be too quick to change a C.E.O. without providing the necessary support. Lasso had three years with little real threat of being fired. That gave him time to understand the game (by the final episode, he had learned what the offside rule is), the culture of the club and how to make it all work. It was only midway through his last season that he discovered his sporting vision — “total football” — that he used to turn his team of losers into winners. “Meaning matters more than means,” Bruce Feiler, the author of “The Search: Finding Meaningful Work in a Post-Career World,” told CNBC this week. Younger workers increasingly put a priority on work-life balance and personal fulfillment over money in their careers. On the show, Lasso himself best embodied that trait by walking away from the job after finishing second rather than sticking around and trying to win it all again. Even though he finally cracked the sport, he returned to Kansas to be closer to his family. “He shows vulnerability,” Stewart-Allen said. “He cries. He has panic attacks. He’s not perfect, and he doesn’t try to hide that. I think that is very realistic and endearing and builds empathy with people.” Last week, we wrote about a recent Pew survey that found almost half of Americans do not use all of their paid time off, and we asked you for your thoughts. A lot of you cited the same reasons as respondents to the survey for not using all of your time — banking time to use in an emergency, fearing that taking vacation will make you vulnerable during a layoff or worrying that work will accumulate to stressful levels while you’re away. We also heard from many readers who do use all of their paid time off. Here are a few of your reasons: Quilvio wrote that he is in his early 20s, younger than most of his co-workers, and that “generationally, we have different mind-set around P.T.O. and work.” He added, “I think as long as I’m getting the work done, the days (and hours) I work aren’t as important.” Another reader, who asked not to be named, wrote that she used to work at a prestigious New York City law firm where most senior attorneys did not take all of their paid vacation days. Talking with them about their weekend leisure activities, she realized why: “It dawned on me (silly woman) — they have WIVES AND SERVANTS who do all the nonwork work for them! So they have time and energy to unwind on both evenings AND weekends. They are not making calls to set up doctor appointments for their kids (or, likely, for themselves either), they are not making dinner after work every night, they do not attend P.T.A. meetings, they are not burdened with the zillion daily decisions and tasks of keeping a household going.” Stephanie, a director at a hospital, said she granted whatever time her employees needed. “It’s a retention tool,” she wrote. “We have a high-performing team. If I take care of my managers, they take care of their staff. The staff then are better able to care for their patients.” Thanks for reading! We’d like your feedback. Please email thoughts and suggestions to dealbook@nytimes.com. Ephrat Livni reports from Washington on the intersection of business and policy for DealBook. Previously, she was a senior reporter at Quartz, covering law and politics, and has practiced law in the public and private sectors.   @el72champs Sarah Kessler is a senior staff editor for DealBook and the author of “Gigged,” a book about workers in the gig economy. @sarahfkessler Ravi Mattu is the managing editor of DealBook, based in London. He joined The New York Times in 2022 from the Financial Times, where he held a number of senior roles in Hong Kong and London. @ravmattu Executives from leading A.I. companies, including OpenAI and Google, warned that the technology they were building might one day pose an existential threat to humanity. One of the most urgent warnings about the risks of A.I. has come from Geoffrey Hinton, whom many consider to be the godfather of artificial intelligence. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. The Age of A.I. In Silicon Valley’s hacker houses, young A.I. entrepreneurs are partying, innovating — and hoping not to get crushed by the big guys. A lawyer representing a man who sued an airline relied on ChatGPT to help prepare a court filing. It did not go well. How has the Shoggoth, an octopus-like creature from a science fiction story, come to symbolize the state of artificial intelligence? Kevin Roose explains. Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach."
40,https://www.imf.org/en/News/Articles/2023/06/05/sp060523-fdmd-ai-adamsmith,"The IMF Press Center is a password-protected site for working journalists. The IMF Press Center is a password-protected site for working journalists. Sign up to receive free e-mail notices when new series and/or country items are posted on the IMF website. Modify your profile By Gita Gopinath, First Deputy Managing Director, IMF Speech to commemorate 300th anniversary of Adam Smith’s birth University of Glasgow, June 5, 2023 June 5, 2023 As Prepared For Delivery Thank you, Professor Sir Muscatelli, for that kind introduction. And thank you to the University of Glasgow, not only for the incredible honor of this degree but also for inviting me to participate in this celebration of Adam Smith and his legacy. Nowadays, it’s almost impossible to talk about economics without invoking Adam Smith. We take for granted many of his concepts, such as the division of labor and the invisible hand. Yet, at the time when he was writing, these ideas went against the grain. He wasn’t afraid to push boundaries and question established thinking. Smith grappled with how to advance well-being and prosperity at a time of great change. The Industrial Revolution was ushering in new technologies that would revolutionize the nature of work, create winners and losers, and potentially transform society. But their impact wasn’t yet clear. The Wealth of Nations, for example, was published the same year James Watt unveiled his steam engine. Today, we find ourselves at a similar inflection point, where a new technology, generative artificial intelligence, could change our lives in spectacular—and possibly existential—ways. It could even redefine what it means to be human. Given the parallels between Adam Smith’s time and ours, I’d like to propose a thought experiment: If he were alive today, how would Adam Smith have responded to the emergence of this new “artificial hand”? Beyond the Invisible Hand To explore this question, I’d like to start with his most famous work, The Wealth of Nations. A seminal idea in this work is that the wealth of a nation is determined by the living standards of its people, and that those standards can be raised by lifting productivity, that is the amount of output produced per worker. This idea is especially relevant today because global productivity growth has been slowing for more than a decade, undermining the advancement of living standards. AI could certainly help reverse this trend. We could foresee a world in which it boosts economic growth and benefits workers. AI could raise productivity by automating certain cognitive tasks while giving rise to new higher-productivity tasks for humans to perform. With machines taking care of routine and repetitive tasks, humans could spend more time on what makes us unique: being creative innovators and problem solvers. Early evidence suggests AI could substantially raise productivity. A recent study examined how customer-service agents worked with a conversational assistant that used generative artificial intelligence. The AI assistant monitored customer chats and gave agents suggestions for how to respond. The study found that productivity rose by 14% with the use of this technology. It's interesting to note that the greatest productivity impact was on newer and lower-skilled workers. Why? The study suggests that AI can help spread the knowledge of more experienced, productive workers. Imagine how productive a company could be if every employee performed at the level of its best employee! If such dynamics hold on a broad scale, the benefits could be vast. Goldman Sachs has forecast that AI could increase global output by 7%, or roughly $7 trillion, over a decade. That is more than the combined size of the economies of India and the United Kingdom. While it is far from certain that such sizeable gains will be realized, it is probably safe to say that when it comes to maximizing efficiency, Adam Smith would be wary of stifling the artificial hand of AI. Aside from the gains in productivity, AI could shake up the labor market in unprecedented ways. Recently, we have seen the loss of “middle-skill” jobs due to automation, resulting in large clusters of high-paying and low-paying jobs at either pole of labor markets. The literature shows that AI could affect occupations and industries differently than previous waves of automation. Recent empirical studies suggest AI could reduce job-market polarization, by putting downward pressure on wages of high-paying jobs. Some studies suggest that AI adoption could flatten the hierarchical structures of firms, increasing the number of workers in junior positions and decreasing the number in middle management and senior roles. The number of jobs affected could be sweeping—some researchers estimate that two-thirds of U.S. occupations could be vulnerable to some form of automation. So, what will be the net impact on the job market? It is by no means guaranteed that AI will benefit humans, or that the gains of the winners will be sufficient to compensate the losers. It’s quite possible that AI might simply replace human jobs without creating new, more productive work for humans to move into, as the economist Daron Acemoglu has noted. Thus, despite AI’s potential, we need to consider the broad negative effect it could have on employment—and the social upheaval that could cause. Given that the well-being of the individual and the plight of the common worker underpinned much of Adam Smith’s thinking, this would surely have troubled him. He was interested in developing an economy that worked for everyone—not simply a chosen few. Throughout The Wealth of Nations, he criticized the mercantilist trade system under which England sought to expand its exports at all costs, with too much market power being concentrated in the hands of companies granted trading monopolies. Today, the market for the components to develop AI tools is highly concentrated. A single company has a dominant position in the market for silicon chips best suited for AI applications, for example. Many AI models require massive computing power and huge amounts of data—the lifeblood through which these models hone their “intelligence.” To be sure, open-source programmers have shown an impressive ability to design their own AIs. But only a handful of large corporations may have the computing and data firepower to develop high-end models in the future.   While Smith would have been impressed by the emergence of such a powerful technology in a globalized economy, he might also have realized that the invisible hand alone may not be enough to ensure broad benefits to society. In fact, in many areas—from finance to manufacturing— the invisible hand hasn’t been enough to ensure broad benefits for quite some time. New Approach to Regulation Which brings me to a point I’d like to emphasize—we urgently need sound, smart regulations that ensure AI is harnessed for the benefit of society. One of the challenges is the extent to which humans may come to depend on the judgment of AI systems. They rely on existing data, and hence may replicate the embedded bias in that data. Some models have shown a tendency to confidently defend false information—a phenomenon known as AI “hallucination.” If we cede control to AI in areas such as medicine and critical infrastructure, the risks could be severe and even existential. When it comes to AI, we need more than new rules: we need to recognize that this might be an entirely new game. And that will require an entirely new approach to public policy. New legislation proposed by the EU is an encouraging start. The EU’s Artificial Intelligence Act classifies AI by risk levels. The highest-risk systems would be banned. This would include government systems that rank people based on social compliance, known as “social scoring.” The next-highest risk level would be tightly regulated, with requirements for transparency and human oversight. Beyond regulating AI systems directly, we must be prepared to address the broader effects of AI on our economies and societies. Given the threat of widespread job losses, it is critical for governments to develop nimble social safety nets to help those whose jobs are displaced, and to reinvigorate labor market policies to help workers remain in the labor market. Taxation policies should also be carefully assessed to ensure tax systems don’t favor indiscriminate substitution of labor. Making the right adjustments to the education system will be crucial. We need to prepare the next generation of workers to operate these new technologies and provide current employees with ongoing training opportunities. Demand for STEM specialists will likely grow. However, the value of a liberal arts education—which teaches students to think about ‘big questions’ facing humanity and do so by drawing on many disciplines—may also increase. Clearly, we need international coordination on regulation, because AI operates across borders. It is therefore encouraging to see that the G7 has formed a working group to study AI. In the end, we’ll need a truly global set of rules. Considering how fast the technology is moving, time is of the essence. All that said, to truly consider the implications of AI from Adam Smith’s perspective, we need to go back to his first major work, The Theory of Moral Sentiments. Smith explored what enables us to behave morally. In his view, it’s our ability to experience “sympathy”: we can imagine each other’s joy and pain, and as a result, we temper our “passions” and learn to be civil toward others. It’s what allows us to build and sustain a rules-based society. But what happens when you add artificial intelligence into the mix? Of course, AI has been part of our lives for years—it completes our sentences when we’re typing on our phones and recommends what video we should watch next. What’s remarkable about the latest wave of generative AI technology is its ability to comb vast amounts of knowledge and distill it into a convincing set of messages. AI doesn’t just think and learn fast—it now speaks like us, too. It’s unclear whether AI will evolve to the point where it could be called truly sentient. But if it can already replicate human speech, it may be difficult to know the difference. The glue that binds the concept of society conceived by Smith—sympathetic human beings interacting in the spirit of compromise—begins to disintegrate. This has deeply disturbed scholars such as Yuval Harari. Through its mastery of language, Harari argues, AI could form close relationships with people, using “fake intimacy” to influence our opinions and worldviews. That has the potential to destabilize societies. It may even undermine our basic understanding of human civilization, given that our cultural norms, from religion to nationhood, are based on accepted social narratives. It's telling that even the pioneers of AI technology are wary of the existential risks it poses. Just last week, more than 350 AI industry leaders signed a statement calling for global priority to be placed on mitigating the risk of “extinction” from AI. In doing so, they put the risk on par with pandemics and nuclear wars. So much of Adam Smith’s work is based on the idea of information being effectively transmitted through society. Markets send signals through prices to producers and consumers. Human beings pick up emotional cues from each other, enabling them to civilize their behavior. But AI can significantly damage the integrity of that information and the fundamental benefits that it confers to society. Smith would no doubt be troubled by the possibility of “hallucinating” software spreading fake news and deepening divides in society. Thus, there’s a good chance he would have supported rules that protect consumer privacy, and limit misinformation in the age of AI. To conclude, I’d like to stress that this debate is ongoing, and I don’t claim to have all the answers. I’ve pointed out a few of the issues surrounding AI, and how we can use Adam Smith’s thinking and philosophy as a guide to help us navigate the path ahead. AI could be as disruptive as the Industrial Revolution was in Adam Smith’s time. We will need to carefully balance support for innovation with regulatory oversight. Because of AI’s unique ability to mimic human thinking, we will need to develop a unique set of rules and policies to make sure it benefits society. And those rules will need to be global. The advent of AI shows that multilateral cooperation is more important than ever.   It's a challenge that will require us to break out of our own echo chambers and consider the broad interest of humanity. Adam Smith is best remembered for his contribution to economics, but his body of knowledge was much broader. He was a student of the law, history, rhetoric, languages, and mathematics. In the same spirit, harnessing AI for the good of humanity will require an interdisciplinary approach. Writing on the cusp of the Industrial Revolution, Smith could hardly have foreseen the world we live in today, some 300 years after his birth. Now, we may once again be on the brink of technological transformations we can’t foresee. For better or worse, humans aren’t known for walking away from the next stage of scientific and technological progress. Usually, we simply muddle through. This time, as we confront the power and perils of the artificial hand, we need to summon every ounce of our empathy and ingenuity—the very things that make human intelligence so special. Phone: +1 202 623-7100Email: MEDIA@IMF.org © 2022 International Monetary Fund. All rights reserved."
41,https://www.socialmediatoday.com/news/instagram-tests-new-ai-chatbot-experience-in-dms/652132,"Let Social Media Today's free newsletter keep you informed, straight from your inbox. There’s never been a tech trend that Meta didn’t like, and generative AI is no exception, with the company currently developing various new generative AI tools for Facebook, Instagram and WhatsApp, that are designed to capitalize on emerging usage trends, while also ensuring that Meta doesn’t miss the boat on any major shifts. And while we’ve already seen examples of generative AI tools to help create Facebook ads, visual prompt tools for Instagram stickers, and multi-modal AI advancements, Instagram’s also developing a new AI chat option, which looks similar to Snapchat’s My AI tool. As you can see in this example, shared by app researcher Alessandro Paluzzi, Instagram’s currently developing a new AI chat option that would enable you to ask questions of an AI system within any chat thread. You’d be able to ask questions of the AI tool in-stream, while it would also be able to give you advice on how to write more effective messages. As shown in this example, by typing @ai into the chat field, you’d then be diverted to its AI chatbot, which would be available to answer your questions at any time. So, much like My AI, the bot will incorporate AI responses into a discussion, adding another element to your Instagram DMs. That could touch on two key usage developments. For one, more conversation on IG has been switching to DMs, with users posting fewer Stories and feed posts than they have in the past. As such, Instagram’s been looking for more ways to expand on its messaging options, and this could help feed into that trend, merging with usage behaviors. The other development, of course, is generative AI, and building these tools into social apps. At present, I still haven’t seen a truly game-changing way to incorporate generative AI tools into social apps, with most simply adding in text-to-vision prompts or chatbots into the UI. Both of those are somewhat interesting use cases, but they’re not really amazing, with this new IG messaging function likely falling into the same category – interesting to have on hand, and to try out with some wacky questions. But maybe not a hugely useful addition. But it would bring generative AI into Instagram, which is what Meta really wants, at least as a starting point. As Meta CEO Mark Zuckerberg noted about AI via his new Instagram Channel recently: “In the short term, we’ll focus on building creative and expressive tools. Over the longer term, we’ll focus on developing AI personas that can help people in a variety of ways. We’re exploring experiences with text (like cat in WhatsApp and Messenger), with images (like creative Instagram filters and ad formats), and with video and multi-modal experiences.” The personas element would also be built into this IG chat option – though again, I don’t see that being a major lure for users, or a big engagement factor for AI bots in general. Like, it’s still an AI bot. Whether I ask it to respond to me in the voice of Hulk Hogan, or Shakespeare, it’s still just a bot pumping out the same answers. Would it be a big functional update? Probably not, but as a starting point, this is where we’re seeing generative AI merge into common functionality, as a means to acclimatize people to it, before merging it into new areas. Those new areas could be far more revolutionary – like, for example, generative AI that can build VR worlds. Functions like that are the next level, but each platform has to start somewhere in developing their AI response tools. Get the free daily newsletter read by industry experts For months, we’ve been tracking Elon Musk’s $44 billion deal with Twitter. Now that the deal is done, we watch as the Musk-Twitter era begins. Keep up with the story. Subscribe to the Social Media Today free daily newsletter Keep up with the story. Subscribe to the Social Media Today free daily newsletter Subscribe to Social Media Today for top news, trends & analysis Get the free daily newsletter read by industry experts Want to share a company announcement with your peers? Get started ➔ For months, we’ve been tracking Elon Musk’s $44 billion deal with Twitter. Now that the deal is done, we watch as the Musk-Twitter era begins. The free newsletter covering the top industry headlines"
42,https://www.theregister.com/2023/06/06/netherlands_minister_asks_big_tech/,"Singapore's ATxSG conference has opened with a feisty encounter in which Microsoft's president for Asia argued that bad AI's worst effects - even deaths - may need to be tolerated. Speaking on the event's opening panel in Singapore on Tuesday, Microsoft Asia president Ahmed Mazhari was teamed with the Netherlands' minister for digitalization Alexandra van Huffelen and Nvidia vice president Keith Strier. Van Huffelen was the sceptic, with Mazhari and Strier happy to position the tech as being led by boundary-testing pioneers driving innovation and the economy in the face of wet blanket governments. The politician laid out her philosophy from the start: Societies can only utilize the full opportunities tech bring if they trust it is safe. She defined the role of government as responsible for setting regulations and laying out penalties for those who break the rules. Mazhari entered the ATxSG conversation defensively. ""I want to remind the people here and those listening, that we've actually defined the principles of AI as far as back as 2016 or 2017,"" he said in his opening statement. Mazhari then claimed that Microsoft was both willing and already participating with governments and institutions to ensure his company provides “the trust that [AI] technology deserves.” That’s when things got awkward. ""It is interesting to hear that - before you may go further with this, because we presently see the technology that your company is now putting to market. You're a shareholder in OpenAI, which is actually not adhering to these principles yet, right?"" Van Huffelen pointed out. Mazhari conceded her point was ""appropriate."" “At this point in time, there is no global body, there is no equivalent of the IAEA, for that matter and perhaps the world needs something,” said Mazhari, adding ""the world would benefit from having some form of more international regulation."" The minister then highlighted that rules were being formed for AI in Europe, as well as in North America and other countries. ""But I mean, are you going to adhere to those rules?” said Van Huffelen, leading the Microsoft exec to respond that the software giant adheres to the rules of the jurisdiction it operates in. Van Huffelen then spoke for every frustrated internet user in the room when she said she was often personally coerced into giving permission for the sharing of her information online because Big Tech does not give any other option, suggesting that tactics to comply were often underhanded with companies unwilling to abandon adversarial business motives. “When I tried to use a certain social media platform, I have to read through something that I do not want to read through, or cannot read through - and say yes. That's what most people do,” explained the minister. ""If you want to really protect people, make sure that only the data are used that they want to be used."" Strier ventured that there were many varying opinions on the current state of AI, with some experts seeing risks and others not. The Nvidia vice president regarded regulation as the foundation of solution, but not the only solution, as he advocated for professional standards, social norms that define boundaries, and education. The panel drew parallels between AI and the automotive industry. Van Huffelen argued that using AI was like having an untested car on the road and ""kind of scary."" Mazhari said thousands die in road accidents every day but car manufacturers are allowed to carry on. ""If we took cars away from the world – not sure what will happen to human productivity,"" said the exec. ""Are you saying people get killed from generative AI? I don’t know what to say,"" answered the minister. ® Send us news The Register Biting the hand that feeds IT Copyright. All rights reserved © 1998–2023"
43,https://www.forbes.com/sites/patrickmoorhead/2023/06/05/microsoft-build-2023-brings-ai-tools-to-where-work-is-done-with-copilots-and-plugins,"Microsoft chairman and CEO Satya Nadella speaks to attendees at Microsoft Build 2023. Microsoft Build 2023, Microsoft's annual flagship event for developers, showcased AI-centric announcements across the Microsoft portfolio. The company provided a wealth of information over two days, much of which focused on what the company has done with its OpenAI investment. Microsoft got the jump on its generative AI (GAI) initiatives at a much smaller February event it hosted in Redmond, with just a handful of analysts and media in attendance. Although the company's integration of AI into Bing was exciting, I was somewhat skeptical about how GAI would meaningfully take hold, especially in the enterprise. In the months since then, Microsoft has expanded its use of AI across its apps and services, including Microsoft 365 (which I wrote about here), Windows 11 integrated Bing Chat, Bing and Edge and more. Moving forward, I don't imagine there will be any part of Microsoft that won't have some element of AI. With the massive investments the company has made into OpenAI, it makes sense that it would continue to go all in on AI integrations. AI was certainly front and center—and placed squarely where people do their work—at Microsoft Build, with integrations embedded at the point of need rather than merely being scattered across disparate apps. In a nutshell, GAI incorporated across Microsoft’s platform now acts as a centralized assistant that empowers users to collaborate and complete tasks regardless of which application they’re using. With this approach, Microsoft is working to solidify its AI first-mover advantage, meaningfully taking its story from buzzwords to business value. This article outlines some of the highlights from Microsoft Build. I’ll also examine how the company’s latest AI developments fit into two general themes from Build 2023: plugins and copilots. Plugging into the developer community Many of Microsoft's announcements at the event answered how the company is extending generative AI to its diverse developer community. The company has the world's largest developer communities and ecosystems, from Azure to Windows to devices and everything in between. Microsoft must constantly face the challenge of fostering healthy, inclusive and safe developer communities with transformational tools that enable even more people from diverse backgrounds to develop software. I believe the company has demonstrated that it’s up for this challenge yet again with its new AI solutions for developers. Microsoft announced it would adopt the same open plugin standard that OpenAI introduced for ChatGPT, growing the AI plugin ecosystem to leverage ChatGPT services within Microsoft. This means that developers can now use one platform to build plugins that work across consumer and business touchpoints, including ChatGPT, Bing, Dynamics 365 Copilot and Microsoft 365 Copilot. Any plugins for AI applications built on the Azure OpenAI Service will be interoperable with this same plugin standard. This ups the ante for developers to create experiences that enable people to interact with apps using text and language prompts the same way they would use a chatbot. This is another example of meeting people where they are and providing the tools they need to drive better outcomes. Bing Chat plugin partners Microsoft also announced support for new plugins for Bing Chat. These add-ons interact with a wide range of platforms including Atlassian, Adobe, Instacart, Zillow, Klarna and many others, along with the already announced OpenTable and WolframAlpha. Microsoft expects thousands of plugins by the time Copilot is generally available. The vast user experience improvement of ""interacting"" with an app in this way has yet to be realized. Still, once the models are better trained—particularly with tenant data—having a chatbot in OpenTable will be like having a concierge that can make recommendations and reservations. Similarly, an Atlassian chatbot could become a scrum master (or scrum copilot) capable of organizing dev teams' workflows. Building the groundwork for enterprise generative AI with Copilots As I mentioned, one central theme of Microsoft's announcement was copilots. At the event, Microsoft showcased updated features for copilots that cater to a wide range of users. These include Dynamics 365 Copilot, Microsoft 365 Copilot and Copilot for Power Platform. Microsoft's approach to each copilot builds on the belief that AI's current place in the workforce is to complement people in their roles rather than replace them. Integrating copilots directly into users' workflows makes access to information readily accessible in the context of use rather than requiring the user to toggle through tools to accomplish an AI-assisted task. Microsoft's plan for copilots in workflows is nicely illustrated (pun intended) with the DALL·E-powered Bing Image Creator now functioning within Bing Chat. The company has opened a full public preview of the platform so that anyone with a Microsoft account can create images using a text prompt. Microsoft also announced the expansion of a new AI-powered Bing for the Windows 11 taskbar, mobile and Skype. Panos Panay, executive vice president and chief product officer, Microsoft, at Microsoft Build 2023. Windows Copilot: It goes to (Windows) 11 Throughout the past year, Windows has experienced remarkable growth, primarily driven by the widespread adoption of Windows 11. Particularly noteworthy in fueling this growth has been developer engagement. Microsoft reported a notable 24% year-over-year increase in the usage of devices dedicated to development purposes. Building on the integration into Windows 11 back in February that brought the new AI-powered Bing to the taskbar, Windows Copilot now makes Windows the first PC platform to centralize AI assistance. Using Bing Chat and first-party and third-party plugins, users can concentrate on realizing ideas, completing projects and collaborating effectively rather than expending energy searching for, launching and working with multiple applications. Copilot in the Windows 11 taskbar opens the Copilot sidebar, which can help with tasks such as summaries and explanations. It provides a productivity boost and offers rudimentary (at least for now) IT support, as users can ask it to adjust their computer's settings. Microsoft will start testing the Windows Copilot for Windows 11 publicly in June before a wider rollout. Dev Home makes Windows dev machines easier to use In one of many announcements catering to the development community, Microsoft showed that it is making it easier for developers to set up and use Windows dev machines. Dev Home is designed to allow developers to get a quick overview of their projects through GitHub widgets that surface GitHub issues and pull requests. Microsoft said it would eventually add the Xbox GDK to Dev Home to expand functionality to game developers. The whole thing is essentially self-contained for developers, so that hopefully, spinning up a dev environment on a personal system will be much less clunky and less likely to foul up the system. A new Dev Home section of Windows 11 is now available in preview. Microsoft also announced that Windows Terminal (a developer tool that enables multiple command-line apps or shells to run side-by-side in a customizable environment) would have an AI-powered chatbot. Through an integration with GitHub, developers who use GitHub Copilot can now use the chatbot directly within Windows Terminal to receive code recommendations and explanations for errors as well as to perform other actions. Microsoft says it's also exploring integrating GitHub Copilot with other developer tools. Consumer announcements from Microsoft Build 2023 Microsoft made several consumer announcements not specific to developers at Build, including the announcement of Bing as ChatGPT's default search engine. ChatGPT Plus’s paid users will now see citations for the chatbot's responses when surfaced by Bing. This is not surprising, given Microsoft's multi-billion-dollar investment in OpenAI. With all the talk and media hysteria about chatbots ""hallucinating,"" Bing citations will help users discern real information and increase their confidence in ChatGPT’s results. Microsoft is also bringing 365 Copilot to its Edge web browser. 365 Copilot will live within the browser's sidebar to use content from the web for projects in Microsoft 365 apps. Again, this allows for less toggling and more focused work—something any Microsoft 365 user should appreciate. Developments for the cloud Microsoft also announced the implementation of its Hybrid Loop—initially introduced at last year’s Microsoft Build—designed to enhance AI development across different platforms. Hybrid Loop uses ONNX Runtime as a gateway to Windows AI and Olive, Microsoft’s toolchain that makes it easier to optimize models for different devices. With ONNX Runtime, third-party developers can use the same tools Microsoft uses to run AI models on Windows or other devices, whether it's using CPU, GPU, NPU or hybrid with Azure. The goal is to support AI development from Azure to client devices, enabling hybrid AI mode to build for both ends of the spectrum. Hybrid inferencing scenarios refers to the use of local resources when possible, with the ability to switch to the cloud when needed. Although I expected to see more on-device AI integrations from Microsoft, the Qualcomm partnership to deploy the Qualcomm AI Engine to deliver efficient machine learning at the edge highlights the increasing adoption of hybrid AI, distributing inference between the cloud and edge. This trend is driven by the demand for security, low latency and high performance, as well as the growing trend of AI at the edge where data is collected. Microsoft has also partnered with AMD, Intel and Nvidia for new silicon support. Microsoft showcased its Azure AI Content Safety service that facilitates the establishment of secure online environments. Leveraging AI models, it identifies and categorizes offensive, violent, sexual and self-harm content in images and text, assigning severity scores to aid businesses in restricting content and prioritizing moderation. Azure AI Content Safety can comprehend nuance and context, minimizing false positives and alleviating the burden on content moderation teams. This is especially important as regulators try to figure out what constitutes “responsible” use of AI and how to regulate it via policy. Microsoft Fabric to unify analytics stacks Microsoft introduced Microsoft Fabric, an end-to-end unified analytics solution. Fabric is designed to help enterprises eliminate data silos and duplication and reduce the time it takes to turn raw data into business intelligence. A unified solution consolidating the necessary data provisioning, transformation, modeling and analysis services into one UI is a smart move for Microsoft, one that will help enterprises extract more value from their data while laying a foundation for the AI era. Microsoft will continue to offer enterprise-grade PaaS solutions for data analytics. More than just repackaging existing tools, Fabric's value proposition represents an evolution of those offerings in the form of a simplified SaaS solution (Fabric) that can connect to existing PaaS offerings such as Azure Synapse Analytics and Azure Data Factory. At the core of the new platform is Microsoft's OneLake data lake. However, the platform can integrate data from Amazon S3 and will soon support data from Google Cloud as well. I think enterprises will appreciate streamlining their data infrastructure without being forced to rely exclusively on one cloud vendor. The GAI race started with search, but that was just the tip of the iceberg. Once a mild sceptic, I’m now convinced AI is going to change nearly everything, particularly workflows. With these latest announcements, Microsoft is focusing on targeted use cases and UX refinement for developers and consumers. Microsoft did an excellent job showing the potential to unlock new opportunities that all these evolving technologies bring. The sheer number of use cases for developers addressed by Microsoft’s services, devices and applications made this year’s event one of the most exciting Microsoft Builds I've seen. Note: This analysis contains significant contributions from Melody Brue, Modern Work Vice President and Principal Analyst. Moor Insights & Strategy provides or has provided paid services to technology companies like all research and tech industry analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking, and video and speaking sponsorships. The company has had or currently has paid business relationships with 8×8, Accenture, A10 Networks, Advanced Micro Devices, Amazon, Amazon Web Services, Ambient Scientific, Ampere Computing, Anuta Networks, Applied Brain Research, Applied Micro, Apstra, Arm, Aruba Networks (now HPE), Atom Computing, AT&T, Aura, Automation Anywhere, AWS, A-10 Strategies, Bitfusion, Blaize, Box, Broadcom, C3.AI, Calix, Cadence Systems, Campfire, Cisco Systems, Clear Software, Cloudera, Clumio, Cohesity, Cognitive Systems, CompuCom, Cradlepoint, CyberArk, Dell, Dell EMC, Dell Technologies, Diablo Technologies, Dialogue Group, Digital Optics, Dreamium Labs, D-Wave, Echelon, Ericsson, Extreme Networks, Five9, Flex, Foundries.io, Foxconn, Frame (now VMware), Fujitsu, Gen Z Consortium, Glue Networks, GlobalFoundries, Revolve (now Google), Google Cloud, Graphcore, Groq, Hiregenics, Hotwire Global, HP Inc., Hewlett Packard Enterprise, Honeywell, Huawei Technologies, HYCU, IBM, Infinidat, Infoblox, Infosys, Inseego, IonQ, IonVR, Inseego, Infosys, Infiot, Intel, Interdigital, Jabil Circuit, Juniper Networks, Keysight, Konica Minolta, Lattice Semiconductor, Lenovo, Linux Foundation, Lightbits Labs, LogicMonitor, LoRa Alliance, Luminar, MapBox, Marvell Technology, Mavenir, Marseille Inc, Mayfair Equity, Meraki (Cisco), Merck KGaA, Mesophere, Micron Technology, Microsoft, MiTEL, Mojo Networks, MongoDB, Multefire Alliance, National Instruments, Neat, NetApp, Nightwatch, NOKIA, Nortek, Novumind, NVIDIA, Nutanix, Nuvia (now Qualcomm), NXP, onsemi, ONUG, OpenStack Foundation, Oracle, Palo Alto Networks, Panasas, Peraso, Pexip, Pixelworks, Plume Design, PlusAI, Poly (formerly Plantronics), Portworx, Pure Storage, Qualcomm, Quantinuum, Rackspace, Rambus, Rayvolt E-Bikes, Red Hat, Renesas, Residio, Samsung Electronics, Samsung Semi, SAP, SAS, Scale Computing, Schneider Electric, SiFive, Silver Peak (now Aruba-HPE), SkyWorks, SONY Optical Storage, Splunk, Springpath (now Cisco), Spirent, Splunk, Sprint (now T-Mobile), Stratus Technologies, Symantec, Synaptics, Syniverse, Synopsys, Tanium, Telesign,TE Connectivity, TensTorrent, Tobii Technology, Teradata,T-Mobile, Treasure Data, Twitter, Unity Technologies, UiPath, Verizon Communications, VAST Data, Ventana Micro Systems, Vidyo, VMware, Wave Computing, Wellsmith, Xilinx, Zayo, Zebra, Zededa, Zendesk, Zoho, Zoom, and Zscaler. Moor Insights & Strategy founder, CEO, and Chief Analyst Patrick Moorhead is an investor in dMY Technology Group Inc. VI, Fivestone Partners, Frore Systems, Groq, MemryX, Movandi, and Ventana Micro., MemryX, Movandi, and Ventana Micro."
44,https://finance.yahoo.com/news/citi-quants-ai-driven-tech-085832770.html,"(Bloomberg) -- The roaring rally in tech stocks has further to go, as the buzz around artificial intelligence and hopes for a pause in the Federal Reserve’s rate hikes give them an edge, according to Citigroup Inc. quantitative strategists. Most Read from Bloomberg Ukraine Dam Blast Blamed on Russia Tips War Into New Phase Apple Headset Looks Sleek in Person But Battery Pack Stands Out Apple’s $3,499 Vision Pro Headset Will Test Marketing Might SEC Sues Binance and CEO Zhao for Breaking Securities Rules A Wall Street Titan Scores One of the Best Real Estate Trades Ever The growth style investing factor in the US has by far been the most successful with a nearly 20% jump this year, as AI-driven gains boosted Nvidia Corp. to a $1 trillion market value and drove a record outperformance of tech stocks against non-tech S&P 500 companies. That aside, an unpredictable economic outlook means that US technology stocks may rise even more. “We remain positive on Growth for June as we see more tailwinds than headwinds manifesting for this Style over the next month,” a team including Chris Montagu wrote in a note. He however warned that gains in stock market indexes were driven by fewer stocks in May, which posed a risk. A pause in Fed rates is yet another positive for tech, according to separate note from Citi. “The potential Fed rate skip this month and cautious relative positioning in the equity market are tailwinds to growth performance as well,” the strategists said. The Nasdaq 100’s rally since December has lifted the gauge’s 12-month forward price-to-earnings ratio to 25.7 times, above the 20.7 times average of the past decade, though below the 30.5 times reached in 2020. These “reasonable valuations” will support growth performance going forward, the note said. Even JPMorgan Chase & Co. strategists led by Marko Kolanovic said they are more positive on tech this year, but warned that stocks are likely to face an increasingly challenging trade off between growth and policy in the second half. For Citi, tech will win either way. “Growth is likely to rebound further should investor sentiment improve further,” the Citi team said. “On the extreme opposite, growth may provide more downside protection in a recessionary environment since macro risk in growth is low.” Most Read from Bloomberg Businessweek A $1.5 Trillion Backstop for Homebuyers Props Up Banks Instead Giorgia Meloni Seeks to Cement Power by Remaking Corporate Italy Moving Saharan Sunshine to Europe via Undersea Cables Harvard MBA Grads Enter a Tepid Job Market Hoping for the Best An App Aims to Stop Malaria-Carrying Mosquitoes From Reproducing ©2023 Bloomberg L.P. (Bloomberg) -- EQT AB Chief Executive Officer Christian Sinding sees consolidation on the horizon for alternative asset managers due to fundraising pressures and challenging financing conditions.Most Read from BloombergUkraine Dam Blast Blamed on Russia Tips War Into New PhaseApple Headset Looks Sleek in Person But Battery Pack Stands OutApple’s $3,499 Vision Pro Headset Will Test Marketing MightSEC Sues Binance and CEO Zhao for Breaking Securities RulesA Wall Street Titan Scores One of the Best European shares were range-bound on Tuesday as worries of further interest rate hikes by major central banks in the face of slowing economic growth countered support from healthcare giant Novo Nordisk. The pan-European STOXX 600 index inched 0.1% higher to 460.42 points. Danish drug developer Novo Nordisk rose 3.5%, as trading resumed following a holiday in Copenhagen on Monday. Citi analyst Christopher Danely says chip stocks ""are due for a pullback.” He's cautious on the group, and has moved Micron stock to the front of the pack. Subway is seeking big new franchisees in the United States in a push to revamp its ownership model, but low restaurant profits and outdated stores are making it a tough sell for the global sandwich chain. Several multi-unit operators - the more sophisticated, financially sound franchisees that Subway desires - examined the possibility of entering the chain's system by buying swaths of restaurants but walked away after seeing how little money they made, according to two of their advisers. Lawyer Justin Klein and consultant John Gordon said their clients were deterred by low margins and the prospect of making necessary renovations. (Bloomberg) -- Oil erased all the gains that followed Saudi Arabia’s surprise weekend pledge for extra supply cuts.Most Read from BloombergUkraine Dam Blast Blamed on Russia Tips War Into New PhaseApple Headset Looks Sleek in Person But Battery Pack Stands OutApple’s $3,499 Vision Pro Headset Will Test Marketing MightSEC Sues Binance and CEO Zhao for Breaking Securities RulesA Wall Street Titan Scores One of the Best Real Estate Trades EverWest Texas Intermediate dipped below $71 a barrel on Tue Stocking shelves with low-priced notebooks and slashing prices on sneakers and uniforms, major retailers including Walmart, Foot Locker and Marks & Spencer are preparing for a challenging back-to-school shopping season this year. Dimon will meet with the New Democratic Coalition in a closed-door lunch, the report said. JPMorgan said they had nothing to confirm or say on the record when contacted by Reuters, while members of New Democratic Coalition did not respond to Reuters requests for comment outside business hours. Last month, Bloomberg reported that Dimon along with other banking executives met with Senate Majority Leader Chuck Schumer, a New York Democrat, to discuss the federal debt limit. Actors represented by the Hollywood union SAG-AFTRA voted Monday evening to authorize a strike if they don't agree on a new contract with major studios, streamers and production companies by June 30. Dow Jones futures fell Tuesday morning, as the stock market continued to pause. Cryptocurrency exchange Coinbase plunged 16% on a SEC lawsuit. Just 10 stocks are doing all the lifting on the S&P 500 this year. But it's looking like too much of a good thing, Morningstar says. The summer travel season is shaping up to be solid. Retail sales in the euro zone were unchanged in April, Eurostat said on Tuesday, as consumers spent less on food and car fuel, but increased purchases of other products, particularly online. Retail sales volumes in the 20 nations sharing the euro currency was at the same level as in March and was 2.6% lower year-on-year. Consumption has been weak all year as real incomes fall and households are now spending a larger part of their incomes on expensive energy and on credit and mortgage repayments, eroding demand for others goods. (Bloomberg) -- Intel Corp., the largest US chipmaker by revenue, will sell part of its holdings in Mobileye Global Inc., raising about $1.48 billion for its ambitious spending plans. Most Read from BloombergApple Headset Looks Sleek in Person But Battery Pack Stands OutUkraine Dam Blast Blamed on Russia Tips War Into New PhaseApple’s $3,499 Vision Pro Headset Will Test Marketing MightSEC Sues Binance and CEO Zhao for Breaking Securities RulesA Wall Street Titan Scores One of the Best Real Estate Ford Motor Co's decision to allow customers to use Tesla's electric-vehicle charging network has sent ripples through the industry, raising questions about a national U.S. charging standard as well as the fate of charging startups that are struggling. The deal, announced last month, would open more than 12,000 Tesla Superchargers to drivers of Ford vehicles in North America starting in 2024. The tie-up puts pressure on other companies and the administration of U.S. President Joe Biden to fall in line or spend more to up their games, according to industry executives, investors, bankers and consultants. The regulator said the U.S.’s largest crypto platform violated rules that require it to register as an exchange. As a serial entrepreneur who co-founded Tesla Inc., revolutionized the electric car industry and is sending rockets into space, Elon Musk isn’t known for being a real estate guru. But lately, the billionaire has been sounding the alarm for the sector. “Commercial real estate is melting down fast,” Musk said in a recent tweet. “Home values next.” He elaborated on the dire forecast during an interview with former Fox News personality Tucker Carlson in April. “We really haven’t seen the commercial ""As we move into the the second week of June, our confidence that [the] S&P 500 will gain >20% in 2023 has increased,"" Fundstrat's Tom Lee said. The Dow Jones slid while Apple stock reversed despite unveiling the Vision Pro headset. A Cathie Wood stock popped higher. Blackstone real estate investment trust (BREIT) is known as one of America’s largest and most dependable privately held REITs when it comes to delivering investor returns. However, 2023 has proven to be a difficult year for real estate investors, and Blackstone is not immune. As of May 1, 2023, Blackstone announced it is limiting investor withdrawals from its REIT, which is worth an estimated $70 billion. This move is not a new trend, as Blackstone has been limiting monthly investor withdrawals It's been a rough year for high net worth investors. And they're starting to act scared with their money."
45,https://www.bbc.com/news/uk-england-coventry-warwickshire-65813102,"Advanced AI software was crucial in proving a paedophile groomed and sexually abused young girls, police reveal in a new TV programme. The conviction of Luke Cassidy, 29, from Coventry, has been documented by BBC Two's Forensics: The Real CSI. Investigators said they struggled at first to find enough evidence to prove Cassidy controlled and groomed girls. But via his phone they went on to find deleted messages and photos which led to his conviction in January. Warning: this article contains information some people may find distressing. Cassidy was arrested when a 12-year-old girl reported he groomed her on the social media platform Snapchat and then raped her. But while he admitted to detectives he had sex with her, he claimed she told him she was 16. Additionally, his victim's account of grooming was in need of corroboration for prosecution. First, Cassidy's Nissan Juke was seized by police as forensic coordinator Jo Ward said the victim told them she had been inside the vehicle. But evidence only proved Cassidy had sex in it - there was no DNA linking his victim to the car. Digital forensics officer Dan Coley then examined the phones of defendant and victim but could only prove they knew each other and had spoken. ""There is nothing potentially there to support the victim's account of what's happened,"" he said of an initial stage of the inquiry that ran, he added, the risk of being ""the end of that investigation"". But then came a breakthrough, along with the means to unpick Cassidy's account. Detectives explained that a 13-year-old girl had come forward to inform them she was sexually propositioned by a man online. When it became clear Cassidy was behind that content, investigators were finally able to build a case against him that could bring the first victim into their scope. ArtificialI Intelligence (AI) enables computers to perform complex tasks. In this case, specialist software was trained on the words used in Cassidy's messages to the second victim and then applied to a search for similar language anywhere on Cassidy's phone. Indecent images of children were uncovered plus messages that showed he was grooming children and was indeed a predator, Mr Coley said. Cassidy was then charged. In court he pleaded guilty to one charge of rape and one of possession with intent to supply Class B drugs, but he denied a charge of online grooming and five more counts of rape. However, he was convicted of all charges at Warwick Crown Court in December and jailed for 19 years, with five more on licence. He was also placed on the sex offenders register for an indefinite period. ""This result matters to me,"" Mr Coley said. ""Not just as a parent but also in the hope that ultimately, we are here to try and help safeguard children."" Det Con Corinne Hatton, from West Midlands Police, said Cassidy targeted the most vulnerable in society and had ""taken innocence"". Police said 14 further victims had been uncovered by June and officers were working to identify and support them. Watch ""Forensics: The Real CSI - Tracking a Paedophile"" on BBC Two at 21:00 BST on Tuesday 6 June or via BBC iPlayer after broadcast. If you're affected by any of the issues in this article you can find details of organisations who can help via the BBC Action Line. Follow BBC West Midlands on Facebook, Twitter and Instagram. Send your story ideas to: newsonline.westmidlands@bbc.co.uk AI 'godfather' feels 'lost' over life's work The artist who recreates the faces of the unknown dead County's PCSOs given powers to use forensic spray Mystery of 1948 'Somerton Man' solved - researcher Forensic test sees clothing bloodstains in seconds Forensics: The Real CSI West Midlands Police Milestone for A444 Bermuda Bridge work as safety barriers being installed Leamington hotel and Warwick restaurant shortlisted for hospitality awards Nominate outstanding Coventry NHS staff at the OSCA's Leamington company is showcasing custom built motorcycle for new riders Forensic officers at scene of stabbing at Nuneaton flats as witness appeal launched Man dies after car overturns on south Warwickshire road Evacuations under way as water gushes through damaged Ukraine dam Prince Harry: Tabloids hacked my voicemails when I was at Eton Inside the Taliban's drug war - opium poppy crops slashed Can UK’s Storm Shadow missiles change Ukraine war? Why Putin has put this religious art on display The one thing Mike Pence needs to beat his old boss Who is no longer world's richest? Our quickfire quiz... Haunting images of deadly India train crash in 2002 Jason Derulo makes 'unsexy' investment in car wash Why personalised medicine hasn't arrived How is my country doing tackling climate change? Kashmir battles alarming drug addiction crisis The rise of the 'no-wash' movement Why the city that never sleeps is slowly sinking The generation clocking the most hours © 2023 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking."
46,https://www.fool.com/investing/2023/06/05/2-ai-growth-stocks-make-14-cathie-woods-portfolio/,"Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. You’re reading a free article with opinions that may differ from The Motley Fool’s Premium Investing Services. Become a Motley Fool member today to get instant access to our top analyst recommendations, in-depth research, investing resources, and more. Learn More Cathie Wood is the founder and CEO of Ark Invest, an asset management company focused on disruptive technology stocks, especially those that use artificial intelligence (AI). For instance, Ark has made Tesla (TSLA 1.70%) and UiPath (PATH) its two largest holdings. They account for 8.2% and 6.1% of its portfolio, respectively, signaling high conviction. Here's what investors should know about these AI growth stocks. Tesla led the auto industry in battery electric vehicle (BEV) sales with 24% market share in the first quarter, while runner-up BYD accounted for 15% of BEV sales. The company also reported the highest operating margin among volume automakers in 2022, an achievement CEO Elon Musk attributes to unparalleled manufacturing technology, and management believes the company can maintain its industry-leading margins in 2023 and beyond. What's behind that confidence? One factor is Tesla's ability to produce battery packs (the most expensive part of an electric car) at a lower cost per kilowatt-hour than its peers. Additionally, the company recently unveiled a new vehicle assembly system that will be implemented at Gigafactory Mexico in late 2024. It promises to cut production costs in half while reducing its factory footprint by 40%. But management believes full self-driving (FSD) software will ultimately be the most important source of profitability. FSD software can be sold at 100% gross profit, according to Musk, and it will power the robotaxi Tesla plans to mass-produce in 2024. Ultimately, FSD software will allow the company to launch an autonomous ride-hailing service, entering a market that Ark Invest says could generate $9 trillion in annual revenue by 2030. Of course, autonomous ride-hailing is still in its infancy, but Tesla has a great shot at becoming an industry leader in the future. Musk says Tesla is ""one of the world's leading AI companies"" due to its expertise in AI software and hardware. Tesla has more autonomous driving data than any other automaker because it has more autopilot-enabled vehicles on the road. That data advantage hints at more advanced FSD software because data is the cornerstone of AI. Additionally, Musk says the in-car hardware that runs the FSD software is the ""most efficient inference computer in the world."" Tesla shares currently trade at 8.6 time sales, a discount to the three-year average of 16 times sales. That creates a reasonable buying opportunity, but only for investors who believe the robotaxi narrative. If Tesla fails to evolve into a software and services company (i.e., FSD software and autonomous ride-hailing services), the stock is wildly overvalued at its current price. UiPath specializes in enterprise automation. Its platform helps businesses discover automation opportunities and build software to automate various tasks and workflows, such as extracting document data, moving files, completing forms, and updating databases. The UiPath platform leans on robotic process automation (RPA) to automate simple tasks, but it also incorporates AI to automate complex tasks. For instance, RPA software can pull data from a structured document (i.e., a document with form fields), but RPA software coupled with AI can pull data from an unstructured document, make sense of that data, then act on it. Industry experts have recognized UiPath as a market leader in several software verticals, including RPA, process mining, and intelligent document processing. UiPath delivered mediocre financial results in the first quarter. Its dollar-based net retention rate dropped 16 percentage points to 122%, but that means the average customer still spent 22% more over the past year despite the uncertain economy. First-quarter revenue increased 18% to $290 million and the company reported $67 million in cash from operations, up from a loss of $53 million in the prior year. Looking ahead, UiPath should be able to accelerate growth when economic conditions improve. Many businesses have cut back on spending for fear of a recession, but IT investments will rebound at some point, and implementing RPA and AI solutions will likely be a top priority when that happens. RPA and AI promise to improve productivity and reduce operating costs, and any upswing in demand should draw more businesses to the UiPath platform. On that note, UiPath estimates its addressable market at $61 billion, meaning the company has hardly scratched the surface of its potential. Shares currently trade at 9.1 times sales, a discount to the two-year average of 17.2 times sales. At that price, investors should consider buying a small position in this growth stock. Trevor Jennewine has positions in Tesla and UiPath. The Motley Fool has positions in and recommends BYD, Tesla, and UiPath. The Motley Fool has a disclosure policy. *Average returns of all recommendations since inception. Cost basis and return based on previous market day close. Invest better with The Motley Fool. Get stock recommendations, portfolio guidance, and more from The Motley Fool's premium services. Making the world smarter, happier, and richer. Market data powered by Xignite."
47,https://neurosciencenews.com/ai-synase-23397/,"Summary: Researchers utilized artificial intelligence (AI) to track and visualize changes in synapse strength in live animals. Synapses are the brain’s communication points, crucial for learning, memory, and aging processes. By employing machine learning, scientists enhanced image quality, enabling the detection and tracking of individual synapses over time. This advancement may offer critical insights into how the brain is affected by aging, disease, or injury. Source: Johns Hopkins Medicine Johns Hopkins scientists have developed a method involving artificial intelligence to visualize and track changes in the strength of synapses — the connection points through which nerve cells in the brain communicate — in live animals. The technique, described in Nature Methods, should lead, the scientists say, to a better understanding of how such connections in human brains change with learning, aging, injury and disease. “If you want to learn more about how an orchestra plays, you have to watch individual players over time, and this new method does that for synapses in the brains of living animals,” says Dwight Bergles, Ph.D., the Diana Sylvestre and Charles Homcy Professor in the Solomon H. Snyder Department of Neuroscience at the Johns Hopkins University (JHU) School of Medicine. Bergles co-authored the study with colleagues Adam Charles, Ph.D., M.E., and Jeremias Sulam, Ph.D., both assistant professors in the biomedical engineering department, and Richard Huganir, Ph.D., Bloomberg Distinguished Professor at JHU and Director of the Solomon H. Snyder Department of Neuroscience. All four researchers are members of Johns Hopkins’ Kavli Neuroscience Discovery Institute. Nerve cells transfer information from one cell to another by exchanging chemical messages at synapses (“junctions”). In the brain, the authors explain, different life experiences, such as exposure to new environments and learning skills, are thought to induce changes at synapses, strengthening or weakening these connections to allow learning and memory. Understanding how these minute changes occur across the trillions of synapses in our brains is a daunting challenge, but it is central to uncovering how the brain works when healthy and how it is altered by disease. To determine which synapses change during a particular life event, scientists have long sought better ways to visualize the shifting chemistry of synaptic messaging, necessitated by the high density of synapses in the brain and their small size — traits that make them extremely hard to visualize even with new state-of-the-art microscopes. “We needed to go from challenging, blurry, noisy imaging data to extract the signal portions we need to see,” Charles says. To do so, Bergles, Sulam, Charles, Huganir and their colleagues turned to machine learning, a computational framework that allows flexible development of automatic data processing tools. Machine learning has been successfully applied to many domains across biomedical imaging, and in this case, the scientists leveraged the approach to enhance the quality of images composed of thousands of synapses. Although it can be a powerful tool for automated detection, greatly surpassing human speeds, the system must first be “trained,” teaching the algorithm what high quality images of synapses should look like. In these experiments, the researchers worked with genetically altered mice in which glutamate receptors — the chemical sensors at synapses — glowed green (fluoresced) when exposed to light. Because each receptor emits the same amount of light, the amount of fluorescence generated by a synapse in these mice is an indication of the number of synapses, and therefore its strength. As expected, imaging in the intact brain produced low quality pictures in which individual clusters of glutamate receptors at synapses were difficult to see clearly, let alone to be individually detected and tracked over time. To convert these into higher quality images, the scientists trained a machine learning algorithm with images taken of brain slices (ex vivo) derived from the same type of genetically altered mice. Because these images weren’t from living animals, it was possible to produce much higher quality images using a different microscopy technique, as well as low quality images — similar to those taken in live animals — of the same views. This cross-modality data collection framework enabled the team to develop an enhancement algorithm that can produce higher resolution images from low quality ones, similar to the images collected from living mice. In this way, data collected from the intact brain can be significantly enhanced and able to detect and track individual synapses (in the thousands) during multiday experiments.  To follow changes in receptors over time in living mice, the researchers then used microscopy to take repeated images of the same synapses in mice over several weeks. After capturing baseline images, the team placed the animals in a chamber with new sights, smells and tactile stimulation for a single five-minute period. They then imaged the same area of the brain every other day to see if and how the new stimuli had affected the number of glutamate receptors at synapses. Although the focus of the work was on developing a set of methods to analyze synapse level changes in many different contexts, the researchers found that this simple change in environment caused a spectrum of alterations in fluorescence across synapses in the cerebral cortex, indicating connections where the strength increased and others where it decreased, with a bias toward strengthening in animals exposed to the novel environment. The studies were enabled through close collaboration among scientists with distinct expertise, ranging from molecular biology to artificial intelligence, who don’t normally work closely together. But such collaboration, is encouraged at the cross disciplinary Kavli Neuroscience Discovery Institute, Bergles says. The researchers are now using this machine learning approach to study synaptic changes in animal models of Alzheimer’s disease, and they believe the method could shed new light on synaptic changes that occur in other disease and injury contexts. “We are really excited to see how and where the rest of the scientific community will take this,” Sulam says. Funding: The experiments in this study were conducted by Yu Kang Xu (a Ph.D. student and Kavli Neuroscience Discovery Institute fellow at JHU), Austin Graves, Ph.D. (assistant research professor in biomedical engineering at JHU) and Gabrielle Coste (neuroscience Ph.D. student at JHU). This research was funded by the National Institutes of Health (RO1 RF1MH121539). Author: Vanessa WastaSource: Johns Hopkins MedicineContact: Vanessa Wasta – Johns Hopkins MedicineImage: The image is credited to Neuroscience News Original Research: Open access.“Cross-modality supervised image restoration enables nanoscale tracking of synaptic plasticity in living mice” by Dwight Bergles et al. Nature Methods Cross-modality supervised image restoration enables nanoscale tracking of synaptic plasticity in living mice Learning is thought to involve changes in glutamate receptors at synapses, submicron structures that mediate communication between neurons in the central nervous system. Due to their small size and high density, synapses are difficult to resolve in vivo, limiting our ability to directly relate receptor dynamics to animal behavior. Here we developed a combination of computational and biological methods to overcome these challenges. First, we trained a deep-learning image-restoration algorithm that combines the advantages of ex vivo super-resolution and in vivo imaging modalities to overcome limitations specific to each optical system. When applied to in vivo images from transgenic mice expressing fluorescently labeled glutamate receptors, this restoration algorithm super-resolved synapses, enabling the tracking of behavior-associated synaptic plasticity with high spatial resolution. This method demonstrates the capabilities of image enhancement to learn from ex vivo data and imaging techniques to improve in vivo imaging resolution. I would like additional information on this article. Were the animals involved under classical conditioning (i.e. Becoming innate to a specific behavior or thing)? Your email address will not be published. Required fields are marked * Notify me of follow-up comments by email. Notify me of new posts by email. Neuroscience News SitemapNeuroscience Graduate and Undergraduate ProgramsFree Neuroscience MOOCsAboutContact UsPrivacy PolicySubmit Neuroscience NewsSubscribe for Emails Neuroscience ResearchPsychology NewsBrain Cancer ResearchAlzheimer’s DiseaseParkinson’s NewsAutism / ASD NewsNeurotechnology NewsArtificial Intelligence NewsRobotics News Neuroscience News is an online science magazine offering free to read research articles about neuroscience, neurology, psychology, artificial intelligence, neurotechnology, robotics, deep learning, neurosurgery, mental health and more."
48,https://www.gartner.com/en/articles/what-cios-need-to-know-about-deploying-ai,"or call jsbacContact or call jsbacContact June 05, 2023 Contributor: John Hillery and Nathan Lewis These five points are essential for making important leadership decisions. Even before ChatGPT, one-third of CIOs say their organization had already deployed artificial intelligence (AI) technologies, and 15% more believe they will deploy AI within the next year, according to the 2023 Gartner CIO and Technology Executive Survey. But deciding how best to proceed means factoring AI into business value, risk, talent and investment priorities. Download now: Your Detailed Guide to the 2023 Gartner Top 10 Strategic Technology Trends Business leaders have high expectations about AI that CIOs will need to manage. CIOs need to be fluent in the technical language of AI as well as the risks and opportunities for their business. Here are five things every CIO should know about the AI landscape to become a successful business leader in the rollout of AI. Most organizations typically deploy AI in a business unit or area for the following use cases: Smart process automation and robotics systems Automating and personalizing at scale Increase workforce productivity and AI-enabled decisions accuracy Watch now: Beyond the Hype: The Practical Applications & Use Cases of Generative AI CIOs often expect AI to add value to the business but must be clear on what’s feasible. Most AI business value is generated from one-off, point-to-point solutions. Getting more value from solutions at scale may require deep business process changes, and new ways of working between AI teams and software engineering, because AI is difficult to integrate into existing systems. The following use cases are both highly feasible and highly likely to drive business value, so investment here will be easy to justify:  These are examples of highly feasible AI use cases for which the business value is likely medium, so investment will be more opportunistic: Cross-selling and upselling Sales content personalization Generative AI can augment and accelerate multiple business capabilities, but CIOs need to be aware of emerging government regulations and frameworks around AI, especially as increased usage triggers more questions about ethics and responsibility. The following risks are associated with AI and generative AI.  Regulatory. AI poses legal risks by potentially opening up organizations to lawsuits over copyrighted or protected content, information and data. Reputational. AI can amplify biases and create a “black box” — an AI system with no user visibility into inputs and operations. Competency. AI requires a unique set of skills that need to be intentionally sourced through upskilling existing talent or from academia or startups. False output. Generative AI, and ChatGPT specifically, can be unstable, be erroneous in reasoning, can fail to comprehend the entire context, has limited explainability and trackability, and is biased. Security. Your sensitive data and intellectual property can be used to generate responses to users outside the organization — such as service provider employees and hackers. There are many ways of acquiring AI outside of internal development, such as enterprise applications you are already using, packaged applications you can buy and AI add-ons (chatbots, virtual assistants, etc.). Organizations can: Buy APIs (e.g., Amazon Web Services, Google, IBM, Microsoft) and packaged applications (e.g., IBM, Microsoft, Oracle, SAP, SAS) Build open source (e.g., Python, Apache Spark, TensorFlor), data science/machine learning platforms, citizen data science tools Outsource to global and/or local consultants, specialists and/or systems integrators CIOs indicate that AI talent is not a major resource concern, and they combine both internal and external hiring to source talent needed for successful AI deployment. Four roles are key, though: data scientists, data engineers, AI engineers and business experts. AI is growing at a rapid pace with trends and technologies continuously emerging. CIOs need to be prepared for what lies ahead. Create a succinct AI strategy document that synthesizes your vision and potential benefits, audits and mitigates risks, captures KPIs, and outlines best practices for value creation. Identify sponsors for AI projects and ensure their KPIs are being measured accurately and communicated widely. Invest in data literacy programs to instill a data-driven culture. Instill responsible AI practices and make them foundational to your AI strategy, not an afterthought. John Hillery is Managing Vice President in Peer and Practitioner Research for the Gartner CIO Research Group. His current research focus is IT strategy, governance, operating models, performance measurement, and talent and evolution of the CIO role. Register for the 2022 conference and join the World's Most Important Gathering of CIOs and IT Executives™ to get the latest insights on technology and more. Recommended resources for Gartner clients*: What CIOs Need to Know About AICIO Technology and Innovation Leadership Primer for 2023 *Note that some documents may not be available to all Gartner clients. Explore Gartner’s Data and Analytics Roadmap with proven stages and activities to align all stakeholders and drive business value. Explore Gartner’s Data and Analytics Roadmap with proven stages and activities to align all stakeholders and drive business value. By clicking the ""Continue"" button, you are agreeing to the Gartner Terms of Use and Privacy Policy. Please provide the consent below I have read, understood and accepted Gartner Separate Consent Letter , whereby I agree (1) to provide Gartner with my personal information, and understand that information will be transferred outside of mainland China and processed by Gartner group companies and other legitimate processing parties and (2) to be contacted by Gartner group companies via internet, mobile/telephone and email, for the purposes of sales, marketing and research. By clicking the ""Subscribe"" button, you are agreeing to the Gartner Terms of Use and Privacy Policy. ©2023 Gartner, Inc. and/or its affiliates. All rights reserved. ©2023 Gartner, Inc. and/or its affiliates. All rights reserved. Clients receive 24/7 access to proven management and technology research, expert advice, benchmarks, diagnostics and more. Fill out the form to connect with a representative and learn more. jsbacContactjsbacContact 8 a.m. – 7 p.m. ET 8 a.m. – 5 p.m. GMT Monday through Friday Please provide the consent below I have read, understood and accepted Gartner Separate Consent Letter , whereby I agree (1) to provide Gartner with my personal information, and understand that information will be transferred outside of mainland China and processed by Gartner group companies and other legitimate processing parties and (2) to be contacted by Gartner group companies via internet, mobile/telephone and email, for the purposes of sales, marketing and research. By clicking the ""Submit"" button, you are agreeing to the Gartner Terms of Use and Privacy Policy. By clicking the """" button, you are agreeing to the Gartner Terms of Use and Privacy Policy."
49,https://www.theverge.com/2023/6/5/23749338/zoom-ai-summaries-missed-meetings,"By Emma Roth, a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO. Zoom now lets users use AI to catch up on missed meetings. The feature, which Zoom first announced in March, has finally arrived as a trial for users in “select plans,” according to a post on Zoom’s website. With Zoom IQ — the app’s AI-powered assistant — hosts can now generate summaries of meetings and send them to users through Zoom Team Chat or email, all without actually recording the meetings. It’s hard to tell how accurate (or detailed) the meeting summaries are without trying them out for ourselves, but it still seems like a much quicker way to get a recap on anything you’ve missed, as opposed to watching an entire prerecorded meeting. In addition to AI-generated meeting summaries, Zoom is launching the ability to compose messages in Team Chat using AI. The feature leverages OpenAI’s technology to create messages “based on the context of a Team Chat thread” and also lets you customize the tone or length of a message before you send it. All of these features build upon what Zoom’s IQ assistant already offers, such as the ability to create meeting highlights and chapters. In the near future, Zoom plans on rolling out several other AI-powered features through its partnership with OpenAI and Anthropic. That includes the ability to write emails with AI using context from previous meetings, phone calls, and emails as well as a way to summarize threads in Zoom Team Chat “with the click of a button.” Zoom is also working on a way for you to use AI to “discreetly” obtain an in-chat summary of a meeting when you arrive late, create whiteboard drafts with text prompts, and automatically organize ideas into categories during brainstorming sessions. According to Zoom, the company “collects data from users’ interactions with the Zoom IQ features, including inputs, messages, and AI-generated content” and could use this information to train Zoom IQ AI models (but not third-party ones) unless you choose not to share data with Zoom. Alongside Zoom, other productivity platforms, including Salesforce’s Slack and Microsoft 365, have begun incorporating AI features as well. Slack, for example, lets you reply to colleagues with ChatGPT and could soon have AI attend Huddles on your behalf, while Microsoft has rolled out an AI Copilot for its 365 apps. For now, though, only Zoom IQ’s meeting summaries and chat compose features are available as a free trial “for a limited time” to subscribers of Zoom One (Enterprise Plus, Enterprise, Business Plus, Business, Pro) and some Zoom legacy bundles (Enterprise Named Host, Enterprise Active Host, Zoom Meetings Enterprise, Zoom Meetings Business, Zoom Meetings Pro). It’s unclear how much these features will cost after the free trial, however, but Zoom spokesperson Lacretia Taylor tells The Verge that the company will reveal pricing information “in the coming months.” / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. The Verge is a vox media network © 2023 Vox Media, LLC. All Rights Reserved"
50,https://www.nytimes.com/2023/06/02/technology/ai-photo-editing.html,"An A.I.-powered version of Photoshop and the image generator Midjourney live up to the hype. Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Brian X. Chen Hello! Welcome back to On Tech: A.I., a pop-up newsletter that teaches you about artificial intelligence, how it works and how to use it. In last week’s newsletter, I shared the golden prompts for getting the most helpful answers from chatbots like ChatGPT, Bing and Bard. Now that you’re familiar with the general principle of building a relationship with A.I. — the more specific and detailed instructions you give, the better results you’ll get — let’s move on to a slightly different realm. Much of the hype and fears around generative A.I. has been about text. But there have also been rapid and dramatic developments in systems that can generate images. In many cases, these share a similar structure to text-based generative A.I., but they can also be much weirder — and lend themselves to some very fun creative pursuits. Image generators are trained on billions of images, which enable them to produce new creations that were once the sole dominion of painters and other artists. Sometimes experts can’t tell the difference between A.I.-created images and actual photographs (a circumstance that has fueled dangerous misinformation campaigns in addition to fun creations). And these tools are already changing the way that creative professionals do their jobs. Compared to products like ChatGPT, image generating A.I. tools are not as well developed. They require jumping through a few more hoops, and may cost a bit of money. But if you’re interested in learning the ropes there’s no better time to start. Last week, Adobe added a generative A.I. feature into a beta version of Photoshop, its iconic graphics software, and creators on social networks like TikTok and Instagram have been buzzing about it ever since. I have a fair amount of experience with Photoshop. When I tested the new feature, called “generative fill,” I was impressed with how quickly and competently the A.I. carried out tasks that would have taken me at least an hour to do on my own. In less than five minutes and with only a few clicks, I used the feature to remove objects, add objects and swap backgrounds. (To experiment with these tools yourself, start by signing up for a free trial of Adobe Creative Suite. Then, install the new Adobe Photoshop beta, which includes generative fill.) Once you have Photoshop beta installed, import a photo and try these tricks: To change a background, click the “object selection” icon (it has an arrow pointed at a box), then under the Select menu, click “inverse” to select the background. Next click the “generative fill” box and type in a prompt — or leave it blank to let Photoshop come up with a new background concept for you. I used these steps to edit a photo of my corgi, Max. I typed “kennel” for the prompt, and clicked “generate"" to replace the background. Here’s the before (left) and after. To remove objects, use the lasso tool. In this photo of my motorcycle, I wanted to erase a tractor behind a fence in the background. I traced around the tractor, and then I clicked the “generative fill” box and hit “generate” without entering a prompt. The software correctly removed the tractor and filled in the background while leaving the fence intact. Photo editors at The New York Times do not enhance or alter photos, or generate images using artificial intelligence. But my first thought after testing generative fill was that photo editors working in other contexts, like marketing, could be soon out of work. When I shared this theory with Adobe’s chief technology officer, Ely Greenfield, he said that it might make photo editing more accessible, but he was optimistic that humans would still be needed. “I can make really pretty images with it, but frankly, I still make boring images,” he said. “When I look at the content that artists create when you put this in their hands versus what I create, their stuff is so much more interesting because they know how to tell a story.” I confess that what I’ve done with generative fill is far less exciting than what others have been posting on social media. Lorenzo Green, who tweets about A.I., posted a collage of famous album covers, including Michael Jackson’s “Thriller” and Adele’s “21” that were expanded with generative fill. The results were quite entertaining. (One note: If installing Photoshop feels daunting, a quicker way to test Adobe’s A.I. is to visit the Adobe Firefly website. There, you can open the generative fill tool, upload an image and click the “add” tool to trace around a subject, such as a dog. Then click “background” and type in a prompt like “beach.”) Tools like DALL-E and Midjourney can create entirely new images in seconds. They work similarly to chatbots: You type in a text prompt — the more specific, the better. To write a quality prompt, start with the medium you’d like to emulate, followed by the subject and any extra details. For example, typing “a photograph of a cat wearing a sweater in a brightly lit room” in the DALL-E prompt box will generate something like this: DALL-E, which is owned by Open AI, the maker of ChatGPT, was one of the first widely available A.I. image generators that was simple for people to use. For $15, you get 115 credits; one credit can be used to generate a set of four images. Midjourney, another popular image generator, is a work in progress, so the user experience is not as polished. The service costs $10 a month, and entering prompts can be a little more complicated, because it requires joining a separate messaging app, Discord. Nonetheless, the project can create high-quality, realistic images. To use it, join Discord and then request an invitation to the Midjourney server. After joining the server, inside the chat box, type “/imagine” followed by a prompt. I typed “/imagine a manga cover of a corgi in a ninja turtle costume” and generated a set of convincing images: Though it’s fine to type in a basic request, some have found obscure prompts that generated exceptional results (Beebom, a tech blog, has a list of examples). At Columbia University, Lance Weiler is teaching students how to leverage A.I., including Midjourney, to produce artwork. Whichever tool you use, bear in mind that the onus is on you to use this tech responsibly. Technologists warn that image generators can increase the spread of deepfakes and misinformation. But the tools can also be used in positive and constructive ways, like making family photos look better and brainstorming artistic concepts. Next week, I’ll share some tips on how to use A.I. to speed up aspects of office jobs, such as drafting talking points and generating presentation slides. In case you’re wondering, the delightfully demented image at the top of this newsletter was created by a human — the illustrator Charles Desmarais — not by A.I. Brian X. Chen is the lead consumer technology writer for The Times. He reviews products and writes Tech Fix, a column about the social implications of the tech we use. Before joining The Times in 2011, he reported on Apple and the wireless industry for Wired. @bxchen Executives from leading A.I. companies, including OpenAI and Google, warned that the technology they were building might one day pose an existential threat to humanity. One of the most urgent warnings about the risks of A.I. has come from Geoffrey Hinton, whom many consider to be the godfather of artificial intelligence. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. The Age of A.I. In Silicon Valley’s hacker houses, young A.I. entrepreneurs are partying, innovating — and hoping not to get crushed by the big guys. A lawyer representing a man who sued an airline relied on ChatGPT to help prepare a court filing. It did not go well. How has the Shoggoth, an octopus-like creature from a science fiction story, come to symbolize the state of artificial intelligence? Kevin Roose explains. Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach."
51,https://www.wired.com/story/how-ai-protects-inbox-phishing/,"To revist this article, visit My Profile, then View saved stories. To revist this article, visit My Profile, then View saved stories. When Aparna Pappu, vice president and general manager of Google Workspace, spoke at Google I/O on May 10, she laid out a vision for artificial intelligence that helps users wade through their inbox. Pappu showed how generative AI can whisper summaries of long email threads in your ear, pull in relevant data from local files as you salsa together through unread messages, and dip you low to the ground as it suggests insertable text. Welcome to the inbox of the future.  While the specifics of how it’ll arrive remain unclear, generative AI is poised to fundamentally alter how people communicate over email.  A broader subset of AI, called machine learning, already performs a kind of safety dance long after you've logged off. “Machine learning has been a critical part of what we’ve used to secure Gmail,” Pappu tells WIRED. A few, errant clicks on a suspicious email can wreak havoc on your security, so how does machine learning help deflect phishing attacks? Neil Kumaran, a product lead at Google who focuses on security, explains that machine learning can look at the phrasing of incoming emails and compare it to past attacks. It can also flag unusual message patterns and sniff out any weirdness emanating from the metadata. Machine learning can do more than just flag dangerous messages as they pop up. Kumaran points out that it also can be used to track the people responsible for phishing attacks. He says, “At the time of account creation, we do evaluations. We try to figure out, ‘Does it look like this account is going to be used for malicious purposes?’” In the event of a successful phishing attack on your Google account, AI is involved with the recovery process as well. The company uses machine learning to help decide which login attempts are legit. “How do we extrapolate intelligence from user reports to identify attacks that we may not know about, or at least start to model the impact on our users?” asks Kumaran. The answer from Google, like the answer to many questions in 2023, is more AI. This instance of AI is not a flirty chatbot teasing you with long exchanges late into the night; it’s a burly bouncer kicking out the rabble-rousers with its algorithmic arms crossed. On the reverse side, what’s instigating even more phishing attacks on your email inbox? I’ll give you one guess. First letter “A,” last letter “I.” For years, security experts have warned about the potential for AI-generated phishing attacks to overwhelm your inbox. “It’s very, very hard to detect AI with the naked eye, either through the dialect or through the URL,” says Patrick Harr, CEO of SlashNext, a messaging security company. Just like when people use AI-generated images and videos to create fairly convincing deepfakes, attackers may use AI-generated text to personalize phishing attempts in a way that’s difficult for users to detect. Multiple companies focused on email security are working on models and using machine-learning techniques in an effort to further protect your inbox. “We take the corpus of data that’s coming in and do what’s called supervised learning,” says Hatem Naguib, CEO of Barracuda Networks, an IT security firm. In supervised learning, someone adds labels to a portion of the email data. Which messages are likely to be safe? Which ones are suspicious? This data is extrapolated to help a company flag phishing attacks with machine learning. It's a valuable aspect of phishing detection, but attackers remain on the prowl for ways to circumvent protections. A persistent scam about a made-up Yeti Cooler giveaway evaded filters last year with an unexpected kind of HTML anchoring.  Cybercriminals will remain intent on hacking your online accounts, especially your business email. Those who utilize generative AI may be able to better translate their phishing attacks into multiple languages, and chatbot-style applications can automate parts of the back-and-forth messages with potential victims. Despite all of the possible phishing attacks enabled by AI, Aparna Pappu remains optimistic about the continued development of better, more refined security protections. “You’ve lowered the cost of what it takes to potentially lure someone,” she says. “But, on the flip side, we’ve built up greater detection capabilities as a result of these technologies.” 📧 Get the best stories from WIRED’s iconic archive in your inbox 🎧 Our new podcast wants you to Have a Nice Future The explosive legacy of the pandemic hand sanitizer boom Scientists gave people psychedelics—then erased their memory I asked AI chatbots to help me shop. They all failed The race is on to crack an artist’s “test” signal from aliens The speedrunners trying to break Tears of the Kingdom ⛺ Embrace the new season with the Gear team’s best picks for best tents, umbrellas, and robot vacuums Lily Hay Newman Lily Hay Newman More From WIRED © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
52,https://www.theguardian.com/technology/2023/jun/04/ai-poses-national-security-threat-warns-terror-watchdog,"Security services fear the new technology could be used to groom vulnerable people The creators of artificial intelligence need to abandon their “tech utopian” mindset, according to the terror watchdog, amid fears that the new technology could be used to groom vulnerable individuals. Jonathan Hall KC, whose role is to review the adequacy of terrorism legislation, said the national security threat from AI was becoming ever more apparent and the technology needed to be designed with the intentions of terrorists firmly in mind. He said too much AI development focused on the potential positives of the technology while neglecting to consider how terrorists might use it to carry out attacks. “They need to have some horrible little 15-year-old neo-Nazi in the room with them, working out what they might do. You’ve got to hardwire the defences against what you know people will do with it,” said Hall. The government’s independent reviewer of terrorism legislation admitted he was increasingly concerned by the scope for artificial intelligence chatbots to persuade vulnerable or neurodivergent individuals to launch terrorist attacks. “What worries me is the suggestibility of humans when immersed in this world and the computer is off the hook. Use of language, in the context of national security, matters because ultimately language persuades people to do things.” The security services are understood to be particularly concerned with the ability of AI chatbots to groom children, who are already a growing part of MI5’s terror caseload. As calls grow for regulation of the technology following warnings last week from AI pioneers that it could threaten the survival of the human race, it is expected that the prime minister, Rishi Sunak, will raise the issue when he travels to the US on Wednesday to meet President Biden and senior congressional figures. Back in the UK, efforts are intensifying to confront national security challenges posed by AI with a partnership between MI5 and the Alan Turing Institute, the national body for data science and artificial intelligence, leading the way. Alexander Blanchard, a digital ethics research fellow in the institute’s defence and security programme, said its work with the security services indicated the UK was treating the security challenges presented by AI extremely seriously. “There’s a lot of a willingness among defence and security policy makers to understand what’s going on, how actors could be using AI, what the threats are. “There really is a sense of a need to keep abreast of what’s going on. There’s work on understanding what the risks are, what the long-term risks are [and] what the risks are for next-generation technology.” Last week, Sunak said that Britain wanted to become a global centre for AI and its regulation, insisting it could deliver “massive benefits to the economy and society”. Both Blanchard and Hall say the central issue is how humans retain “cognitive autonomy” – control – over AI and how this control is built into the technology. The potential for vulnerable individuals alone in their bedrooms to be quickly groomed by AI is increasingly evident, says Hall. On Friday, Matthew King, 19, was jailed for life for plotting a terror attack, with experts noting the speed at which he had been radicalised after watching extremist material online. Sign up to First Edition Archie Bland and Nimo Omer take you through the top stories and what they mean, free every weekday morning after newsletter promotion Hall said tech companies need to learn from the errors of past complacency – social media has been a key platform for exchanging terrorist content in the past. Greater transparency from the firms behind AI technology was also needed, Hall added, primarily around how many staff and moderators they employed. “We need absolute clarity about how many people are working on these things and their moderation,” he said. “How many are actually involved when they say they’ve got guardrails in place? Who is checking the guardrails? If you’ve got a two-man company, how much time are they devoting to public safety? Probably little or nothing.” New laws to tackle the terrorism threat from AI might also be required, said Hall, to curb the growing danger of lethal autonomous weapons – devices that use AI to select their targets. Hall said: “You’re talking about [This is] a type of terrorist who wants deniability, who wants to be able to ‘fly and forget’. They can literally throw a drone into the air and drive away. No one knows what its artificial intelligence is going to decide. It might just dive-bomb a crowd, for example. Do our criminal laws capture that sort of behaviour? Generally terrorism is about intent; intent by human rather than intent by machine.” Lethal autonomous weaponry – or “loitering munitions” – have already been seen on the battlefields of Ukraine, raising morality questions over the implications of the airborne autonomous killing machine. “AI can learn and adapt, interacting with the environment and upgrading its behaviour,” Blanchard said."
53,https://www.timesofisrael.com/openais-sam-altman-says-israel-will-have-huge-role-to-play-in-ai-revolution,"Sharon Wrobel is a tech reporter for The Times of Israel. OpenAI CEO Sam Altman said Monday that he is impressed with the talent pool in Israel and expressed his confidence that the local tech ecosystem will play a “huge role” in the artificial intelligence revolution transforming the world in the coming years. “There are two things I have observed that are particular about Israel: the first is talent density and the second is the relentlessness, drive, and ambition of Israeli entrepreneurs,” Altman said at an event at Tel Aviv University. “Those two things together are optimal to lead to incredible prosperity both in terms of AI research and AI applications.” Altman arrived in Israel this week as part of a worldwide tour to meet with AI users and developers as well as policymakers. At Tel Aviv University, Altman and Ilya Sutskever, the creators behind ChatGPT — the viral chatbot released late last year that mimics human writing — were hosted by Dr. Nadav Cohen from TAU’s School of Computer Science. ChatGPT is a tool that is based on a so-called large language model trained with text data to answer questions, or prompts, as a human would. Altman and Sutskever discussed the threats and challenges that AI and superintelligence will bring in the future and answered questions from students, data scientists, AI developers, and high-tech professionals. With support of tech billionaire Elon Musk, Altman set up OpenAI in 2015 as a research and development lab with a mission to ensure that artificial intelligence benefits all of humanity. Altman founded the AI lab together with Sutskever who also serves as the chief scientist at OpenAI. Russian-born Sutskever grew up in Israel and moved to Canada with his family at the age of 16. The two high-tech pioneers encouraged Israeli developers and entrepreneurs to delve into the unchartered territory of AI, which they predict has a huge number of opportunities and positive applications. “I will just say go for it, just do it,” Sutskever enthused. At the same time, both Altman and Sutskever emphasized the need to take the existential threats of AI seriously by creating a frontier regulatory body, similar to nuclear power control bodies, in an effort to limit it and use it responsibly. With the swift development of AI models and its rapid use, lawmakers around the world are contemplating regulation of the technology and how to deal with safety issues and other potential dangers. Prime Minister Benjamin Netanyahu on Monday announced plans for a national AI policy in both the civilian and the security spheres. Netanyahu said he spoke Sunday night with Tesla’s Musk about the need for governments to understand both the opportunities and the dangers of AI and about Israel turning into a “significant global player in the field.” “We are at the dawn of a new era for humanity, an era of artificial intelligence,” Netanyahu said. “Things are changing at a dizzying pace and Israel must formulate a national policy on this issue.” Netanyahu said that during a phone conversation with Altman, the tech entrepreneur also expressed confidence that Israel could become a “main global player in the field.” During the conversation, the two discussed “opportunities and challenges facing the world and the State of Israel in connection to AI,” according to the Prime Minister’s Office. Part of the conversation also focused on cooperation in the field of AI development. Altman is not planning to meet Netanyahu in person on the AI celebrity’s frenetic trip to Israel. Netanyahu’s and Altman’s offices denied a formal request having been made from the Prime Minister’s Office, amid reports that Altman eschewed a personal meeting. Earlier on Monday, Altman met with President Isaac Herzog, together with Sutskever, who also serves as OpenAI’s chief scientist; its COO, Brad Lightcap; and its VP of Public Policy, Anna Makanju. After that, Altman visited Microsoft’s research and development center in Israel. The US tech giant has invested billions of dollars in OpenAI. Altman told Herzog that his visit to Israel is “very special” to him. “The rate at which the tech and startup community in Israel is embracing AI is incredible to watch,” Altman said. “The energy on making use of the technology and its positive benefits is fantastic to see, and I am sure Israel will play a huge role – it’s tech community is truly amazing.” The two discussed the risks and benefits of AI and the fast pace the technology is developing. Herzog uttered his personal interest in leading a discussion within Israel and worldwide on the ethics and morality, and other aspects of AI technology. “Clearly side by side with the great opportunities of this incredible technology, there are also many risks to humanity and to the independence of human beings in the future,” Herzog remarked. “Medicine will be dramatically improved by AI, however, issues of ethics, and morality, questions of fake news, show the risks.” During the visit at Microsoft Israel’s R&D center, Altman met with its general manager Michal Braverman-Blumenstyk as well as with employees. “The advancements made by OpenAI are driving unparalleled human progress, comparable to the impact of the Internet revolution,” commented Braverman-Blumenstyk. “I was truly impressed by Sam’s dedication to promoting the responsible use of artificial intelligence for positive change.” Earlier this year, Microsoft announced that it was backing OpenAI and in recent weeks as the US tech giant plans to bring AI to the masses, it has started to make upgrades to its systems and applications by integrating ChatGPT features into its products, including the Teams platform, and the Bing search engine. It is also expected to adapt the app to its Office suite. “By integrating OpenAI technologies into Microsoft products, we position ourselves at the forefront of the global technology landscape,” said Braverman-Blumenstyk. In response to a question about the feasibility of opening an OpenAI local branch in Israel, Altman said that the company prefers to work together and in one location, but that it is examining different options for investing in Israel, Microsoft said in a statement. Microsoft currently operates a number of development centers in Israel including in Herzliya, Haifa, Tel Aviv, and Nazareth. The tech giant employs more than 2,000 people in Israel, working mostly in R&D, on projects including cybersecurity, AI technologies, big data, and healthcare. Following the blitz visit to Israel on Monday, Altman is scheduled to travel to Jordan, Qatar, the United Arab Emirates, India, and South Korea this week. As part of the OpenAI world tour started in early May, the Jewish-born tech founder already visited Toronto, Washington DC, Rio de Janeiro, Lagos, and Lisbon. Altman also met with entrepreneurs, heads of state, and policymakers in Madrid, Warsaw, Paris, London, and Munich. Do you rely on The Times of Israel for accurate and insightful news on Israel and the Jewish world? If so, please join The Times of Israel Community. For as little as $6/month, you will: We’re really pleased that you’ve read X Times of Israel articles in the past month. That’s why we started the Times of Israel eleven years ago - to provide discerning readers like you with must-read coverage of Israel and the Jewish world. So now we have a request. Unlike other news outlets, we haven’t put up a paywall. But as the journalism we do is costly, we invite readers for whom The Times of Israel has become important to help support our work by joining The Times of Israel Community. For as little as $6 a month you can help support our quality journalism while enjoying The Times of Israel AD-FREE, as well as accessing exclusive content available only to Times of Israel Community members. Thank you, David Horovitz, Founding Editor of The Times of Israel Today's Daily Briefing Blackouts shine light on ill-preparedness for climate change SAVING LIVES AROUND THE GLOBE Signed, Sealed, Delivered? A fresh look at Israelâs founding moral compass"
54,https://www.cnbc.com/2023/06/03/artificial-intelligence-not-a-fad-new-fund-capitalizes-on-boom.html,"Help for Low Credit Scores All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best Credit Unions Best Mortgages for Small Down Payment Best Mortgages for No Down Payment Best Mortgages with No Origination Fee Best Mortgages for Average Credit Score Adjustable Rate Mortgages Affording a Mortgage Best Life Insurance Best Homeowners Insurance Best Renters Insurance Best Car Insurance All Credit Monitoring Best Credit Monitoring Services Best Identity Theft Protection How to Boost Your Credit Score Credit Repair Services All Personal Finance Best Budgeting Apps Best Expense Tracker Apps Best Money Transfer Apps Best Resale Apps and Sites Buy Now Pay Later (BNPL) Apps Best Debt Relief All Small Business Best Small Business Savings Accounts Best Small Business Checking Accounts Best Credit Cards for Small Business Best Small Business Loans Best Tax Software for Small Business Best Tax Software Best Tax Software for Small Businesses All Help for Low Credit Scores Best Credit Cards for Bad Credit Best Personal Loans for Bad Credit Best Debt Consolidation Loans for Bad Credit Personal Loans if You Don't Have Credit Best Credit Cards for Building Credit Personal Loans for 580 Credit Score or Lower Personal Loans for 670 Credit Score or Lower Best Mortgages for Bad Credit Best Hardship Loans How to Boost Your Credit Score Best IRA Accounts Best Roth IRA Accounts Best Investing Apps Best Free Stock Trading Platforms A major ETF firm provider is betting the artificial intelligence boom is just starting. Roundhill Investments launched the Generative AI & Technology ETF (CHAT) less than 20 days ago. It's the first-ever exchange-traded fund designed to track companies involved in generative AI and other related technologies. ""These companies, we believe, are not just a fad. They're powering something that could be as ubiquitous as the internet itself,"" the firm's chief strategy officer, Dave Mazza, told ""ETF Edge"" this week. ""We're not talking about hopes and dreams [or] some theme or fad that could happen 30 years in the future which may change the world."" Mazza notes the fund includes not just pure play AI companies like C3.ai but also large-cap tech companies such as Microsoft and AI chipmaker Nvidia. Nvidia is the fund's top holding at 8%, according to the company website. Its shares are up almost 42% over the past two months. Since the beginning of the year, Nvidia stock has soared 169%. ""This [AI] is an area that's going to get a lot of attention,"" said Mazza. His bullish forecast comes amid concerns AI is a price bubble that will pop and take down the Big Tech rally. In a recent interview on CNBC's ""Fast Money,"" Richard Bernstein Advisors' Dan Suzuki — a Big Tech bear since June 2021 — compared the AI rally to the dot-com bubble in the late 1990s. ""People jump from narrative to narrative,"" the firm's deputy chief investment officer said on Wednesday. ""I love the technology. I think the applications will be huge. That doesn't mean it's a good investment."" The CHAT ETF is up more than 8% since it started trading on May 18. Got a confidential news tip? We want to hear from you. Sign up for free newsletters and get more CNBC delivered to your inbox Get this delivered to your inbox, and more info about our products and services.  © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Data also provided by"
55,https://theconversation.com/ai-makes-our-journalism-even-more-important-207016,"Notes from The Conversation newsroom Editor, The Conversation View all partners One of the troubling things about modern life is the fact that so many of our ordinary decisions are manipulated by vested interests and opaque forces. This isn’t entirely a new phenomenon, we have always been easily swayed – by the slick advertisers of Madison Avenue, the chocolate bars placed strategically at checkouts, the newspapers screaming what should alarm us the most today. But the social media age we are living in today makes that sort of manipulation seem like child’s play. The algorithms that shape contemporary reality are far more pervasive, and far more of a health hazard, than any 30-second TV ad spot. They’re designed to make us anxious and angry, get us hooked, spy on us, mine our data and monetise us, as Harvard academic Soshana Zuboff laid bare in her chilling monograph on surveillance capitalism. And now we have a new and even more potent reality-shaper to contend with, in the form of artificial intelligence. The Washington Post recently published an article on the ways in which this new technology is trawling the internet to teach itself about the world. Using as an example Google’s C4 Data set, The Post listed some of the top 200 sites that are providing “facts” to teach AI. A lot of the sites got low scores for trustworthiness, such as RT.com which came in at 65 and is a Russian state-backed propaganda site. Far-right news site Breitbart.com came in at 159. There were other sites that were offensive or dodgy for various reasons, including piracy. Google has announced that soon, when you search a question on its site, the first result displayed will be an AI-generated answer. There is every chance now that answers based on disinformation and misinformation could be part of the world presented to us by Google and other powerful digital platforms. On the upside, The Washington Post also found quite a few reputable sources high on the list: the likes of The Washington Post and The Conversation, which came in at 153. We were encouraged to learn that The Conversation rated in the top 200 global sources of evidence-based information for this data set – it’s quite an achievement for a digital publisher that provides articles by academic experts. But the sad truth is that reliable sources of information are thin on the ground and increasingly hard to finance given the collapsing media business models. The bright hope offered by digital start-ups like Vice and Buzzfeed has vanished in recent weeks as they are shuttered or sold. Meanwhile most traditional media companies can’t make enough money to operate the sorts of newsrooms they once did. So they concentrate on breaking news and big investigations and many of the things that matter most – education, health, science, arts, public policy – are glossed over. Evidence-based information informed by the latest thinking and research could not be more important. Google executive Liz Reid has said the company’s AI model will use information from “the open web” – that means websites that aren’t behind a paywall or login page. But many of the most trustworthy sources of information – academic journals, high-quality newspapers – are locked behind a paywall. So there is a real chance AI-powered search engines will be biased toward low-quality information, or even falsehoods. Everyone needs access to the best and most up-to-date information to make important decisions in their lives, whether it be who to vote for or what to do about an unusual cough or how to plan for climate change. That’s why there has never been a more important time to invest in quality, evidence-based journalism that is free to read. At The Conversation, we work with academics to share their knowledge and research as widely as possible. We give away our journalism and, instead of creating a paywall and charging subscribers, we rely on the kindness of people like you to help fund our not-for-profit newsroom. The laborious work of journalists and academics – researching, fact-checking, evaluating sources – is more important than ever. We do this work and make it freely available to serve the public good, but we need your help to ensure that we don’t go the way of Buzzfeed or Vice. Please make a donation to The Conversation of whatever you can afford. Every little bit helps us provide access to quality information that empowers everyone to make better decisions. Copyright © 2010–2023, The Conversation"
56,https://www.livemint.com/market/stock-market-news/the-ai-revolution-is-here-should-you-fire-your-fund-manager-11686022691178.html,"My in-laws celebrated their 50th wedding anniversary last month. God bless them. Now, as their only son-in-law who does writing for a living, I was requested to write their invitation card. And I happily obliged. However, little did I know that there isn't a lot of overlap between writing about stocks and writing invitations. It isn't as easy as it seems. Besides, my reputation as a good writer in the eyes of my in-laws, was also at stake. This is when I hit upon an idea - outsourcing this task to ChatGPT. This small AI tool that is winning rave reviews the world over, was perfectly suited for it. After all, writing a small invitation note would be a child's play for a software that can put together a brand-new crime thriller without as much as breaking a sweat. Yes, that's right. ChatGPT not only helps you edit your book writing. It can even write the entire book for you. Welcome to the world of Artificial Intelligence. So, getting back to the assignment from my in-laws, I fed instructions to ChatGPT for the kind of invitation letter I wanted. And voila, out came a delightful small note. All I had to do were a few edits and the note was ready. Needless to say, the in-laws were mighty impressed. As for me, I easily saved couple of hours' worth of effort. In other words, ChatGPT made me more productive. It freed up my time to pursue other activities. Well, these are early days. But there's no denying that this gem of a software, powered by slick hardware, is having the exact same impact on people's lives across the spectrum. It's making them more productive by doing things faster and at the same time, producing a much better-quality output. There are reports galore of ChatGPT donning multiple hats i.e. turning into a coder, an editor, and even a resume writer of the highest quality, with minimum fuss. Little wonder, experts across the world are calling AI or Artificial Intelligence of which ChatGPT is a live example, one of the biggest inventions in human history. In fact, Bill Gates, one of the pioneers of the tech revolution, has termed it as revolutionary as the mobile phone and the internet. There's a solid reason behind why Gates feels this way. He has been in touch with the team behind ChatGPT for close to 6-7 years now and in mid-2022, he gave the team a formidable challenge. Train the AI they are working on to pass an Advanced Placement Biology exam. Make it capable of answering questions that it hasn't been specifically trained for. If the team manages to achieve this feat, then they will have made a true breakthrough in the eyes of Bill Gates. Well, the results shocked Bill Gates. He thought that the challenge would keep the team occupied for 2-3 years. They finished it in just a few months. Not only did the AI pass the exam, it did so with flying colours. The result convinced Gates that something truly revolutionary has now arrived. In case you are still struggling to figure what makes AI truly revolutionary, read the Bill Gates instruction to the AI team once again. Bill Gates wanted the team to make the AI capable of answering questions that it hasn't been specifically trained for. Here, in this very sentence, is hidden the true essence of AI. You see, the computers that we have used uptill now weren't capable of thinking on their own. Their outputs were entirely based on a set of instructions or the code as they are called in the technical language. To put it crudely, no matter how fast or how efficient the computer, it is still a calculating machine at its core. However, AI is a different beast altogether. It's an intelligence that has been inspired by the natural or the human form of intelligence. In other words, it tries to mimic the human brain. Just as the human brain is capable of taking in tens of thousands of inputs and then processing them to arrive at a decision, something similar is happening inside the machines that are powering Artificial Intelligence or AI. In fact, Bill Gates has summed it up brilliantly by way of the following paragraph... For decades, the question was when computers would be better than humans at something other than making calculations. Now, with the arrival of machine learning and large amounts of computing power, sophisticated AIs are a reality, and they will get better very fast. Here's another way of putting it. Imagine that you need to arrange your cupboard and you take help from robots. Now, a robot powered by a simple computer program, will arrange it the same way every single time. However, it's entirely possible that a robot powered by AI will take your personal preference into account while doing the same. So, it will arrange the cupboard of your wife in a different way than yours and so on. Ditto for making tea. While the conventional robot will make the same tea for all the family members, an AI powered robot is very much capable of serving you a masala tea and your wife a green tea if that's what it has been trained to do. Let me make a broad generalisation here. You see, computers have evolved from performing calculations to now learning on a very large data set. This learning is then being used for reasoning and problem solving, very much like us humans. However, they do not have the limitations of speed and processing power of the human brain. So they're proving to be more intelligent and productive. Now, just as every coin has two sides, so does Artificial Intelligence. It's not only its benefits and enormous advantages that are doing the rounds of internet and social media. A lot of experts are also talking about the downsides and the disadvantages that come with it. Foremost among these is the fear of job loss across a wide variety of industries. We are well aware that software has already eaten the lunch of a lot of the workforce that was engaged in low level work. Now, its elder brother i.e. Artificial Intelligence, will go after the high level work. As it keeps becoming more sophisticated, there don't seem to be too many jobs that are safe from the AI onslaught. We've already seen how ChatGPT has shown promise to code better than the best coders, write fiction better than the best fiction writers, and even thrash out arguments better than the best lawyers. In fact, here's a sobering statistic. Back in 2013, a University of Oxford study found that 47% of US jobs could be eliminated by AI over the next 20 years. Now, I usually take such studies with a pinch of salt. However, the more I see AI at work and the output it's capable of generating, the more the spectre of widespread job replacements looks real. What about investing and stock market research though? Is AI likely to snatch the fund manager's job as well? Can it make better investment recommendations than the smartest fund managers? More importantly, should one start looking at it as a genuine alternative to investment advisors and money managers? Well, a question of exactly the same kind was posed to Messrs. Warren Buffett and Charlie Munger at the recent Berkshire Hathaway AGM. Here's what Munger said in response to a question about whether AI technology would have a positive impact on stocks, the market and society as a whole. Well, if you went into BYD's factories in China, you would see robotics going at an unbelievable rate. So, we're going to see a lot more robotics in the world. I am personally skeptical of some of the hype that is going into artificial intelligence. I think old-fashioned intelligence works pretty well. Not surprisingly, his partner Warren Buffett is also skeptical of AI tech's impact on stocks and the stock market. The Oracle of Omaha opined that AI tech may have the potential to change everything in the world. However, it cannot change how men think and behave. I believe that's a wonderful observation. Investing is first and foremost about trying to keep your head when everyone is losing theirs. It's about being fearful when others are greedy and greedy when others are fearful. It's also about not giving in to the herd mentality and having one's own independent opinion backed by facts and figures. I don't think AI tech is in a position to influence any of these things in a meaningful way nor will it be 10 years from now. Mr Market is likely to remain a moody, temperamental fellow with or without AI. He will continue to have mood swings. He will continue to keep pricing stocks that either reflect too much optimism or a lot of pessimism from time to time. AI technology could perhaps smoothen the volatility in the stock market. But for all you know, it can even increase it. What is certain is that the stock markets will continue to remain volatile, thus providing opportunities to sensible investors of the value investing kind. So, to sum up, the AI genie is well and truly out of the bottle. And as an optimist, it's my belief that it will prove to be a net positive for the society. However, as far as investing is concerned, AI is unlikely to have any significant impact unless it figures out a way to change human gene and in the process, fundamentally change how humans behave. Until then, keep your fund managers and your advisors close to you and keep following sensible investment principles. You should be okay over the long term. Disclaimer: This article is for information purposes only. It is not a stock recommendation and should not be treated as such. This article is syndicated from Equitymaster.com. Get the best recommendations on Stocks, Mutual Funds and more based on your Risk profile! Download the Mint app and read premium stories Log in to our website to save your bookmarks. It'll just take a moment. You are just one step away from creating your watchlist! Oops! Looks like you have exceeded the limit to bookmark the image. Remove some to bookmark this image. Your session has expired, please login again. You are now subscribed to our newsletters. In case you can’t find any email from our side, please check the spam folder. This is a subscriber only feature Subscribe Now to get daily updates on WhatsApp"
57,https://www.foxnews.com/opinion/kamala-harris-trusted-ai-regulation,"This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper. The ""Dancin' In The Country"" musician discusses the realm of possibilities for artificial intelligence within the music industry while attending the ACM Awards. Recently, the White House decided that appointing an unqualified, politicized leader is perfect for tackling the complex issue of AI regulation. Kamala Harris, who has now become the AI czar, will likely lead America into a very gloomy future. The nation must correct this blunder before it’s too late.   We can only solve a problem by asking the right questions and Harris and the polarized Congress are clearly unable to do so. The United States must replace her with an unbiased committee of experts who can protect and fully develop effective AI regulations. It should also shield decisions moving forward from the current toxic partisan environment.   Unless you’ve been living under a rock, every newsfeed and broadcast available has explained how important it is for us to control AI. So, you would assume that the White House would adopt an all-hands-on-deck approach to preventing the downfall of humanity.   BIDEN SAYS ARTIFICIAL INTELLIGENCE SCIENTISTS WORRIED ABOUT TECH OVERTAKING HUMAN THINKING AND PLANNING Instead, they’ve been asking the public about accountability measures, funneling $140 million into research, development, and budget proposals and making symbolic appointments tainted with incompetence and political motives.  Vice President Kamala Harris who failed as border czar now also gets to be AI czar. Regardless of whether the vice president is qualified for her current job, she was never a good option to become the AI czar. Her knowledge of the expansive, almost uncontrollable tech is minimal. Former and current presidential staff call her a bully and report that the White House has become an unhealthy environment with her in it. We also know about her failure to improve the southern border crisis.   Harris is not only an uneducated regulator of AI but an inappropriate leader to drive results in a high-stakes game. In parallel, an unbiased examination of the recent congressional hearings with the CEO of Open AI (creator of ChatGPT) and the CEO of TikTok, reveals the lack of knowledge of our representatives of what is at stake. Offering a no-win situation between the decision-makers in both the executive and legislative branches.  Congress and Harris, fueled with their political agendas and handicapped by their lack of understanding of AI implications that go far beyond privacy protection will have little chance of formulating a solution forward.   The only way we can find the correct solution is through an unbiased committee of experts spanning legal, social, economic, and technical modalities. This group should fully explore and grasp how AI will affect our national and personal security as well as the unprecedented economic opportunities.   They must explore the real dangers of AI unlike the newly released AI Risk Management Framework and the Blueprint for an AI Bill of Rights which are fixated on the issue of privacy and discrimination.  An independent ""Warren Commission"" like an unbiased and apolitical leader must be appointed to lead a committee of experts that would have a broad understanding of AI applications and their social and economic integrations that will help us develop effective policies. A committee of experts that is not made up of Microsoft, Google, OpenAI and other usual suspects with significant monetary interests.  AI regulation is a wide-reaching issue not segregated by national borders. Our mistakes in the U.S. or any country will have an effect on the greater population, which is why we need a strong group of critical thinkers. The wise words of business educator Marshall Goldsmith are extremely applicable to our current predicament. What got us here, won’t get us there: Selfish concerns and the prioritization of privacy and budget won’t result in streamlined AI regulation.  Political maneuvering and on-air showcasing for public appeasement from Biden and Harris meeting with big tech CEOs and Altman testifying in front of Congress do not alleviate the possibility of AI destroying the world as we know it.  Sam Altman, chief executive officer of OpenAI, during a fireside chat at University College London (UCL) in London, UK, on Wednesday, May 24, 2023.  (Photographer: Chris J. Ratcliffe/Bloomberg via Getty Images) This is why we need an independent committee to balance the importance of personal privacy with identifying dangerous actors. We need a policing system for classifying and defining suspicious signals. To take down malicious actors, the public and AI providers must also surrender a sliver of their privacy. If not, this exciting technology could be repurposed as a weapon of mass destruction.  CLICK HERE TO GET THE OPINION NEWSLETTER Let’s stop the political nonsense and showcasing in pointless meetings. Let’s not put our national security and humanity at risk as we put politics over logic. Speeches are nice and it is true that companies have an ethical, legal and moral responsibility to ensure that their AI is safe.   It is also true that harmful technology can come from companies with the best intentions. However, AI implications are very different from other previously faced global threats like nuclear annihilation. Former secretary of State Henry Kissinger believes that U.S.-China tensions are mimicking Cold War times, but the possibility for destruction has raised drastically.   CLICK HERE TO GET THE FOX NEWS APP Foreign foes can leverage AI to develop weapons more fatal than previous nuclear threats. The legislation and policies needed now are not limited to controlling countries or big companies. The threat can come from a few smart people with computers anywhere in the world.   Legislation and policies have to include social change as well as serious penalties. To adequately protect the world, regulations must be divorced from political beliefs, and we must move past our comfort zone for the benefit of humanity.  CLICK HERE TO READ MORE FROM SID MOHASSEB Sid Mohasseb is Adjunct Professor in Dynamic Data-Driven Strategy at the University of Southern California and is a former National Strategic Innovation Leader for Strategy at KPMG. He is the author of The Caterpillar's Edge: Evolve, Evolve Again, and Thrive in Business and You Are Not Them: The Authentic Entrepreneur's Way. Get the recap of top opinion commentary and original content throughout the week. You've successfully subscribed to this newsletter! This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper."
58,https://techcrunch.com/2023/06/03/artificial-intelligence-vc-portfolio/,"Welcome to the TechCrunch Exchange, a weekly startups-and-markets newsletter. It’s inspired by the daily TechCrunch+ column where it gets its name. Want it in your inbox every Saturday? Sign up here. This week, I explore how one VC is feeling about a big chunk of her portfolio pivot into AI. And I have some updates on my new fixation: internal developer portals. — Anna TechCrunch+ recently noted a paradox: “VCs love to talk about AI, but they aren’t writing as many checks as you might think.” Maybe because they don’t need to make new investments to own shares in AI companies. “Lots of existing investments started pivoting into AI — roughly two companies every single month,” Day One Ventures founder and general partner Masha Bucher told me. For instance, email app Superhuman is one of the startups in her portfolio that recently ventured into AI."
59,https://www.the-sun.com/tech/8293960/artificial-intelligence-wiping-jobs-chatgpt/,"FEARS about artificial intelligence systems causing widespread unemployment to continue to grow as workers across the country see work taken away from them.  At least one content writer recently revealed his work would be replaced by ChatGPT – virtually killing his business in a matter of days.  “It wiped me out,” Eric Fein told the Washington Post.  Earlier this year, the 34-year-old held business with 10 ongoing contracts.  As he charged $60 an hour for his work, it ultimately made up half of his annual income.  However, in March, Fein was notified by his largest client that the company would be using ChatGPT instead.  Soon, the nine other companies followed suit.  While he pleaded with the companies to reconsider, only one of the original 10 rehired Fein.  They company was not pleased with ChatGPT’s work, according to the Washington Post.  To ensure a steady income and solid future for himself and family, Fein told the outlet that he plans to enter a trade that AI hasn’t picked up just yet.  “A trade is more future proof,” Fein said.  Fein’s desire to change careers comes as many coders, writers, graphic designers, have begun to fear that AI systems like ChatGPT will soon be able to complete their job, leading to future layoffs.  In May alone, artificial intelligence was the reported reason behind nearly 4,000 job cuts, according to data compiled by Challenger, Gray & Christmas.  The executive outplacement firm found that 3,900 job losses were due to AI, while 19,598 was due to businesses closing.  Two months earlier, Goldman Sachs estimated that AI systems could potentially replace upwards of 300 million full-time jobs.  “If generative AI delivers on its promised capabilities, the labor market could face significant disruption,” the March report reads.  “Using data on occupational tasks in both the US and Europe, we find that roughly two-thirds of current jobs are exposed to some degree of AI automation, and that generative AI could substitute up to one-fourth of current work.” While the fears grow, many skeptics remain. The UN’s Department of Economic and Social Affairs has estimated that suggestions of AI taking the majority of jobs are unrealistic. The department has suggested that much of the technology is not versatile or adaptable to excel in human jobs.  Additionally, as AI advances, technological developments will also create new jobs at the same time. © 2020 THE SUN, US, INC. ALL RIGHTS RESERVED | TERMS OF USE | PRIVACY | YOUR AD CHOICES | SITEMAP"
60,https://www.fox10phoenix.com/news/voice-cloning-ai-scams-rise-arizona-ag-asu-professor-send-warnings,"PHOENIX - The Arizona Attorney General has a warning about artificial intelligence as people are using the technology to clone voices, tricking someone into thinking a loved one is on the phone and needs money. AG Kris Mayes says her office is getting more and more calls about this scam, and she wants to warn the public about it now so fewer people get tricked into sending money. ""I think this technology has evolved more quickly than any technology in human history,"" Mayes remarked. Artificial intelligence is being used to clone voices and scammers are taking advantage by using other people's voices in phone scams. ""If you get a call from someone on the other line that sounds like your mother, you can't assume that's your mother. Unfortunately, it's somewhat awkward to ask your mother some kind of a password, 'Are you really my mom? Can you answer the following question?' but that's where we are right now,"" said Professor Subbarao Kambhampati with ASU's School of Computing and Augmented Intelligence. The FBI said victims include children and non-consenting adults. Mayes says scammers are even taking it to the next level. ""What’s also happening is these scammers are, in some cases, using spoofing equipment that can spoof your phone number, so the combination of them being able to make it look like it’s actually your phone number in tandem with cloning your voice is making these scams and frauds even more dangerous,"" she said. Kambhampati says there needs to be regulations on artificial intelligence for reasons like voice-clone scams. The study found that looking at cancer and neurodegenerative diseases with artificial intelligence could lead to discovering how to alleviate symptoms - or maybe even prevent the diagnosis from happening at all. ""They expect that you use them for good purposes. For example, you might want to say a story to your kid in their grandmother's voice … that's the kind of things they expect you to use it for,"" he said. If you get a call, and you're unsure if it's the person they are claiming to be, Mayes says to call that person's actual phone number to verify and report scams to local law enforcement and the attorney general's office. All the scammer needs is three seconds of someone talking. That's put into a website that then generates a voice. From there, you can type whatever you want it to say. ""The surprising part is it doesn't take that much data, that much voice sample to actually train it to speak like you,"" Kambhampati said. ""The current state of the art is even with a three-second clip of your voice, the system can imitate you … and say any text."" On a voice cloning website, it'll ask for 10 to 20 voice samples using different emotions. ""The better the voice sample, the better compelling imitation is, but you can do a pretty passable imitation with just 3 seconds of the voice,"" Kambhampati said. After those voice samples are submitted, a voice is generated. That voice can say anything a person writes in the text with a click of a button. ""Many people grew up thinking they can trust their eyes, they can trust their ears. That's no longer true,"" Kambhampati said. Breaking news delivered fast By clicking Sign Up, I confirmthat I have read and agreeto the Privacy Policy and Terms of Service. The latest Arizona headlines, national news events of the day + sports, weather, and traffic updates. This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX Television Stations"
61,https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/23/fact-sheet-biden-harris-administration-takes-new-steps-to-advance-responsible-artificial-intelligence-research-development-and-deployment,"The White House 1600 Pennsylvania Ave NW Washington, DC 20500 Today, the Biden-Harris Administration is announcing new efforts that will advance the research, development, and deployment of responsible artificial intelligence (AI) that protects individuals’ rights and safety and delivers results for the American people.AI is one of the most powerful technologies of our time, with broad applications. President Biden has been clear that in order to seize the opportunities AI presents, we must first manage its risks. To that end, the Administration has taken significant action to promote responsible AI innovation that places people, communities, and the public good at the center, and manages risks to individuals and our society, security, and economy. This includes the landmark Blueprint for an AI Bill of Rights and related executive actions, the AI Risk Management Framework, a roadmap for standing up a National AI Research Resource, active work to address the national security concerns raised by AI, as well as investments and actions announced earlier this month. Last week, the Administration also convened representatives from leading AI companies for a briefing from experts across the national security community on cyber threats to AI systems and best practices to secure high-value networks and information.Today’s announcements include: In addition to these new announcements, the White House is hosting a listening session with workers today to hear firsthand experiences with employers’ use of automated technologies for surveillance, monitoring, evaluation, and management. The listening session will include workers representing diverse sectors of the economy, including call centers, trucking, warehousing, health care, and gig work, as well as policy experts, researchers, and policymakers. This listening session follows an RFI released by OSTP earlier this month to advance the Administration’s understanding of the design, deployment, prevalence, and impacts of automated technologies that monitor and track workers. We'll be in touch with the latest information on how President Biden and his administration are working for the American people, as well as ways you can get involved and help our country build back better. Opt in to send and receive text messages from President Biden. The White House 1600 Pennsylvania Ave NW Washington, DC 20500"
62,https://www.cbsnews.com/news/ai-job-losses-artificial-intelligence-challenger-report,"Watch CBS News By Elizabeth Napolitano June 2, 2023 / 5:59 PM / MoneyWatch For those wondering when AI will start replacing human jobs, the answer is it already has. Artificial intelligence contributed to nearly 4,000 job losses last month, according to data from Challenger, Gray & Christmas, as interest in the rapidly evolving technology's ability to perform advanced organizational tasks and lighten workloads has intensified.  The report released Thursday by the outplacement firm shows that layoff announcements from U.S.-based employers reached more than 80,000 in May — a 20% jump from the prior month and nearly four times the level for the same month last year. Of those cuts, AI was responsible for 3,900, or roughly 5% of all jobs lost, making it the seventh-highest contributor to employment losses in May cited by employers. The job cuts come as businesses waste no time adopting advanced AI technology to automate a range of tasks — including creative work, such as writing, as well as administrative and clerical work. The AI industry is expected to grow to more than $1 trillion fueled by major technological advancements that became apparent last fall with the launch of OpenAI's ChatGPT bot, a report by Bloomberg Intelligence analysts shows.  This is the first time AI was included on the Challenger report, but not the first time the rapidly advancing technology has made headlines for replacing humans. The Washington Post reported this week on two copywriters who lost their livelihoods because employers (or clients) decided that ChatGPT could perform the job at a cheaper price. Media companies such as CNET have already laid off reporters while using AI to write articles, which later had to be corrected for plagiarism. Earlier this year, an eating disorder helpline used a chatbot to replace human staff members who had unionized. It recently had to pull the plug on the bot after it gave people problematic dieting advice. In March, investment bank Goldman Sachs predicted in a report that AI could eventually replace 300 million full-time jobs globally and affect nearly one-fifth of employment — with a particular hit to white-collar jobs often considered automation-proof, such as administrative and legal professions. AI is also a concern in the TV and entertainment writers' strike that began in May, with writers demanding better pay and job security in addition to a near-total ban on the use of AI to produce written entertainment content. But analysts note that as with previous technology that has replaced human workers, generative AI is already creating new jobs, and the burgeoning industry is just getting started. ""Generative AI is expected to become a monster employment generator because of estimates of a mushrooming $1.3 trillion AI market that will boost sales and ad spending for the Tech industry,"" Ben Emons, a principal at NewEdge Wealth, said Friday in a note. First published on June 2, 2023 / 5:59 PM © 2023 CBS Interactive Inc. All Rights Reserved. Copyright ©2023 CBS Interactive Inc. All rights reserved. Quotes delayed at least 15 minutes. Market data provided by ICE Data Services. ICE Limitations. Powered and implemented by FactSet. News provided by The Associated Press. Legal Statement."
63,https://www.wired.com/story/security-roundup-ai-scams-voice-cloning/,"To revist this article, visit My Profile, then View saved stories. To revist this article, visit My Profile, then View saved stories. Code hidden inside PC motherboards left millions of machines vulnerable to malicious updates, researchers revealed this week. Staff at security firm Eclypsium found code within hundreds of models of motherboards created by Taiwanese manufacturer Gigabyte that allowed an updater program to download and run another piece of software. While the system was intended to keep the motherboard updated, the researchers found that the mechanism was implemented insecurely, potentially allowing attackers to hijack the backdoor and install malware. Elsewhere, Moscow-based cybersecurity firm Kaspersky revealed that its staff had been targeted by newly discovered zero-click malware impacting iPhones. Victims were sent a malicious message, including an attachment, on Apple’s iMessage. The attack automatically started exploiting multiple vulnerabilities to give the attackers access to devices, before the message deleted itself. Kaspersky says it believes the attack impacted more people than just its own staff. On the same day as Kaspersky revealed the iOS attack, Russia’s Federal Security Service, also known as the FSB, claimed thousands of Russians had been targeted by new iOS malware and accused the US National Security Agency (NSA) of conducting the attack. The Russian intelligence agency also claimed Apple had helped the NSA. The FSB did not publish technical details to support its claims, and Apple said it has never inserted a backdoor into its devices. If that’s not enough encouragement to keep your devices updated, we’ve rounded up all the security patches issued in May. Apple, Google, and Microsoft all released important patches last month—go and make sure you're up to date. And there’s more. Each week we round up the security stories we didn’t cover in depth ourselves. Click on the headlines to read the full stories. And stay safe out there. Lina Khan, the chair of the US Federal Trade Commission, warned this week that the agency is seeing criminals using artificial intelligence tools to “turbocharge” fraud and scams. The comments, which were made in New York and first reported by Bloomberg, cited examples of voice-cloning technology where AI was being used to trick people into thinking they were hearing a family member’s voice. Recent machine-learning advances have made it possible for people’s voices to be imitated with only a few short clips of training data—although experts say AI-generated voice clips can vary widely in quality. In recent months, however, there has been a reported rise in the number of scam attempts apparently involving generated audio clips. Khan said that officials and lawmakers “need to be vigilant early” and that while new laws governing AI are being considered, existing laws still apply to many cases. In a rare admission of failure, North Korean leaders said that the hermit nation’s attempt to put a spy satellite into orbit didn’t go as planned this week. They also said the country would attempt another launch in the future. On May 31, the Chollima-1 rocket, which was carrying the satellite, launched successfully, but its second stage failed to operate, causing the rocket to plunge into the sea. The launch triggered an emergency evacuation alert in South Korea, but this was later retracted by officials. The satellite would have been North Korea’s first official spy satellite, which experts say would give it the ability to monitor the Korean Peninsula. The country has previously launched satellites, but experts believe they have not sent images back to North Korea. The failed launch comes at a time of high tensions on the peninsula, as North Korea continues to try to develop high-tech weapons and rockets. In response to the launch, South Korea announced new sanctions against the Kimsuky hacking group, which is linked to North Korea and is said to have stolen secret information linked to space development. In recent years, Amazon has come under scrutiny for lax controls on people’s data. This week the US Federal Trade Commission, with the support of the Department of Justice, hit the tech giant with two settlements for a litany of failings concerning children’s data and its Ring smart home cameras. In one instance, officials say, a former Ring employee spied on female customers in 2017—Amazon purchased Ring in 2018—viewing videos of them in their bedrooms and bathrooms. The FTC says Ring had given staff “dangerously overbroad access” to videos and had a “lax attitude toward privacy and security.” In a separate statement, the FTC said Amazon kept recordings of kids using its voice assistant Alexa and did not delete data when parents requested it. The FTC ordered Amazon to pay around $30 million in response to the two settlements and introduce some new privacy measures. Perhaps more consequentially, the FTC said that Amazon should delete or destroy Ring recordings from before March 2018 as well as any “models or algorithms” that were developed from the data that was improperly collected. The order has to be approved by a judge before it is implemented. Amazon has said it disagrees with the FTC, and it denies “violating the law,” but it added that the “settlements put these matters behind us.” As companies around the world race to build generative AI systems into their products, the cybersecurity industry is getting in on the action. This week OpenAI, the creator of text- and image-generating systems ChatGPT and Dall-E, opened a new program to work out how AI can best be used by cybersecurity professionals. The project is offering grants to those developing new systems. OpenAI has proposed a number of potential projects, ranging from using machine learning to detect social engineering efforts and producing threat intelligence to inspecting source code for vulnerabilities and developing honeypots to trap hackers. While recent AI developments have been faster than many experts predicted, AI has been used in the cybersecurity industry for several years—although many claims don’t necessarily live up to the hype. The US Air Force is moving quickly on testing artificial intelligence in flying machines—in January, it tested a tactical aircraft being flown by AI. However, this week, a new claim started circulating: that during a simulated test, a drone controlled by AI started to “attack” and “killed” a human operator overseeing it, because they were stopping it from accomplishing its objectives. “The system started realizing that while they did identify the threat, at times the human operator would tell it not to kill that threat, but it got its points by killing that threat,” said Colnel Tucker Hamilton, according to a summary of an event at the Royal Aeronautical Society, in London. Hamilton continued to say that when the system was trained to not kill the operator, it started to target the communications tower the operator was using to communicate with the drone, stopping its messages from being sent. However, the US Air Force says the simulation never took place. Spokesperson Ann Stefanek said the comments were “taken out of context and were meant to be anecdotal.” Hamilton has also clarified that he “misspoke” and he was talking about a “thought experiment.” Despite this, the described scenario highlights the unintended ways that automated systems could bend rules imposed on them to achieve the goals they have been set to achieve. Called specification gaming by researchers, other instances have seen a simulated version of Tetris pause the game to avoid losing, and an AI game character killed itself on level one to avoid dying on the next level. 📧 Get the best stories from WIRED’s iconic archive in your inbox 🎧 Our new podcast wants you to Have a Nice Future The explosive legacy of the pandemic hand sanitizer boom Scientists gave people psychedelics—then erased their memory I asked AI chatbots to help me shop. They all failed The race is on to crack an artist’s “test” signal from aliens The speedrunners trying to break Tears of the Kingdom ⛺ Embrace the new season with the Gear team’s best picks for best tents, umbrellas, and robot vacuums Lily Hay Newman Lily Hay Newman Lily Hay Newman More From WIRED © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
64,https://www.makeuseof.com/most-promising-ai-hardware-technology,"The future is bright for AI. Artificial Intelligence (AI) has made remarkable advancements since the end of 2022. Increasingly sophisticated AI-based software applications are revolutionizing various sectors by providing inventive solutions. From seamless customer service chatbots to stunning visual generators, AI is enhancing our daily experiences. However, behind the scenes, AI hardware is pivotal in fueling these intelligent systems. AI hardware refers to specialized computer hardware designed to perform AI-related tasks efficiently. This includes specific chips and integrated circuits that offer faster processing and energy-saving capabilities. In addition, they provide the necessary infrastructure to execute AI algorithms and models effectively. The role of AI hardware in machine learning is crucial as it aids in the execution of complex programs for deep learning models. Furthermore, compared to conventional computer hardware like central processing units (CPUs), AI hardware can accelerate numerous processes, significantly reducing the time and cost required for algorithm training and execution. Furthermore, with the growing popularity of AI and machine learning models, there has been an increased demand for acceleration solutions. As a result, companies like Nvidia, the world's leading GPU manufacturer, have witnessed substantial growth. In June 2023, The Washington Post reported that Nvidia's market value surpassed $1 trillion, surpassing the worth of Tesla and Meta. Nvidia's success highlights the significance of AI hardware in today's technology landscape. If you're familiar with what edge computing is, you likely have some understanding of edge computing chips. These specialized processors are designed specifically to run AI models at the network's edge. With edge computing chips, users can process data and perform crucial analytical operations directly at the source of the data, eliminating the need for data transmission to centralized systems. The applications for edge computing chips are diverse and extensive. They find utility in self-driving cars, facial recognition systems, smart cameras, drones, portable medical devices, and other real-time decision-making scenarios. The advantages of edge computing chips are significant. Firstly, they greatly reduce latency by processing data near its source, enhancing the overall performance of AI ecosystems. Additionally, edge computing enhances security by minimizing the amount of data that needs to be transmitted to the cloud. Here are some of the leading manufacturers of AI hardware in the field of edge computing chips: Some might wonder, ""What is quantum computing, and is it even real?"" Quantum computing is indeed a real and advanced computing system that operates based on the principles of quantum mechanics. While classical computers use bits, quantum computing utilizes quantum bits (qubits) to perform computations. These qubits enable quantum computing systems to process large datasets more efficiently, making them highly suitable for AI, machine learning, and deep learning models. The applications of quantum hardware have the potential to revolutionize AI algorithms. For example, in drug discovery, quantum hardware can simulate the behavior of molecules, aiding researchers in accurately identifying new drugs. Similarly, in material science, it can contribute to climate change predictions. The financial sector can benefit from quantum hardware by developing price prediction tools. Below are the significant benefits of quantum computing for AI: Application Specific Integrated Circuits (ASICs) are designed for targeted tasks like image processing and speech recognition (though you may have heard about ASICs through cryptocurrency mining). Their purpose is to accelerate AI procedures to meet the specific needs of your business, providing an efficient infrastructure that enhances overall speed within the ecosystem. ASICs are cost-effective compared to traditional central processing units (CPUs) or graphics processing units (GPUs). This is due to their power efficiency and superior task performance, surpassing CPUs and GPUs. As a result, ASICs facilitate AI algorithms across various applications. These integrated circuits can handle substantial volumes of data, making them instrumental in training artificial intelligence models. Their applications extend to diverse fields, including natural language processing of texts and speech data. Furthermore, they simplify the deployment of complex machine-learning mechanisms. Neuromorphic hardware represents a significant advancement in computer hardware technology, aiming to mimic the functioning of the human brain. This innovative hardware emulates the human nervous system and adopts a neural network infrastructure, operating with a bottom-up approach. The network comprises interconnected processors, referred to as neurons. In contrast to traditional computing hardware that processes data sequentially, neuromorphic hardware excels at parallel processing. This parallel processing capability enables the network to simultaneously execute multiple tasks, resulting in improved speed and energy efficiency. Furthermore, neuromorphic hardware offers several other compelling advantages. It can be trained with extensive datasets, making it suitable for a wide range of applications, including image detection, speech recognition, and natural language processing. Additionally, the accuracy of neuromorphic hardware is remarkable, as it rapidly learns from vast amounts of data. Here are some of the most notable neuromorphic computing applications: A Field Programmable Gate Array (FPGA) is an advanced integrated circuit that offers valuable benefits for implementing AI software. These specialized chips can be customized and programmed to meet the specific requirements of the AI ecosystem, earning them the name ""field-programmable."" FPGAs consist of configurable logic blocks (CLBs) that are interconnected and programmable. This inherent flexibility allows for a wide range of applications in the field of AI. In addition, these chips can be programmed to handle operations of varying complexity levels, adapting to the system's specific needs. Operating like a read-only memory chip but with a higher gate capacity, FPGAs offer the advantage of re-programmability. This means they can be programmed multiple times, allowing for adjustments and scalability per the evolving requirements. Furthermore, FPGAs are more efficient than traditional computing hardware, offering a robust and cost-effective architecture for AI applications. In addition to their customization and performance advantages, FPGAs also provide enhanced security measures. Their complete architecture ensures robust protection, making them reliable for secure AI implementations. AI hardware is on the cusp of transformative advancements. Evolving AI applications demand specialized systems to meet computational needs. Innovations in processors, accelerators, and neuromorphic chips prioritize efficiency, speed, energy savings, and parallel computing. Integrating AI hardware into edge and IoT devices enables on-device processing, reduced latency, and enhanced privacy. Convergence with quantum computing and neuromorphic engineering unlocks the potential for exponential power and human-like learning. The future of AI hardware holds the promise of powerful, efficient, and specialized computing systems that will revolutionize industries and reshape our interactions with intelligent technologies. Wasay Ali is a versatile professional writer with global experience and a background in mechanical engineering and social science. He is adept at crafting news and informational content for the crypto space and has experience writing for other niches. He has worked with several digital marketing agencies and clients in the US, UK, Pakistan, and Europe. He is a dedicated volunteer and enjoys reading, writing, poetry, and going to the gym. He is an INFJ-A personality type dedicated to positively impacting the world. Wasay has a passion for writing as it allows him to express his creativity, share his knowledge, and connect with people worldwide. He is known for his ability to create high-quality, engaging, and compelling articles that resonate with readers."
65,https://www.onmanorama.com/news/kerala/2023/06/06/artificial-intelligence-cameras-safe-kerala-project-traffic-violations.html,"Thiruvananthapuram: As many as 28,891 people got caught for violating traffic rules on the first day of the Artificial Intelligence (AI) -enabled cameras that went live across the state on Monday. The most offenders who got trapped in the surveillance cameras from 9 am to 5 pm on Monday were from the Kollam district – 4774 people. It is followed by Thiruvananthapuram (4362), Pathanamthitta (1177), Alappuzha (1288), Kottayam (21940, Idukki (14830, Ernakulam (1889), Thrissur (3995), Palakkad (1007), Malappuram (545), Kozhikode (1550), Wayanad (1146), Kannur (2437) and Kasaragod (1040). The first to get trapped in the surveillance cameras was three persons in Thiruvananthapuram city at 9 am itself. They were caught riding on two motorbikes without helmets and travelling in a car with no seat belts. The number of offences on the first day of levying fine was far less than those on May 3 when 1.93 lakh traffic violations were detected. Transport Minister Antony Raju pointed out that the lesser traffic violations during the eight-hour daytime, including peak traffic hours, are a good sign. Meanwhile, the Motor Vehicle Department experienced technical glitches while sending the fine challans to traffic offenders. Till then, the department had sent only awareness notices to such people. The National Informatics Centre (NIC) officials noticed the issue when the challans were issued for the first time. The same was rectified by 6 pm. However, the SMS alerts to those who got caught for traffic offences will be sent only from Tuesday. This is due to a delay in the process of getting the nod from the Telecom Authority of India (Trai) after explaining to them the method of sending the alerts. Height to be gauged in exempting children below 12 Meanwhile, changes will be incorporated in the camera surveillance system to exempt fines for children below 12 years travelling in two-wheelers. If three persons are travelling on a motorcycle, only the third person who is having the minimum predetermined height will be levied the fine. The control room technician too would inspect the images and the fine will be levied only after this. The system too will have an inbuilt facility to inspect the pictures of others below this height and exempt them. Though the Government decided to exempt those under 12 years, there were practical difficulties that necessitated a temporary change in the camera software. Go live with Manorama Online App, the number one Malayalam News site on our mobiles and tablets. © Copyright 2023 Onmanorama. All Rights Reserved."
66,https://theconversation.com/how-ai-might-soon-rescue-consumers-from-signing-up-to-unfair-terms-and-conditions-205596,"Senior Lecturer in Law, University of Portsmouth PhD Candidate in Computing/Research Assistant, University of Portsmouth PhD Candidate in Law/Research Assistant, University of Portsmouth does not work for, consult, own shares in or receive funding from any company or organisation that would benefit from this article. Enguerrand Boitel and Paris Bradley do not work for, consult, own shares in or receive funding from any company or organisation that would benefit from this article, and have disclosed no relevant affiliations beyond their academic appointment. University of Portsmouth provides funding as a member of The Conversation UK. View all partners Most of us buy goods on the internet without reading the terms and conditions. We take it as a given that the clauses in these standardised agreements are non-negotiable, and hope that they are in our best interests. Too often, however, this doesn’t seem to be the case. From laptop makers to airlines to buy now, pay later companies, there are endless rows about whether terms and conditions are fair. Obtaining hard facts about the size of this problem is difficult. But certainly there are a lot of unhappy shoppers on consumer forums. Many, for example, have experienced difficulties in enforcing their contractual rights or were unaware of what they agreed to when they bought something. In such circumstances it can be difficult to find free or affordable legal advice, so many probably just give up. One argument is that people should just read the contract before clicking “I agree”, but most of us don’t have the time or ability to do so. There will soon probably be a solution to this, however. Instead of having to plough through all the small print ourselves, we might soon be able to do it using artificial intelligence. AI tools to analyse legal documents have existed in a very basic form for a while. They can flag up potential issues such as rights violations that the consumer might want to investigate further. But you have to copy and paste terms sentence by sentence because the AI is so limited in how much text it can handle, and they are designed as a guide to reading the blurb yourself rather than removing the need altogether. There are more sophisticated AI tools that solve the related problem of reading web policy documents. Rather than pasting in text, you upload the relevant URL. The important but narrow focus here is on how web providers use your data. This makes it easier to teach an AI model everything it needs to know – particularly in such a heavily regulated area. With terms and conditions, the challenge is their varied nature. Vendors are much more free to formulate everything in their own words, which makes detecting and understanding these much more difficult for an AI. There are also lots of variations between different jurisdictions, such as “solicitor” in the UK and “attorney” in the US. This means that an AI trained with US data may mislead consumers from the UK. Yet it’s often not clear in the existing tools which jurisdiction they are designed for. You might be wondering if the alternative might be just to copy and paste terms and conditions into one of the latest AI chatbots like ChatGPT, but that’s not a solution either. These general models are not specifically trained on legal texts or legal analysis. This means that any advice they give is just as likely to be accurate, inaccurate or entirely made up. As far as we are aware, no team of developers is trying to create a dedicated terms and conditions AI for consumers using models such as Open AI’s GPT-4, which underpins ChatGPT. Instead, many AI developers seem to be concentrating on the more lucrative area of creating tools that will automate legal work for law firms and other companies. This could even lead to terms less favourable to consumers, since the focus will likely be on cutting costs rather than improving service quality. To change this situation, lead author Jens Krebs and his colleague Ella Haig at the University of Portsmouth have been developing a terms and conditions app for England and Wales. When fully developed, it will enable people to copy and paste an entire document into the prompt. It will then list any terms that might unexpectedly affect the consumer, for example, by failing to meet legislative standards such as the Consumer Rights Act 2015. It will also compare all terms to those used by comparable vendors to ensure that nothing unusual has been slipped in. When it spots something unusual, it will then advise the consumer to read that part before deciding whether to go ahead. The project is currently at the stage of testing the app on different AI models to see which is most effective. So far Google’s Bert is coming out best with 81% accuracy, testing it against data where the researchers know what the perfect result should be. Nothing will be launched until accuracy hits 90% to 95%. The hope is that the app will be made available to consumer groups like Which? in 2024 and then go on general release in 2025. The intention is that it will be free to use. The key obstacle for such a project is the lack of examples of detrimental terms on which to train the AI – exactly the same problem consumers face if they are brave enough to try to judge terms and conditions. The long-term plan for continuing to increase accuracy in the Portsmouth app is to supplement and replace its training data with real data from consumer organisations, the government and consumers. The hope is that the app will be at the forefront of a new generation of AI tools designed to make terms and conditions less opaque. As well as potentially reducing the number of unhappy consumers, these might also help people who are already signed up to unreasonable terms to prepare and present their case – thereby reducing the need for lawyers. If such services take off, the hope would be that they also discourage vendors from pushing the boundaries of what is acceptable. If terms and conditions become a bit more favourable to consumers, that would be a huge win for this emerging technology. Copyright © 2010–2023, The Conversation"
67,https://www.bbc.com/news/technology-65789916,"A US Air Force colonel ""mis-spoke"" when describing an experiment in which an AI-enabled drone opted to attack its operator in order to complete its mission, the service has said. Colonel Tucker Hamilton, chief of AI test and operations in the US Air Force, was speaking at a conference organised by the Royal Aeronautical Society. A report about it went viral. The Air Force says no such experiment took place. In his talk, he had described a simulation in which an AI-enabled drone was repeatedly stopped from completing its task of destroying Surface-to-Air Missile sites by its human operator. He said that in the end, despite having been trained not to kill the operator, the drone destroyed the communication tower so that the operator could no longer communicate with it. ""We've never run that experiment, nor would we need to in order to realise that this is a plausible outcome,"" Col Hamilton later clarified in a statement to the Royal Aeronautical Society. He added that it was a ""thought experiment"" rather than anything which had actually taken place. There have been a number of warnings about the threat to humanity posed by AI issued recently by people working in the sector, although not all experts agree how serious a risk it is. Speaking to the BBC earlier this week, Prof Yoshua Bengio, one of three computer scientists described as the ""godfathers"" of AI after winning a prestigious Turing Award for their work, said he thought the military should not be allowed to have AI powers at all. He described it as ""one of the worst places where we could put a super-intelligent AI"". I spent several hours this morning speaking to experts in both defence and AI, all of whom were very sceptical about Col Hamilton's claims, which were being widely reported before his clarification. One defence expert told me Col Hamilton's original story seemed to be missing ""important context"", if nothing else. There were also suggestions on social media that had such an experiment taken place, it was more likely to have been a planned scenario rather than the AI-enabled drone being powered by machine learning during the task - which basically means it would not have been choosing its own outcomes as it went along, based on what had happened previously. Steve Wright, professor of aerospace engineering at the University of the West of England, and an expert in unmanned aerial vehicles, told me jokingly that he had ""always been a fan of the Terminator films"" when I asked him for his thoughts about the story. ""In aircraft control computers there are two things to worry about: 'do the right thing' and 'don't do the wrong thing', so this is a classic example of the second,"" he said. ""In reality we address this by always including a second computer that has been programmed using old-style techniques, and this can pull the plug as soon as the first one does something strange."" Follow Zoe Kleinman on Twitter War crimes evidence erased by social media firms AI could lead to extinction, experts warn What is artificial intelligence? Evacuations under way as water gushes through damaged Ukraine dam Prince Harry: Tabloids hacked my voicemails when I was at Eton Inside the Taliban's drug war - opium poppy crops slashed Can UK’s Storm Shadow missiles change Ukraine war? Why Putin has put this religious art on display The one thing Mike Pence needs to beat his old boss Who is no longer world's richest? Our quickfire quiz... Haunting images of deadly India train crash in 2002 Jason Derulo makes 'unsexy' investment in car wash Why personalised medicine hasn't arrived How is my country doing tackling climate change? Kashmir battles alarming drug addiction crisis The rise of the 'no-wash' movement Why the city that never sleeps is slowly sinking The generation clocking the most hours © 2023 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking."
68,https://hbr.org/2023/05/ai-can-help-you-ask-better-questions-and-solve-bigger-problems,"Most companies still view AI rather narrowly, as a tool that alleviates the costs and inefficiencies of repetitive human labor and increasing organizations’ capacity to produce, process, and analyze piles and piles of data. But when paired with “soft” inquiry-related skills it can help people ask better questions and be more innovative. There are two distinct, yet related, paths to do this. 1) Use the technology to change the cadence and patterns of their questions: AI increases question velocity, question variety, and question novelty. 2) Use AI to transform the conditions and settings where people work so that questions that spark change — what we call “catalytic” questions — can emerge. This pushes leaders out of their comfort zones and into the position of being intellectually wrong, emotionally uncomfortable, and behaviorally quiet and more reflective, all of which, it turns out, promotes innovative thinking and action. Just a few years ago, businesses wrestled with artificial intelligence mainly in the abstract — a “future of work” problem they’d have to contend with down the line. Now? More than half the companies around the world are actively adopting AI. Although investments are particularly high in industries such as health care, data management and processing, cloud computing, and fintech, all types of organizations and functions have incorporated AI technology into their operations. And generative AI tools such as ChatGPT are forcing leaders to ask where and how AI can help their businesses. Even so, most companies still view AI rather narrowly, as a tool that alleviates the costs and inefficiencies of repetitive human labor by automating mundane physical tasks (like moving merchandise in warehouses) and increasing organizations’ capacity to produce, process, and analyze piles and piles of data. But the technology can do much more than that. Paired with “soft” inquiry-related skills such as critical thinking, innovation, active learning, complex problem solving, creativity, originality, and initiative, this technology can further our understanding of an increasingly complex world, allowing us to engage in more abstract questioning and shifting our focus from identification to ideation. In our research and workshops with executives, we’re finding that companies have much to gain by treating AI as a knowledge-work collaborator in diverse areas such as product design, process efficiency, and prompt engineering. Partnering with the technology in this way can help people ask smarter questions, which in turn makes them better problem solvers and breakthrough innovators. We are also seeing the initial impacts of more context-aware AI systems (like ChatGPT), and as they continue to improve, the skill of asking questions (or creating prompts) will only become more valuable in the discovery process. Although experts have recognized the need for software engineers to ask smart questions upstream, when developing automated tools (to bake in fewer biases and assumptions), little has been said about the flipside of the relationship between AI and inquiry: the technology’s potential to help people become more inquisitive, creative problem-solvers on the job. We aimed to correct this oversight through design-thinking sessions and extensive follow-up conversations with tech-driven business leaders from a diverse array of countries and industries. We also surveyed roughly 200 leaders, from more than 30 countries who participated in our executive education programs at MIT —to learn how artificial intelligence has affected questioning patterns and innovation behaviors and outcomes in their organizations. (For this research, we’ve defined “artificial intelligence” broadly to include machine learning, deep learning, robotics, and the recent explosion of generative AI.) We have found two distinct, yet related, paths that leaders follow to strengthen their (and their teams’) inquiry muscles as they tap the power of AI in their question-asking work. On the first path, they can use the technology to change the cadence and patterns of their questions: AI increases question velocity, question variety, and question novelty. Results from our ongoing research show that AI can significantly increase all three. On the second path, AI can help transform the conditions and settings where people work so that questions that spark change — what we call “catalytic” questions — can emerge. This pushes leaders out of their comfort zones and into the position of being intellectually wrong, emotionally uncomfortable, and behaviorally quiet and more reflective, all of which, it turns out, promotes innovative thinking and action. Let’s look at how each path can lead to breakthrough ideas. Partnering with AI to ramp up the velocity, variety, and novelty of questions requires companies to train algorithms to answer the basic, easy (yes/no) questions independently and to reveal deeply buried patterns in the data. When this foundation is laid, humans can start exploring the power of more context-dependent and nuanced questions that AI technologies are not yet capable of answering alone. Algorithms can provide immediate answers to questions that leaders pose, in turn allowing them to ask more — and more frequent — questions. In our research, we found that 79% of respondents asked more questions, 18% asked the same amount, and 3% asked fewer. At the cybersecurity firm Cybereason, researchers rely on AI and machine learning to immediately answer the basic questions about what happened in an apparent breach so the team can more quickly turn its attention to formulating deeper questions about why it happened. In the past, CEO Lior Div said, findings were more black-and-white: “It’s an attack. It’s not an attack. It’s good or it’s bad.” But the speed with which AI filled in those blanks opened up a whole new line of questions around intent — and what hackers are really after in a given situation. Of course, there are risks to using AI to generate rapid-fire questions. For one, people may keep asking more and more questions without working their way toward an actionable path, making it important to recognize when the process stops being productive. For another, more questions don’t necessarily amount to better questions, which means you’ll still need to exercise human judgment in deciding how to proceed. AI helps uncover patterns and correlations in large volumes of data — connections that humans can easily miss without the technology. Knowing they have this tool at their disposal frees up leaders to ask farther-ranging questions and explore new ideas that they may not have otherwise considered. In our research, we found that engagement with AI led respondents to ask different questions than they otherwise would have 94% of the time. Consider this example: Kli Pappas, the director of predictive analytics at Colgate-Palmolive, told us that his team tapped AI to understand how charcoal became a wildly popular ingredient in consumer products so they could “find the next charcoal.” Their algorithm generated and answered thousands of questions based on their initial search for data, sketching out a decades-long trajectory from charcoal scrubs in South Korea 20 years ago to charcoal appearing in face washes in the U.S. and then in all kinds of products around the world. The AI-generated data led the team to ask hundreds of less-obvious questions to spark creative thinking about future trends that may be lurking in unexpected places. “We look backwards across categories and try to see how do trends move between categories from hair care, to skincare, to oral care,” Pappas said. “Just doing that puts you a decade or more ahead of the curve.” AI also facilitates deeper insights by helping users arrive at novel, “category jumping” questions — the gold standard of innovative inquiry — that apply understanding from one area to a completely different space. Our research shows that AI led respondents to ask unique questions that changed the direction of their team, organization, or industry 75% of the time. When you know a technology can sift through much more data, and connect more dots, than you could ever do alone, it gives you license to ask wilder questions — things you would never ask if you had to answer them on your own, because they are intractable for the human brain or somehow go against entrenched cognitive biases. While category-jumping questions will not arise in every encounter with AI systems, being open to the possibilities and allowing for freedom of inquiry can pave the way for more instances. Here’s how Mir Imran, a medical innovator and founder of InCube Labs, described the upside when we spoke: “AI can take really obscure variables and make novel connections. When these hidden connections come together, it causes you to reframe your question and deliver disruptive innovations.” In other words, AI’s novel connections can spark your novel questions, which in turn can lead you to investigate solutions others haven’t dreamed of yet — like the robotic pills that Imran’s team recently created to replace external injections with internal ones. AI can take leaders out of their usual mode of operation and force them to cede control over where their questions will take them. That’s a good thing. Increased question velocity, variety, and especially novelty give facilitate recognizing where you’re intellectually wrong, and becoming emotionally uncomfortable and behaviorally quiet — the very conditions that, we’ve found, tend to produce game-changing lines of inquiry. Jeff Wilke — former CEO of Amazon Consumer Worldwide, now a cofounder of Re:Build Manufacturing — has embraced these conditions not only in his day-to-day work as a tech executive but also throughout his career, continually revising his mental models while moving from role to role. When we spoke, he had this to say: “If you seek out things that you don’t know, and you have the courage to be wrong, to be ignorant, to have to ask more questions and maybe be embarrassed socially, then I think you build a more complete model, and that model serves you well over the course of your life.” But there’s a hitch to teaming up with AI: Research suggests that it can be challenging for people to do so congenially because AI’s superhuman capabilities and unpredictable moves may prevent them from fully trusting and engaging with the technology. That tracks with what we’ve observed in organizations and learned from our conversations with leaders. Distrust of the technology is hardly conducive to creative inquiry. So, look for ways to offset that, and don’t just leave it to AI to produce the conditions for breakthrough thinking and problem-solving. Consider how else you might create them. Where is there room in your problem-solving processes for synthesizing things that don’t seem related? How might you use those opportunities to throw people off balance so they’ll generate questions that reach beyond what they intellectually know to be right, what makes them emotionally comfortable, and what they are accustomed to saying and doing? At the same time, how can you create psychological safety for people in your organization to ask far-ranging questions and to use AI more effectively to learn from them, ultimately leading to asking better questions? When psychological safety is present, people can say, without repercussion, “I am wrong,” “I am uncomfortable,” and “I am still thinking”? Rather than neatly resolve all those tensions, leaders and teams must learn to sit with the uncertainty that comes from asking questions that take them into new territory. While the process isn’t easy, the results are exciting, which is perhaps the most important benefit of collaborating with an AI system. Excitement provides momentum and motivation to push through a tough process, fueling further creativity. Artificial intelligence may be superhuman in some ways, but it also has considerable weaknesses. For starters, the technology is fundamentally backward-looking, trained on yesterday’s data — and the future might not look anything like the past. What’s more, inaccurate or otherwise flawed training data (for instance, data skewed by inherent biases) produces poor outcomes. Leaders and their teams must manage such limitations if they are going to treat AI as a creative-thinking partner. How? By focusing on areas where the human brain and machines complement one another. Whereas AI increases the volume of data we can process and the degree of complexity we can manage, our brains work in a reductive manner; we generate ideas and then explain them to other people. Whereas machines lack imagination and moral judgment, we can tap those critical skills as AI helps us increase the velocity, variety, and novelty of the questions we’re asking to solve problems in our organizations. Such differences are the stuff of fruitful collaboration — and optimizing them can reduce the threat of AI to human labor. With humans and AI working to their respective strengths, they can transform unknown unknowns into known unknowns, opening the door to breakthrough thinking: logical and conceptual leaps that neither could make without the other. Harnessing this potential will require leaders to look at artificial intelligence in a new light — one that is less about cost savings, efficiency, and automation and more about inspiration, imagination, and innovation. It will also require building a culture that supports, incentivizes, and rewards asking big questions — and not necessarily knowing the answers."
69,https://techcrunch.com/2023/06/02/competition-concerns-in-the-age-of-ai/,"Antitrust is the engine of free enterprise: it shapes countless lines of commerce, from tech to toilets, beer to baseball and healthcare to hardware. Antitrust drives price, quality, variety, innovation and opportunity. Today, artificial intelligence is rapidly changing how businesses sense, reason and adapt in the market. Across every industry, companies are leveraging machine learning to derive valuable insights without extensive employee involvement. But these groundbreaking capabilities are creating an upheaval in how companies engage with competitors and consumers. Experienced competition and consumer protection lawyers can help companies capitalize on the opportunities AI presents while navigating the terra nova of regulatory and litigation risk. Although it is incorrect to approach AI as a black box, the complexity of AI systems can make reasoning opaque. This means linkages between AI outputs and rational business justifications risk being obscured or even lost entirely. Yet regulators are unlikely to excuse consumer and competitive concerns merely because an organization cannot explain why certain actions were taken and others were not. Legal exposure exists under the Sherman Antitrust Act, Federal Trade Commission Act (FTC), Robinson-Patman Act, as well as state antitrust and consumer protection laws. By implementing policies and processes that preserve human control and accountability, organizations can minimize legal exposure and avoid unintended consequences. A proactive and customized approach is critical. AI affects competition and consumers in countless ways, including when used for core business functions. AI helps companies make pricing decisions by responding quickly to instantaneous changes in demand, inventory and input costs. By synthesizing and summarizing vast amounts of complex data, it can be a significant aid in building and adapting pricing policies. But the outcomes that AI-assisted pricing generates can also be seen as facilitating per se unlawful collusion, such as price-fixing or bid-rigging. According to FTC Chair Lina Khan, AI “can facilitate collusive behavior that unfairly inflates prices.” These concerns may arise directly or indirectly from using AI to perform a diverse array of activities such as benchmarking, disaggregating information, signaling, exchanging information or analyzing pricing trends. Pricing algorithms, for example, may raise antitrust issues when competitors use them to enforce an advance agreement, algorithm vendors initiate or organize an agreement, companies apply algorithms to dramatically raise prices or even when competitors independently employ algorithms that subsequently engage in collusive conduct. The U.S. Department of Justice’s Antitrust Division highlights that “the rise of data aggregation, machine learning, and pricing algorithms … can increase the competitive value of historical data” and warrants “revisiting how we think about the exchange of competitively-sensitive information.”"
70,https://www.businessinsider.com/ai-artificial-intelligence-stocks-to-buy-now-investing-strategy-ubs-2023-6,"Equally as stunning as the capabilities of artificial intelligence (AI) tools like ChatGPT are the jaw-dropping moves of stocks tied to the technology so far in 2023. Like many of its Wall Street peers, UBS recently published a report about how investors can profit from the revolutionary potential of AI as it continues to rapidly evolve. ""While not a 'digital god,' Generative AI's capabilities have advanced at a remarkable pace,"" wrote a dozen UBS analysts and strategists led by Michael Briest in a May 26 note. ""The costs of developing and training models today typically run to many millions of dollars, but open-source models are gaining ground and adoption of the technology appears set to be swift."" Ironically, the UBS note was published just a day after one of the firm's own pundits, Art Cashin, compared the AI stock boom to the dot-com bubble. But even if Cashin is right and the AI hype train could eventually crash just as tech stocks did in the early 2000s, it's worth noting that many of those same stocks eventually rose from the ashes and exceeded all expectations. Though AI is a hot trend right now, corporate management teams certainly haven't indicated that it's merely a fad. UBS found that since the start of the year, close to 500 companies across 27 market sectors have combined to mention AI or ChatGPT over 3,500 times on earnings calls. AI is expected to have a monumental impact on the global economy, and businesses in many industries are expected to enjoy higher revenue and cost savings thanks to AI — though UBS noted that competition will get even stiffer as well. Much has been made about the danger that AI poses to jobs. Eventually, about 40% of working hours could be impacted by AI, researchers at Accenture recently estimated, as Briest noted. However, it's worth noting that just over a third of business-related tasks are already done by machines, according to data from the World Economic Forum cited in the UBS report. In the report, Briest and his colleagues put together a list of over 180 firms that will either be significantly helped or hurt by the continued growth of AI. Insider reviewed that list and highlighted the companies that will be positively impacted by AI, and have both a buy rating from UBS and at least 20% upside to the firm's price targets. Below are the 53 buy-rated companies that will be boosted by AI and have 20% upside or more from current levels, according to the price targets set by the UBS analyst that covers them. Along with each is its ticker, industry, upside to price target, and reasoning. Note that the stocks are sorted in alphabetical order by their industry. Industry: Aerospace & Defense Upside to price target: 21% Reasoning: ""A leader in European defense unmanned technologies and strong user of AI technologies within its US electronics business, including computer vision."" Industry: Automobile & Components Upside to price target: 21% Reasoning: ""This Premium/luxury focus opens up monetization opportunity for in-car UX."" Industry: Automobile & Components Upside to price target: 20% Reasoning: ""Tech-savvy customer base & industry-leading AI in-house know-how."" Industry: Capital Goods Upside to price target: 32% Reasoning: ""Higher semico capex to benefit Rotork."" Industry: Capital Goods Upside to price target: 31% Reasoning: ""Using AI to enhance vision & intelligence of self-driving forklifts/helpers."" Industry: Capital Goods Upside to price target: 20% Reasoning: ""Electrical equipment."" Industry: Capital Goods Upside to price target: 24% Reasoning: ""Distributor helps manage customer inventories."" Industry: Capital Goods Upside to price target: 22% Reasoning: ""Connectors, sensors."" Industry: Capital Goods Upside to price target: 34% Reasoning: ""Commercial HVAC."" Industry: Capital Goods Upside to price target: 23% Industry: Capital Goods Upside to price target: 58% Reasoning: ""Customer solutions utilize AI to drive workforce tasks and inventory management."" Upside to price target: 32% Reasoning: ""Benefit from increased need for semiconductors."" Upside to price target: 33% Reasoning: ""Benefit from increased data center demand (semis, interconnects) and using automation to improve its cost base."" Industry: Consumer Staples Upside to price target: 22% Reasoning: ""Coca-Cola's asset light model enables greater investment for top and bottom line growth."" Industry: Consumer Staples Upside to price target: 22% Reasoning: ""L'Oréal is actively investing behind generative AI for the advertising and promotion of its brands, but also to enhance its Beauty Tech science to accelerate innovation."" Upside to price target: 41% Reasoning: ""Adoption of AI could help HKEX improve efficiency and transparency, which makes HK's market more attractive."" Industry: FinTech and Payments Upside to price target: 32% Reasoning: ""Can supplement existing AI technologies that are used to optimize payments execution and reduce fraudulent activity."" Industry: FinTech and Payments Upside to price target: 20% Reasoning: ""Can supplement existing AI technologies that are used to optimize payments execution and reduce fraudulent activity."" Industry: FinTech and Payments Upside to price target: 97% Reasoning: ""Has a large source of merchant and consumer data that can be used to drive engagement."" Industry: FinTech and Payments Upside to price target: 70% Reasoning: ""At the forefront of innovating and adopting new technologies. Can supplement existing AI capabilities to drive engagement."" Industry: Food Retail Upside to price target: 37% Reasoning: ""AI enhanced sales forecasting and labor scheduling help outperform."" Industry: Food Retail Upside to price target: 124% Reasoning: ""Intellectual shelves driving 1.5ppt improvement in labor costs."" Industry: General Retail Upside to price target: 69% Reasoning: ""Fully online multi-brand offer and taking the initiative to incorporate AI for a better proposition."" Industry: General Retail Upside to price target: 43% Reasoning: ""Has industry leading eCommerce capabilities and been investing significantly in digital for many years."" Industry: General Retail Upside to price target: 67% Reasoning: ""The global industry leader in the denim category has a strong omni-channel platform and continues to make investments across its supply chain."" Upside to price target: 60% Reasoning: ""Ping An has developed [an] in-house AI development department and has deployed AI technology on client service, agent management, and back-office."" Industry: IT Services Upside to price target: 31% Reasoning: ""Capgemini has an attractive mix of sales. While software engineering could see productivity gains, customers are likely to re-invest these."" Upside to price target: 25% Reasoning: ""UK market leader and ability to leverage AI for client service."" Industry: Luxury Goods Upside to price target: 20% Reasoning: ""As the largest company in the sector, LVMH can leverage its resources to develop solutions for brands within its group portfolio."" Upside to price target: 43% Reasoning: ""As the world's largest Gaming play, Tencent is well positioned to benefit from long-term efficiency gains in game R&D."" Upside to price target: 35% Reasoning: ""Early adopter of generative AI in in-game user interaction."" Upside to price target: 55% Reasoning: ""In the short-term, AI replicated music could divert streaming royalties, but in the medium term."" [The sentence was left incomplete in the UBS report.] Industry: Medical Devices & Services Upside to price target: 23% Reasoning: ""HCA is actively piloting AI-powered ambient clinical documentation technology in the acute care setting which promises significant efficiency improvements and cost savings if effectively implemented across the organization."" Industry: Medical Devices & Services Upside to price target: 54% Reasoning: ""Leader in liquid biopsy oncology with a suite of test offerings including Guardant Galaxy which utilizes advanced AI backed analytics for enabling biomarker discovery and improving assay performance. Management plans to expand Galaxy into spatial, genomic, and epigenomic biomarker discovery. This pairs with GuardantInform, its real-world evidence & data platform that biopharma partners can utilize that encompasses all their testing data (~175K patients with integrated clinical and molecular information)."" Industry: Medical Devices & Services Upside to price target: 57% Reasoning: ""Cell-free DNA testing company that utilizes AI, a differentiated cfDNA platform and proprietary library preparation, and bioinformatics to develop blood-based assays. Its Signatera test for minimal residual disease monitoring has tumor-normal whole exome sequencing data matched to clinically complete records across 50+ cancer histologies."" Industry: Oil & Gas Upside to price target: 62% Reasoning: ""Early adopter, offering wide range of AI-driven solutions."" Industry: Oil & Gas Upside to price target: 47% Reasoning: ""Early adopter, offering wide range of AI-driven solutions."" Industry: Real Estate Upside to price target: 22% Reasoning: ""May benefit broadly from the competing use of land supply for data centers (as above) while AI itself may lead to optimization in supply chains, and more profitable use of each square foot (leading to further polarization between the 'best and the rest')."" Upside to price target: 21% Reasoning: ""These large QSR companies are using generative AI technology to cut labor costs, improve customer experience, and drive sales, putting them in DP position to create long-term sales growth and improve profitability, solidifying key competitive advantages."" Upside to price target: 24% Reasoning: ""Leading position in GPU for training/AI workloads, incumbency for inference semis as well as DGX Cloud (AI-as-a-service) offering."" Upside to price target: 24% Reasoning: ""Beneficiary of incremental HBM and GDDR demand."" Upside to price target: 26% Reasoning: ""Beneficiary of incremental HBM and GDDR demand."" Upside to price target: 27% Reasoning: ""Key foundry provider for NVIDIA/AMD's data-center grade GPUs."" Industry: Global Steel Upside to price target: 37% Reasoning: ""Highly innovative S. Korean steel producer in a highly competitive region."" Ticker: KRX: 004020 Industry: Global Steel Upside to price target: 34% Reasoning: ""Highly innovative S. Korean steel producer in a highly competitive region."" Industry: Global Steel Upside to price target: 25% Reasoning: ""Significant budget dedicated to digital transformation and AI."" Upside to price target: 33% Reasoning: ""First full next gen network incumbent in Europe, using Analytics/AI to improve customer facing functions and launch Connectivity-as-a-Service."" Upside to price target: 21% Reasoning: ""Various AI initiatives (customer care and on B2C and B2B side that could enhance market positioning and generate cost savings in the medium-term."" Upside to price target: 38% Reasoning: ""Growing fast in industrial digitalization, abundant cash balance to develop AI capabilities, and largest customer dataset for training."" Industry: Transport & Logistics Upside to price target: 24% Reasoning: ""Whilst DSV is the industry leader both in terms of digital transformation and margin performance, we see greater potential for DP-DHL and K+N to pick the low-hanging fruits and catch up in terms of their technological capabilities, and thus greater scope for margin improvements."" Industry: Wealth and Asset Managers Upside to price target: 28% Reasoning: ""Their WM business leverages OpenAI's technology to access, process, and synthesize content to assimilate MS's own expansive range of intellectual capital in the form of insights into companies, sectors, asset classes, capital markets, and regions around the world."" Industry: Wealth and Asset Managers Upside to price target: 57% Reasoning: ""EQT is currently using a proprietary AI-driven platform (Motherbrain) to identify and assess investment opportunities. EQT is applying Generative AI to the platform and expects an improvement in effectiveness."" Industry: Wealth and Asset Managers Upside to price target: 45% Reasoning: ""Discovery is using AI inside Blackrock's Aladdin combined with its Vitality health chassis to provide personalized insights into savings pathways that consider health, longevity, and wealth risks concurrently."""
71,https://www.azfamily.com/2023/06/05/asu-offering-new-chatgpt-course-teach-students-how-use-ai,"TEMPE, AZ (3TV/CBS 5) -- ASU has created a brand new course, that will teach students how to use the AI program, ChatGPT. It comes after industry leaders issued a warning that A.I. technology can pose a risk to human extinction, at the rate it’s advancing. Predicting AI technology will eventually become a required skill set, ASU’s School for the Future of Innovation in Society believed it was a critical time to teach students how to use AI properly. The course is called ‘Basic Prompt Engineering with ChatGPT: An Introduction’. It’s a six-and-a-half week online course that will teach you how to navigate the program, make prompts, and learn how to make quality decisions using the program. Professor and creator of the course, Andrew Maynard, said this is a first of it’s kind class that will help students learn how to use AI technology responsibly. “There are challenges that we don’t know how to grapple with yet, everything from are they going to take our autonomy away from us or whether we need new ways of regulating it,” Maynard said, “all of these technologies are a double-edged sword, ChatGPT can be used for bad as well as good, so they develop a sophisticated understanding of how to use it responsibly.” This course will start June 29th, but it will also be offered in the upcoming fall semester. Maynard said there’s already conversations in place to advance and expand the course on a need-be basis. See a spelling or grammar error in our story? Please click here to report it and include the headline of the story in your email. Do you have a photo or video of a breaking news story? Send it to us here with a brief description Copyright 2023 KTVK/KPHO. All rights reserved."
72,https://www.theverge.com/23746083/google-ai-search-generative-experience-slow,"By Jay Peters, a news editor who writes about technology, video games, and virtual worlds. He’s submitted several accepted emoji proposals to the Unicode Consortium. The worst thing about Google’s new AI-powered search experience is how long you have to wait. Can you think of the last time you waited for a Google Search result? For me, searches are generally instant. You type a thing in the search box, Google almost immediately spits out an answer to that thing, and then you can click some links to learn more about what you searched for or type something else into the box. It’s a virtuous, useful cycle that has turned Google Search into the most visited website in the world. Google’s Search Generative Experience, on the other hand, has loading animations. Let me back up a little. In May, Google introduced an experimental feature called Search Generative Experience (SGE) that uses Google’s AI systems to summarize search results for you. The idea is that you won’t have to click through a list of links or type something else in the search box; instead, Google will just tell you what you’re looking for. In theory, that means your search queries can be more complex and conversational — a pitch we’ve heard before! — but Google will still be able to answer your questions. If you’ve opted in to SGE, which is only available to people who sign up for Google’s waitlist on its Search Labs, AI summaries will appear right under the search box. I’ve been using SGE for a few days, and I’ve found the responses themselves have been generally fine, if cluttered. For example, when I searched “where can I watch Ted Lasso?” the AI-generated response that appeared was a few sentences long and factually accurate. It’s on Apple TV Plus. Apple TV Plus costs $6.99 per month. Great. But the answers are often augmented with a bunch of extra stuff. On desktop, Google displays source information as cards on the right, even though you can’t easily tell which pieces of information come from which sources (another button can help you with that). On mobile (well, only the Google app for now), the cards appear below the summarized text. Below the query response, you can click a series of potential follow-up prompts, and under all of that is a standard Google search result, which can be littered with additional info boxes. That extra stuff in an SGE result isn’t quite as helpful as it should be, either. When it showed off SGE at I/O, Google also showed how the tool could auto-generate a buying guide on the fly, so I thought “where can I buy Tears of the Kingdom?” would be a softball question. But the result was a mess, littered with giant sponsored cards above the result, a confusing list of suggested retail stores that didn’t actually take me to listings for the game, a Google Map pinpointing those retail stores, and off to the right, three link cards where I could find my way to buying the game. A search for a used iPhone 13 Mini in red didn’t go much better. I should have just scrolled down. An increasingly cluttered search screen isn’t exactly new territory for Google. What bothers me most about SGE is that its summaries take a few seconds to show up. As Google is generating an answer to your query, an empty colored box will appear, with loading bars fading in and out. When the search result finally loads, the colored box expands and Google’s summary pops in, pushing the list of links down the page. I really don’t like waiting for this; if I weren’t testing specifically for this article, for many of my searches, I’d be immediately scrolling away from most generative AI responses so I could click on a link.  Confusingly, SGE broke down for me at weird times, even with some of the top-searched terms. The words “YouTube,” “Amazon,” “Wordle,” “Twitter,” and “Roblox,” for example, all returned an error message: “An AI-powered overview is not available for this search.” “Facebook,” “Gmail,” “Apple,” and “Netflix,” on the other hand, all came back with perfectly fine SGE-formatted answers. But for the queries that were valid, the results took what felt like forever to show up.  When I was testing, the Gmail result showed up fastest, in about two seconds. Netflix’s and Facebook’s took about three and a half seconds, while Apple’s took about five seconds. But for these single-word queries that failed, they all took more than five seconds to try and load before showing the error message, which was incredibly frustrating when I could have just scrolled down to click a link. The Tears of the Kingdom and iPhone 13 Mini queries both took more than six seconds to load — an internet eternity! An internet eternity When I have to wait that long when I’m not specifically doing test queries, I just scroll down past the SGE results to get to something to read or click on. And when I have to tap my foot to wait for SGE answers that are often filled with cruft that I don’t want to sift through, it’s all just making the search experience worse for me. Maybe I’m just stuck in my ways. I like to investigate sources for myself, and I’m generally distrustful of the things AI tools say. But as somebody who has wasted eons of his life looking at loading screens in streaming videos and video games, having to do so on Google Search is a deal-breaker for me. And when the results don’t feel noticeably better than what I could get just by looking at what Google offered before, I don’t think SGE is worth waiting for. / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. The Verge is a vox media network © 2023 Vox Media, LLC. All Rights Reserved"
73,https://techcrunch.com/2023/06/02/this-ai-used-gpt-4-to-become-an-expert-minecraft-player/,"AI researchers have built a Minecraft bot that can explore and expand its capabilities in the game’s open world — but unlike other bots, this one basically wrote its own code through trial and error and lots of GPT-4 queries. Called Voyager, this experimental system is an example of an “embodied agent,” an AI that can move and act freely and purposefully in a simulated or real environment. Personal assistant type AIs and chatbots don’t have to actually do stuff, let alone navigate a complex world to get that stuff done. But that’s exactly what a household robot might be expected to do in the future, so there’s lots of research into how they might do that. Minecraft is a good place to test such things because it’s a very (very) approximate representation of the real world, with simple and straightforward rules and physics, but it’s also complex and open enough that there’s lots to accomplish or try. Purpose-built simulators are great, too, but they have their own limitations. MineDojo is a simulation framework built around Minecraft, since you can’t just plonk a random AI in there and expect it to understand what all these blocks and pigs are doing. Its creators (lots of overlap with the Voyager team) put together YouTube videos about the game, transcripts, wiki articles, and a whole lot of Reddit posts from r/minecraft, among other data, so users can create or fine-tune an AI model on them. It also lets those models be evaluated more or less objectively by seeing how well they do things like build a fence around a llama or find and mine a diamond. Voyager excels at these tasks, performing much better than the only other model that comes close, Auto-GPT. But they have a similar approach: using GPT-4 to write their own code as they go. Normally you’d just train a model on all that good Minecraft data and hope it would figure out how to fight skeletons when the sun goes down. Voyager, however, starts out relatively naive, and as it encounters things in the game, it has a little internal conversation with GPT-4 about what it ought to do and how. Directing the next action, and adding skills to the pile. Image Credits: MineDojo For instance, night falls and those skeletons come out. The agent has a general idea of this, but it asks itself, What would a good player of this game do when there are monsters nearby? Well, GPT-4 says, if you want to explore the world safely, you’ll want to make and equip a sword, then whack the skeleton with it while avoiding getting hit. And that general sense of what to do gets translated to concrete goals: collect stone and wood, build a sword at the crafting table, equip it, and fight a skeleton. Once it’s done those things, they’re entered into a general skill library so that later, when the task is “go deep into a cave to find iron ore,” it doesn’t have to learn to fight again from scratch. It does still use GPT, but it uses the cheaper and faster GPT-3.5, which tells it the skills most relevant to a given situation — so it doesn’t try to mine the skeleton and fight the ore. It’s similar to an agent like Auto-GPT that, when faced with an interface it doesn’t know yet, has to teach itself to navigate it in order to accomplish its goal. But Minecraft is a much deeper environment than it is used to solving for, so a specialty agent like Voyager does far better. It finds more stuff, learns more skills, and explores a much greater area than the other bots. Interestingly but perhaps not surprisingly, GPT-4 wipes the floor with GPT-3.5 (i.e., ChatGPT) when it comes to generating useful code. A test replacing the former with the latter had the agent hit a wall early on, perhaps even literally, and fail to improve. It may not be obvious from talking to the two models that one is much smarter, but the truth is you don’t have to be particularly smart to carry on an apparently intelligent conversation (ask me how I know). Coding is much more difficult and GPT-4 was a big update there. The point of this research isn’t to obsolete Minecraft players but to find methods by which relatively simple AI models can improve themselves based on their “experiences,” for lack of a better word. If we’re going to have robots helping us in our homes, hospitals, and offices, they will need to learn and apply those lessons to future actions. You can read more about Voyager right here."
74,https://cointelegraph.com/news/5-free-chatgpt-and-generative-ai-courses,"Level up your skills in language models and automation with free ChatGPT and generative AI courses. In the realm of technology, becoming proficient in using the most recent generation of generative artificial intelligence (AI) tools has quickly become essential. These tools, like ChatGPT or Bard, have proven to be fun and have the ability to really help us in a variety of areas of our lives. To aid you in embracing this new era of generative AI, here is a compilation of top-notch courses that can serve as invaluable resources for honing your skills and staying at the forefront of this transformative technology. One may improve their knowledge and ability to use the power of generative AI to its utmost potential by exploring these instructional materials, ensuring they make the most of these ground-breaking capabilities. One will discover how to use large language models (LLMs) to quickly build effective apps in the course “ChatGPT Prompt Engineering for Developers” taught by Andrew Ng and Isa Fulford. Users can now create capabilities that were either too expensive, too complicated or perhaps impossible by using the OpenAI API. This course includes best practices for prompt engineering, insights into how LLMs work and examples of how to use LLM APIs for various tasks. Summarizing user evaluations, determining sentiments, identifying subjects, translating or fixing grammar in text, and expanding material by automatically producing emails are some of these tasks. The course focuses on two essential principles for writing strong prompts and walks you through methodical prompt engineering. You will also have the chance to create a unique chatbot. You will learn useful skills in timely engineering with the help of various examples and a Jupyter Notebook environment for practical experience. This training, which is being provided in collaboration with OpenAI, intends to give developers the knowledge and abilities they need to use LLMs effectively. This course is appropriate for you regardless of your level of Python proficiency or your interest in exploring cutting-edge prompt engineering and LLM usage. Enrol in the course “LangChain for LLM Application Development” to learn vital abilities for enhancing language models’ functionality in application development utilizing the LangChain framework. In this course, users will learn how to summon LLMs, write prompts, parse responses, use memory for conversations, create operation sequences, implement question-answering over documents, and explore the evolution of LLMs as reasoning agents. Participants will have a model by the end of the course that may be used as a jumping-off point for more research and diffusion model application development. This hour-long workshop, taught by Andrew Ng and LangChain co-founder Harrison Chase, equips participants to build reliable applications quickly. The course is suitable for beginners; however, some familiarity with Python is helpful. Related: 5 free artificial intelligence courses and certifications Participants who want to create diffusion models from scratch should take the “How Diffusion Models Work” course. This intermediate-level course offers a thorough understanding of the models used in the diffusion process. Participants will learn to build their own diffusion model and acquire useful coding skills. 1/Thrilled to announce: 3 new Generative AI courses!* Building Systems with the ChatGPT API, with OpenAI’s @isafulf* LangChain for LLM Application Development, with LangChain’s @hwchase17* How Diffusion Models Work, by @realSharonZhouCheck them out: https://t.co/IN454k1Wz6 pic.twitter.com/85BP6YbmmZ During the course, participants will: The session, led by Sharon Zhou, lasts one hour and focuses on creating, refining and optimizing diffusion models to enhance participants’ generative AI capabilities. Participants may easily comprehend and expand upon the concepts provided thanks to the use of practical examples and built-in Jupyter Notebooks. The “Building Systems with the ChatGPT API” course will teach participants how to automate intricate workflows by making a series of calls to a powerful language model. This succinct course improves development skills and increases productivity. Individuals will: This one-hour session, taught by Ng of DeepLearning.AI and Fulford of OpenAI, expands on “ChatGPT Prompt Engineering for Developers” (not a prerequisite). Jupyter Notebooks and practical examples make it easier to understand and explore the course material. Collaboration within the OpenAI community guarantees current best practices for optimum performance and responsible usage. The course is appropriate for those with a basic familiarity with Python as well as for intermediate and advanced ML engineers looking for cutting-edge, quick engineering skills for language models. Related: 5 real-world applications of natural language processing (NLP) Join the ""Introduction to ChatGPT"" course by DataCamp to gain the knowledge needed for effective and responsible use of ChatGPT. This course covers ChatGPT's capabilities and restrictions and is appropriate for users of all skill levels. One can discover new integration opportunities, business use cases, and ChatGPT suggestions for best practices. The course is divided into two modules: “Interacting with ChatGPT,” which is available for free, and “Adopting ChatGPT,” which is available for purchase or through a DataCamp subscription. By the end of the course, participants may feel confident applying ChatGPT in various situations, enhancing their speed and efficiency across a wide range of tasks."
75,https://www.forbes.com/sites/jeffkauflin/2023/06/06/inside-the-rise-of-a-fintech-startup-using-ai-and-human-insight-to-fight-fraud/?ss=fintech,"During the summer and fall of 2018, Hasan Hakim Brown, a Floridian in his early 40s, was applying online for loans—for the fake companies and bogus identities he’d set up. He had mixed success. He swindled more than $1 million from a Texas bank. But a few of his other targets, using software from San Francisco–based startup SentiLink, flagged his applications as suspicious because too many Social Security numbers were associated with the same address. Brown, it turned out, had started manufacturing “synthetic identities”—stolen (but real) Social Security numbers merged with made-up names. He later refined his technique, buying a rig from an Atlanta computer consultant that let him simultaneously manage multiple virtual desktops from different IP addresses, thereby evading certain fraud-detection screens. When Covid-19 hit in early 2020 and Congress appropriated hundreds of billions in for­givable Payroll Protection Program loans for hurting businesses, Brown was ready. Ultimately, according to federal court records, including guilty pleas, Brown and his half-dozen criminal associates controlled 700 synthetic identities and dozens of shell businesses and related bank accounts. Overall, the gang defrauded the Small Business Administration and various banks out of more than $20 million. Brown was sentenced to 60 months. While Brown was busy thieving, SentiLink cofounders Naftali Harris and Maxwell Blumenfeld, both now 31, were also thinking about synthetic ID fraud, turning their early insights into a nicely growing niche business. “Everyone initially told us that this type of fraud was impossible, and that we must have misunderstood something,” says CEO Harris. Last year, Harris and Blumenfeld’s six-year-old startup had about $25 million in revenue, more than double that of the year before, Forbes estimates. It counts seven of the U.S.’ 15 biggest banks and six of its 10 largest credit unions, as well as major fintechs like Ramp and Plaid, among its 300-plus customers. SentiLink raised $70 million in July 2021 at a $430 million valu­ation, according to PitchBook. Harris says it’s burning only $1 million a month and has enough cash to keep operating without additional funding for more than five years. He and Blumenfeld are veterans of the 2020 Forbes 30 Under 30 list, and this year SentiLink makes its debut on the Fintech 50, our annual list of the most innovative private fintech startups. Artificial intelligence is, of course, a key part of SentiLink’s business. But Harris and Blumenfeld took a crucial lesson from how they first recognized synthetic fraud: A human, not a computer, made the key connection. In August 2016, the college buddies were both data scientists at buy-now, pay-later startup Affirm. Harris’ team was building the models for approving or declining borrowers. Blumenfeld’s job was to look for fraud. One day Blumenfeld noticed two applicants had the same name and date of birth but different Social Security numbers. He ran that name through the computer and found 12 people had applied for loans with the same name and birthdate but different Social Security numbers. More shocking, all 12 had credit bureau histories and good FICO credit scores above 700. One had a credit card with a $20,000 limit. Another got a $35,000 personal loan. A third had secured a loan for an $80,000 BMW. “This is crazy,” Harris recalls thinking. “These people don’t exist, but they tricked the bureaus into getting a credit report.” Using the same name and birthdate was stupid. But the underlying strategy was clever and patient: Scammers were stealing Social Security numbers from folks who weren’t likely to be actively shopping for credit, such as kids, prisoners and nursing home residents. They paired those numbers with fictitious names and real addresses. Then they built up credit records for their creations by opening checking accounts and making timely payments on loans and credit cards. Eventually, they could use those credit histories to qualify for big loans they wouldn’t repay—an event that’s now known as a “bust-out.” But this type of fraud was little known, even to experts, when Harris and Blumenfeld first came upon it. “They told us that the bureaus had accurate records of all credit-active Americans and that as long as you checked that the identity had a bureau record, this wouldn’t be possible,’’ Harris recalls. But it was. And it still is, despite the launch last year of a somewhat clunky federal database (which SentiLink uses) that enables authorized users to cross-check Social Security numbers and names. Today, 78-person SentiLink has eight full-time employees dedicated to manually reviewing potential fraud attempts and requires others to spend at least an hour a week eyeballing cases to spot emergent patterns—new fraud angles or even legitimate applicants who may be unfairly rejected by the algorithm. “There’s a big misconception with AI that it discovers these things in and of itself,” says Blumenfeld, who is chief operating officer and leads research and development at SentiLink. “The [AI] model in our case is literally trying to mimic what the human would do. It can just scale really quickly.” Ingenuity and speed have been key to the pair’s success so far. Harris grew up in Los Angeles (his father is a finance professor at the University of Southern California) and sped through four years of high school math, English and Spanish in three years at the Milken Commu­nity School, a Jewish day school named after billionaire donor Michael Milken. Without finishing high school, he applied to a dozen top colleges. The Univer­sity of Chicago was one of five that accepted him. During his first few days there, he met Blumenfeld, the son of an art teacher and a tax lawyer. Harris graduated in three years from Chicago with a degree in statistics and started a Ph.D. at Stanford—but found it too theoretical and earned a master’s in statistics instead. In June 2014, Affirm cofounder Max Levchin persua­ded him to become the company’s first data scientist. Harris, Levchin says, is a “very first-principles thinker. He didn’t take anything for granted, evaluated things from scratch and was very, very mathematically competent.” Blumenfeld joined Affirm six months later. Little Big Picture In March 2017, just seven months after their first encounter with synthetic ID fraud, Harris and Blumenfeld decided to build their own company around it. They got $575,000 in seed funding—$300,000 from Dallas venture capital firm Goldcrest and most of the rest from Levchin. They started working out of a windowless basement office in a seedy part of San Francisco. “It was the cheapest office we could find at the time. And it felt like a company that was fighting fraud should be in a basement office,” Blumenfeld muses. To build a useful fraud-scoring model, you need customer data, and lots of it. They used a clever shortcut to get started, buying millions of dollars’ worth of written-off bad debt from len­ders for about $10,000. That entitled them to pull credit reports on the defaulted borrowers and look for telltale patterns. They also started coding specific behaviors into their algorithm. If someone was applying with an email address created just a month before or using an IP address from a different location than their physical address, those counted as red flags. As they scrambled to build their model, the need for it was growing. Research firm Aite-Novarica estimates that U.S. financial institutions’ losses from synthetic ID fraud tripled from $800 million in 2017 to at least $2.4 billion last year. But the firm notes that losses could be more than twice its estimate because some lenders still write off bad debt without knowing whether the deadbeat is synthetic or real. (Aite-Novarica doesn’t estimate losses sustained by the government, telecoms or online gambling sites, which are also big victims.) In 2019, Harris and Blumenfeld finally got their first big bank customer: Synchrony, the consumer lending specialist behind the retail credit cards offered by Amazon and JCPenney. That year SentiLink also raised its first substantial funding—$14 million from Andreessen Horowitz and Felicis Ventures, among others. Hans Morris, managing partner of venture capital firm NYCA, one of the investors, says the boyish, geeky-looking pair have a way with financial services execs. “They’re so nerdy that they’re trusted and charming.” Charming or not, it helps that the duo had targeted the right sort of wonky fraud problem—growing as a threat but not yet so significant that there were already lots of big competitors in the space fielding good models. Then came the pandemic. The surge in e-commerce and the flood of federal money were a boon to both fraudsters and SentiLink, which grew from 12 customers in December 2019 to 45 by the end of 2020. Last year, it processed 323 million identity checks for customers, up from 148 million in 2021. The more data it processes, the better its models are trained—and the more revenue it makes, since many clients pay both a fixed licensing fee and a usage fee for each identity check. SentiLink has expanded beyond synthetic fraud to old-fashioned identity theft and first-party fraud, in which people use their real identities to steal money or goods, often by disputing legitimate charges. Harris says they’ve been able to cross-sell more than half of their synthetic-fraud customers on a second or third product, and about 60% of revenue now comes from new areas. Yet despite such traction, SentiLink has only a tiny sliver of the fraud-prevention market, which totals $15 billion a year, estimates credit bureau (and competitor) Experian. In fact, some banks work with as many as 10 fraud-prevention firms at a time. Companies like Experian, Lexis-Nexis and fintech unicorn Socure all offer a broader set of services than SentiLink. Another challenge: The business moves fast. Crooks continuously come up with new schemes and variations as fraud-prevention models proliferate. “I have 1,000 companies a year saying, ‘I’m better at this than everybody else,’ ” says Max Axler, chief credit officer at Synchrony. Harris and Blumenfeld will need to keep grinding if they want to keep up—with competitors and criminals alike."
76,https://www.cnn.com/2023/05/28/business/microsoft-ai-brad-smith/index.html,"Fear & Greed Index Latest Market News The government needs to work faster to regulate AI, which has more potential for the good of humanity than any other invention preceding it, Brad Smith, Microsoft (MSFT) president and vice chair, said on CBS’ “Face the Nation” Sunday. Its uses are almost “ubiquitous” Smith said, “in medicine and drug discovery and diagnosing diseases, in scrambling the resources of, say, the Red Cross or others in a disaster to find those who are most vulnerable where buildings have collapsed,” the executive added. Smith also said AI isn’t as “mysterious” as many think, adding it is getting more powerful. Lawyer apologizes for fake court citations from ChatGPT “If you have a Roomba at home, it finds its way around your kitchen using artificial intelligence to learn what to bump into and how to get around it,” Smith said. Regarding concerns about AI’s power, Smith said any technology that exists today looked dangerous to people who lived before it. Smith said that there should be a safety break in place. Job disruptions due to AI will unfold over years, not months, Smith said. “For most of us, the way we work will change,” Smith said. “This will be a new skill set we’ll need to, frankly, develop and acquire.” To prevent instances like the fake photo of the explosion near the Pentagon, Smith said there needs to be a watermark system, or “use the power of AI to detect when that happens.” “You embed what we call metadata, it’s part of the file, if it’s removed, we’re able to detect it. If there’s an altered version, we in effect, create a hash. Think of it like the fingerprint of something, and then we can look for that fingerprint across the internet,” Smith said, adding a new path should be found to find a balance between regulating deepfakes and misleading ads and free expression. With a US presidential election year approaching and the ongoing threat of foreign cyber influence operations, Smith said the tech sector needs to come together with governments in an international initiative. Smith supports a new government agency to regulate AI systems. Welcome to the ""generative AI"" era. Resistance is futile “Something that would ensure not only that these models are developed safely, but they’re deployed in say, large data centers, where they can be protected from cybersecurity, physical security and national security threats,” Smith said. Smith did not believe a six-month pause on AI systems that are more powerful than GPT4 is “the answer,” as Elon Musk and Apple co-founder Steve Wozniak have said. “Rather than slow down the pace of technology, which I think is extraordinarily difficult, I don’t think China’s going to jump on that bandwagon,” Smith said. “Let’s use six months to go faster.” Smith suggested an executive order where the government itself says it will only buy AI services from companies that are implementing AI safety protocols. “The world is moving forward,” Smith said. “Let’s make sure that the United States at least keeps pace with the rest of the world.” Most stock quote data provided by BATS. US market indices are shown in real time, except for the S&P 500 which is refreshed every two minutes. All times are ET. Factset: FactSet Research Systems Inc. All rights reserved. Chicago Mercantile: Certain market data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Dow Jones: The Dow Jones branded indices are proprietary to and are calculated, distributed and marketed by DJI Opco, a subsidiary of S&P Dow Jones Indices LLC and have been licensed for use to S&P Opco, LLC and CNN. Standard & Poor’s and S&P are registered trademarks of Standard & Poor’s Financial Services LLC and Dow Jones is a registered trademark of Dow Jones Trademark Holdings LLC. All content of the Dow Jones branded indices Copyright S&P Dow Jones Indices LLC and/or its affiliates. Fair value provided by IndexArb.com. Market holidays and trading hours provided by Copp Clark Limited. © 2023 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved. CNN Sans ™ & © 2016 Cable News Network."
77,https://www.forbes.com/sites/patrickmoorhead/2023/06/05/google-emphasizes-android-ai-large-screens-pixel-and-its-new-pixel-fold/,"Google recently announced new features in its Android operating system and new Pixel devices, including the Pixel Tablet, Pixel 7a and Pixel Fold. If you watched the keynote for Google I/O 2023, you know that Google announced many new artificial intelligence technologies as well. You can also watch my coverage of Google I/O in Episode 168 of the Six Five podcast below. In this article, I want to talk about Google's new AI features for Android and the three new Pixel products. Let's dive in. The excitement around AI today is largely focused on generative AI (GAI), but AI technologies span far more than just the generation of images and texts. Before the recent surge in public interest in GAI, Google had already implemented AI features such as the ability for Google Assistant to screen unwanted or unknown calls in Pixel phones, or the Photo Unblur and Magic Eraser functions in the phones' cameras. We should continue to see Google implement more AI functionality in Google Assistant and in many aspects of Android like the ones discussed below. 50+ redesigned Google apps. At Google I/O 2023, Google announced a new Generative AI Wallpaper feature that allows the user to create a wallpaper from a simple prompt. It also announced Cinematic Wallpaper, which turns photos into 3D wallpapers using AI. I believe these are unique uses of AI for Android and strengthen Android's customizability, a key strength that differentiates Android from iOS. Google also introduced Magic Compose to Google Messages, enabling users to communicate the meaning of a text in different contexts. For example, if I wanted to sound more professional, it can rewrite the text to sound more professional. While Apple is still trying to figure out how to allow users to change the color of their texts, Google is helping its users compose the perfect message. Google mentioned on-device AI earlier in the keynote with the announcement of PaLM 2, specifically the smallest model named Gecko which can be run on mobile devices. On-device AI and hybrid AI are going to enable even more applications in the Android ecosystem to integrate AI. Qualcomm has already recognized the need for AI processing to be distributed between the cloud and devices. Qualcomm has written a great whitepaper here that dives into why the future of AI is hybrid. AI requires a significant amount of processing power and hybrid AI should enable AI applications to save resources by using on-device processing while leaving the door open for more demanding AI applications in the cloud. An application could have smaller customized AI models specifically for the application's use-case with a more general AI, also known as artificial general intelligence (AGI), model picking up the more challenging workloads in the cloud. Google also seemed pretty happy to announce that its Wear OS user base has grown by more than five times since the launch of Wear OS 3 back in 2021—and that it is now the fastest-growing smartwatch platform. While this is something to be happy about, I was slightly disappointed that Google did not announce anything new for Wear OS, instead just offering assurance that developers are investing in the platform and that it will announce more later this year. I would have liked to see more Wear OS announcements and hear Google talk about how it is enabling developers to build apps and connect with the many Wear OS app managers such as Galaxy Wearable and the Pixel Watch app. Similarly, I did not hear any updates or announcements about Fitbit and Google Fit. The Pixel Watch, Google's first iteration of a smartwatch with Fitbit features, carries significant momentum, but one of the biggest criticisms it has faced is that Fitbit, a Google company, is ""built on,"" not built-in. It has been a couple of years since Google acquired Fitbit, and we have not seen much progress in merging the Google Fit and Fitbit platforms. Health and activity applications are a significant area where I could see AI making a tremendous impact, and I believe Google can lean into AI much harder with Fitbit and Google Fit. Over the past couple of Android versions, since Android 12L, Google has emphasized the implementation of Android for larger displays such as tablets and foldables. At its recent event, Google announced more than 50 Google apps that have been optimized for larger screens with smoother transitions across screens, improved multi-column layouts and a dedicated tablet mode. I appreciate these updates—it's high time for them. These Android app improvements and multitasking enhancements should enable more devices to adopt a foldable design without prolonging the feeling that foldables are still at the bleeding edge of the smartphone market. For too long, Samsung has been the only dominant player for foldable devices, at least in North America. Samsung has made it very clear since it debuted the Galaxy Z Fold 3 and Z Flip 3 that foldable smartphones are much more mature than they were five years ago. That makes it the perfect time for Google to launch its own foldable smartphone to take advantage of momentum in this market niche that Samsung has been upholding for too long. Bring on the competitive foldables! The Pixel Fold Enter the Pixel Fold, which brings exciting competition and hope to this space while also keeping expectations low. Part of why I am excited to see more competition in the foldables space is that Google now has skin in the game, giving it more of a reason to lean into foldable technology outside of its partnership with Samsung. I hope that as Google implements more large-screen capabilities, developers and OEMs will in turn plan for these features. That kind of virtuous cycle could mean a lot more innovation in the foldable market over the next few years. At the same time, the Pixel Fold itself will satisfy you only if you keep your expectations low. I say that because the only competition it has right now on the market—thus the only product to compare it to—is the Galaxy Z Fold 4. Compared to that flagship device, the Pixel Fold has only two things going for it that prevent it from being a total flop. First, the Pixel Fold is a Pixel, which means it will get special treatment from Google; second, it has a considerably different aspect ratio from other foldable smartphones. The Pixel Fold has all the great features and capabilities that Google has included across this product line like the AI-enhanced Pixel camera, Pixel Speech and Pixel Call Assist. Google also introduced some unique features to the Pixel Fold that leverage the dual displays; these include Dual Screen Interpreter, a new dynamic taskbar, a split-screen view and rear-camera selfies. The Pixel Fold fits the bill for those who have wanted a foldable smartphone but have not wanted to leave the Pixel ecosystem. The Pixel Fold also has a unique compact design compared to the Galaxy Z Fold series. The Galaxy Z Fold opens up to a 21.6:18 aspect ratio, making it quite a tall device. The Pixel Fold, by contrast, has a 17.4:9 aspect ratio and a more compact folded design—which Google claims makes it the thinnest foldable on the market. The 17.4:9 aspect ratio is much closer to the cinematic 16:9 widescreen ratio used for many TVs, monitors, laptops and online videos. The shorter design also makes it more pocketable than the Galaxy Z Fold 4, which I appreciate. Google Pixel Fold. However, unless you strongly prefer the Pixel ecosystem or prefer the smaller design, the Pixel Fold is not the best value. If we compare it to other foldable smartphones at the premium $1,799 price point, it does not have stylus support like the S Pen on the Galaxy Z Fold 4 or the Surface Pen on the Microsoft Surface Duo 2. It also does not have reverse wireless charging like the Galaxy Z Fold 4 nor a processor as powerful as the powerful Qualcomm Snapdragon Gen 1 in the Surface Duo 2 and Galaxy Z Fold 4. I will have to get my hands on the Pixel Fold to get a better idea of its value and give a better comparison to the Galaxy Z Fold 4. Google says it has the best camera on any foldable, and I am interested in testing that claim. The expectation for the Pixel Fold should not be that it's the best foldable smartphone on the market, but rather an exciting new foldable smartphone for Pixel fans. Like all Google Pixel phones, the Pixel Fold's success will rely largely on how well Google can enhance the foldable experience with AI-driven features and Google services. The Google Pixel Tablet At the same time, Google also launched the Pixel Tablet, an exciting entry into the tablet market that has unique capabilities and great value. Google has recognized that most people use tablets at home as a multi-user device. With this revelation, Google created a unique speaker dock to complement the Pixel Tablet and give it a multi-model purpose. Google Pixel Tablet. The Pixel Tablet is an 11-inch device that runs on Google's Tensor G2 SoC, the same SoC in the Pixel 7 series. It has a similar design to older Pixel devices, with a rounded ceramic back coating and a single camera on the back with pogo pins for the speaker dock. When the Pixel Tablet is attached to the speaker dock, it turns into a Home Hub. The speaker dock is included with the tablet for $499, and additional speaker docks can be purchased separately for $129 each if users want to put them in different rooms of the house. For $499, I am confident in the Tensor G2 and the Pixel Tablet's capabilities, especially with Google's implementation of AI in the camera and Pixel experience. I will be interested to see Google expand its tablet lineup as the Tensor SoC becomes more powerful. The Pixel 7a Google also announced the Pixel 7a, its newest Pixel smartphone, at a more affordable price point. The Pixel 7a starts at $499, which is $100 less than the Pixel 7 that uses the same Tensor G2 SoC. The Pixel 7a has all of the great AI enhancements of the Pixel ecosystem, like Pixel Call Assist and Live Translate. Google Pixel 7a in Charcoal, Snow, Sea and Coral. The biggest difference between the Pixel 7a and the Pixel 7 is that the 7a is slightly smaller, with a 6.1-inch display compared to the Pixel 7's 6.3-inch display. Compared to the Pixel 6a, the 7a has a better camera with a 72% larger sensor and a new 13-megapixel ultrawide lens. The 7a's $499 price compared to the 6a's $449 at launch brings it slightly closer to the Pixel 7. I think many will go back and forth on whether to choose the Pixel 7 with its larger display and better build quality or the more affordable Pixel 7a. While Microsoft and Google duke it out over generative AI for search engines and business suites, I believe we should see Google implement exciting AI technology in Android while Microsoft implements exciting AI technology in Windows. Google is leaning into its strength in customization for Android with features like the new GAI wallpaper and composable messages. I am excited to see more large-screen implementations for Android, and I am excited to get my hands on the Pixel Fold and Pixel Tablet. I am also interested to see the difference for myself between the compact design of the Pixel Fold in comparison to the Galaxy Z Fold 4. Likewise, the Pixel Tablet is uniquely positioned for those invested in the Google Home experience. It will be exciting to see how Google continues investing in large-screen Android devices in the coming years. Note: Moor Insights & Strategy co-op Jacob Freyman contributed to this article. Moor Insights & Strategy provides or has provided paid services to technology companies like all research and tech industry analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking, and video and speaking sponsorships. The company has had or currently has paid business relationships with 8×8, Accenture, A10 Networks, Advanced Micro Devices, Amazon, Amazon Web Services, Ambient Scientific, Ampere Computing, Anuta Networks, Applied Brain Research, Applied Micro, Apstra, Arm, Aruba Networks (now HPE), Atom Computing, AT&T, Aura, Automation Anywhere, AWS, A-10 Strategies, Bitfusion, Blaize, Box, Broadcom, C3.AI, Calix, Cadence Systems, Campfire, Cisco Systems, Clear Software, Cloudera, Clumio, Cohesity, Cognitive Systems, CompuCom, Cradlepoint, CyberArk, Dell, Dell EMC, Dell Technologies, Diablo Technologies, Dialogue Group, Digital Optics, Dreamium Labs, D-Wave, Echelon, Ericsson, Extreme Networks, Five9, Flex, Foundries.io, Foxconn, Frame (now VMware), Fujitsu, Gen Z Consortium, Glue Networks, GlobalFoundries, Revolve (now Google), Google Cloud, Graphcore, Groq, Hiregenics, Hotwire Global, HP Inc., Hewlett Packard Enterprise, Honeywell, Huawei Technologies, HYCU, IBM, Infinidat, Infoblox, Infosys, Inseego, IonQ, IonVR, Inseego, Infosys, Infiot, Intel, Interdigital, Jabil Circuit, Juniper Networks, Keysight, Konica Minolta, Lattice Semiconductor, Lenovo, Linux Foundation, Lightbits Labs, LogicMonitor, LoRa Alliance, Luminar, MapBox, Marvell Technology, Mavenir, Marseille Inc, Mayfair Equity, Meraki (Cisco), Merck KGaA, Mesophere, Micron Technology, Microsoft, MiTEL, Mojo Networks, MongoDB, Multefire Alliance, National Instruments, Neat, NetApp, Nightwatch, NOKIA, Nortek, Novumind, NVIDIA, Nutanix, Nuvia (now Qualcomm), NXP, onsemi, ONUG, OpenStack Foundation, Oracle, Palo Alto Networks, Panasas, Peraso, Pexip, Pixelworks, Plume Design, PlusAI, Poly (formerly Plantronics), Portworx, Pure Storage, Qualcomm, Quantinuum, Rackspace, Rambus, Rayvolt E-Bikes, Red Hat, Renesas, Residio, Samsung Electronics, Samsung Semi, SAP, SAS, Scale Computing, Schneider Electric, SiFive, Silver Peak (now Aruba-HPE), SkyWorks, SONY Optical Storage, Splunk, Springpath (now Cisco), Spirent, Splunk, Sprint (now T-Mobile), Stratus Technologies, Symantec, Synaptics, Syniverse, Synopsys, Tanium, Telesign,TE Connectivity, TensTorrent, Tobii Technology, Teradata,T-Mobile, Treasure Data, Twitter, Unity Technologies, UiPath, Verizon Communications, VAST Data, Ventana Micro Systems, Vidyo, VMware, Wave Computing, Wellsmith, Xilinx, Zayo, Zebra, Zededa, Zendesk, Zoho, Zoom, and Zscaler. Moor Insights & Strategy founder, CEO, and Chief Analyst Patrick Moorhead is an investor in dMY Technology Group Inc. VI, Fivestone Partners, Frore Systems, Groq, MemryX, Movandi, and Ventana Micro., MemryX, Movandi, and Ventana Micro."
78,https://www.fda.gov/science-research/science-and-research-special-topics/artificial-intelligence-and-machine-learning-aiml-drug-development,"The .gov means it’s official.Federal government websites often end in .gov or .mil. Before sharing sensitive information, make sure you're on a federal government site. The site is secure. The https:// ensures that you are connecting to the official website and that any information you provide is encrypted and transmitted securely. Artificial Intelligence (AI) and Machine Learning (ML) can be described as a branch of computer science, statistics, and engineering that uses algorithms or models to perform tasks and exhibit behaviors such as learning, making decisions, and making predictions. ML is considered a subset of AI that allows models to be developed by training algorithms through analysis of data, without models being explicitly programmed. FDA recognizes the increased use of AI/ML throughout the drug development life cycle and across a range of therapeutic areas. In fact, FDA has seen a significant increase in the number of drug and biologic application submissions using AI/ML components over the past few years, with more than 100 submissions reported in 2021. These submissions traverse the landscape of drug development — from drug discovery and clinical research to postmarket safety surveillance and advanced pharmaceutical manufacturing.   Additionally, AI/ML is increasingly integrated in areas where FDA is actively engaged, including Digital Health Technologies (DHTs), and Real-World Data (RWD) analytics.  FDA is committed to ensuring that drugs are safe and effective while facilitating innovations in their development. As with any innovation, AI/ML creates opportunities and new and unique challenges. To meet these challenges, FDA has accelerated its efforts to create an agile regulatory ecosystem that can facilitate innovation while safeguarding public health. As part of this effort, FDA’s Center for Drug Evaluation and Research (CDER), in collaboration with the Center for Biologics Evaluation and Research (CBER) and the Center for Devices and Radiological Health (CDRH), issued an initial discussion paper to communicate with a range of stakeholders and to explore relevant considerations for the use of AI/ML in the development of drugs and biological products. The agency will continue to solicit feedback as it advances regulatory science in this area. AI/ML will undoubtedly play a critical role in drug development, and FDA plans to develop and adopt a flexible risk-based regulatory framework that promotes innovation and protects patient safety."
79,https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html,"The Shoggoth, a character from a science fiction story, captures the essential weirdness of the A.I. moment. Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Kevin Roose A few months ago, while meeting with an A.I. executive in San Francisco, I spotted a strange sticker on his laptop. The sticker depicted a cartoon of a menacing, octopus-like creature with many eyes and a yellow smiley-face attached to one of its tentacles. I asked what it was. “Oh, that’s the Shoggoth,” he explained. “It’s the most important meme in A.I.” And with that, our agenda was officially derailed. Forget about chatbots and compute clusters — I needed to know everything about the Shoggoth, what it meant and why people in the A.I. world were talking about it. The executive explained that the Shoggoth had become a popular reference among workers in artificial intelligence, as a vivid visual metaphor for how a large language model (the type of A.I. system that powers ChatGPT and other chatbots) actually works. But it was only partly a joke, he said, because it also hinted at the anxieties many researchers and engineers have about the tools they’re building. Since then, the Shoggoth has gone viral, or as viral as it’s possible to go in the small world of hyper-online A.I. insiders. It’s a popular meme on A.I. Twitter (including a now-deleted tweet by Elon Musk), a recurring metaphor in essays and message board posts about A.I. risk, and a bit of useful shorthand in conversations with A.I. safety experts. One A.I. start-up, NovelAI, said it recently named a cluster of computers “Shoggy” in homage to the meme. Another A.I. company, Scale AI, designed a line of tote bags featuring the Shoggoth. Shoggoths are fictional creatures, introduced by the science fiction author H.P. Lovecraft in his 1936 novella “At the Mountains of Madness.” In Lovecraft’s telling, Shoggoths were massive, blob-like monsters made out of iridescent black goo, covered in tentacles and eyes. Shoggoths landed in the A.I. world in December, a month after ChatGPT’s release, when Twitter user @TetraspaceWest replied to a tweet about GPT-3 (an OpenAI language model that was ChatGPT’s predecessor) with an image of two hand-drawn Shoggoths — the first labeled “GPT-3” and the second labeled “GPT-3 + RLHF.” The second Shoggoth had, perched on one of its tentacles, a smiley-face mask. In a nutshell, the joke was that in order to prevent A.I. language models from behaving in scary and dangerous ways, A.I. companies have had to train them to act polite and harmless. One popular way to do this is called “reinforcement learning from human feedback,” or R.L.H.F., a process that involves asking humans to score chatbot responses, and feeding those scores back into the A.I. model. Most A.I. researchers agree that models trained using R.L.H.F. are better behaved than models without it. But some argue that fine-tuning a language model this way doesn’t actually make the underlying model less weird and inscrutable. In their view, it’s just a flimsy, friendly mask that obscures the mysterious beast underneath. @TetraspaceWest, the meme’s creator, told me in a Twitter message that the Shoggoth “represents something that thinks in a way that humans don’t understand and that’s totally different from the way that humans think.” Comparing an A.I. language model to a Shoggoth, @TetraspaceWest said, wasn’t necessarily implying that it was evil or sentient, just that its true nature might be unknowable. “I was also thinking about how Lovecraft’s most powerful entities are dangerous — not because they don’t like humans, but because they’re indifferent and their priorities are totally alien to us and don’t involve humans, which is what I think will be true about possible future powerful A.I.” The Shoggoth image caught on, as A.I. chatbots grew popular and users began to notice that some of them seemed to be doing strange, inexplicable things their creators hadn’t intended. In February, when Bing’s chatbot became unhinged and tried to break up my marriage, an A.I. researcher I know congratulated me on “glimpsing the Shoggoth.” A fellow A.I. journalist joked that when it came to fine-tuning Bing, Microsoft had forgotten to put on its smiley-face mask. Eventually, A.I. enthusiasts extended the metaphor. In February, Twitter user @anthrupad created a version of a Shoggoth that had, in addition to a smiley-face labeled “R.L.H.F.,” a more humanlike face labeled “supervised fine-tuning.” (You practically need a computer science degree to get the joke, but it’s a riff on the difference between general A.I. language models and more specialized applications like chatbots.) Today, if you hear mentions of the Shoggoth in the A.I. community, it may be a wink at the strangeness of these systems — the black-box nature of their processes, the way they seem to defy human logic. Or maybe it’s an in-joke, visual shorthand for powerful A.I. systems that seem suspiciously nice. If it’s an A.I. safety researcher talking about the Shoggoth, maybe that person is passionate about preventing A.I. systems from displaying their true, Shoggoth-like nature. In any case, the Shoggoth is a potent metaphor that encapsulates one of the most bizarre facts about the A.I. world, which is that many of the people working on this technology are somewhat mystified by their own creations. They don’t fully understand the inner workings of A.I. language models, how they acquire new capabilities or why they behave unpredictably at times. They aren’t totally sure if A.I. is going to be net-good or net-bad for the world. And some of them have gotten to play around with the versions of this technology that haven’t yet been sanitized for public consumption — the real, unmasked Shoggoths. That some A.I. insiders refer to their creations as Lovecraftian horrors, even as a joke, is unusual by historical standards. (Put it this way: Fifteen years ago, Mark Zuckerberg wasn’t going around comparing Facebook to Cthulhu.) And it reinforces the notion that what’s happening in A.I. today feels, to some of its participants, more like an act of summoning than a software development process. They are creating the blobby, alien Shoggoths, making them bigger and more powerful, and hoping that there are enough smiley faces to cover the scary parts. Kevin Roose is a technology columnist and the author of “Futureproof: 9 Rules for Humans in the Age of Automation.” @kevinroose • Facebook Executives from leading A.I. companies, including OpenAI and Google, warned that the technology they were building might one day pose an existential threat to humanity. One of the most urgent warnings about the risks of A.I. has come from Geoffrey Hinton, whom many consider to be the godfather of artificial intelligence. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. The Age of A.I. In Silicon Valley’s hacker houses, young A.I. entrepreneurs are partying, innovating — and hoping not to get crushed by the big guys. A lawyer representing a man who sued an airline relied on ChatGPT to help prepare a court filing. It did not go well. How has the Shoggoth, an octopus-like creature from a science fiction story, come to symbolize the state of artificial intelligence? Kevin Roose explains. Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach."
80,https://blogs.microsoft.com/blog/2023/05/23/microsoft-build-brings-ai-tools-to-the-forefront-for-developers/,"May 23, 2023 | Frank X. Shaw - Chief Communications Officer, Microsoft You only need two simple letters to accurately convey the major shift in the technology space this year: A and I. Beyond those letters, however, is a complex, evolving and exciting way in which we work, communicate and collaborate. As you will see, artificial intelligence is a common thread as we embark on Microsoft Build, our annual flagship event for developers. It’s already been a landmark year for the industry, starting in January with the announcement of an extension of our partnership with OpenAI to accelerate AI breakthroughs and to ensure these benefits are broadly shared with the world. And in February, Microsoft announced an all-new, AI-powered Bing search engine and Edge browser to transform the largest software category in the world – search. Since then, developments have accelerated at a rapid pace, with several key milestones along the way, including: This is just the beginning of the new era of AI. That’s why Microsoft Build is so important. During this event, we’ll be showcasing how AI is redefining what and how developers build, as well as how AI is changing the future of work. Before we get into the news, let’s talk about two concepts we are discussing at length during Microsoft Build: copilots and plugins. A copilot is an application that uses modern AI and large language models (LLMs) like GPT-4 to assist people with complex tasks. Microsoft first introduced the concept of a copilot nearly two years ago with GitHub Copilot, an AI pair programmer that assists developers with writing code, and we continue to release copilots across many of the company’s core businesses. We believe the copilot represents both a new paradigm in AI-powered software and a profound shift in the way that software is built – from imagining new product scenarios, to the user experience, the architecture, the services that it uses and how to think about safety and security. Plugins are tools first introduced for ChatGPT, and more recently Bing, which augment the capabilities of AI systems, enabling them to interact with application programming interfaces (APIs) from other software and services to retrieve real-time information, incorporate company and other business data, perform new types of computations and safely take action on the user’s behalf. Think of plugins as the connection between copilots and the rest of the digital world. With that said, let’s focus on the news and announcements we’re unveiling during Microsoft Build. Growing the AI plugin ecosystem Microsoft is announcing that we will adopt the same open plugin standard that OpenAI introduced for ChatGPT, enabling interoperability across ChatGPT and the breadth of Microsoft’s copilot offerings. Developers can now use one platform to build plugins that work across both consumer and business surfaces, including ChatGPT, Bing, Dynamics 365 Copilot and Microsoft 365 Copilot. And if you want to develop and use your own plugins with your AI application built on Azure OpenAI Service, it will, by default, be interoperable with this same plugin standard. This means developers can build experiences that enable people to interact with their apps using the most natural user interface: the human language. As part of this shared plugin platform, Bing is adding to its support for plugins. In addition to previously announced plugins for OpenTable and Wolfram Alpha, we will also have Expedia, Instacart, Kayak, Klarna, Redfin and Zillow, among many others in the Bing ecosystem. In addition to the common plugin platform, Microsoft is announcing that Bing is coming to ChatGPT as the default search experience. ChatGPT will now have a world-class search engine built-in to provide more up-to-date answers with access from the web. Now, answers are grounded by search and web data and include citations so users can learn more, all directly from within chat. The new experience is rolling out to ChatGPT Plus subscribers starting today and will be available to free users soon by simply enabling a plugin. Developers can now extend Microsoft 365 Copilot with plugins  We’re also announcing that developers can now integrate their apps and services into Microsoft 365 Copilot with plugins. Plugins for Microsoft 365 Copilot include ChatGPT and Bing plugins, as well as Teams message extensions and Power Platform connectors – enabling developers to leverage their existing investments. And developers will be able to easily build new plugins for Microsoft 365 Copilot with the Microsoft Teams Toolkit for Visual Studio Code and Visual Studio. Developers can also extend Microsoft 365 Copilot by bringing their data into the Microsoft Graph, contextualizing relevant and actionable information with the recently announced Semantic Index for Copilot. More than 50 plugins from partners will be available for customers as part of the early access program, including Atlassian, Adobe, ServiceNow, Thomson Reuters, Moveworks and Mural, with thousands more available by the general availability of Microsoft 365 Copilot. New Azure AI tooling to help developers build, operationalize deploy their own next-generation AI apps It starts with our new Azure AI Studio. We’re making it simple to integrate external data sources into Azure OpenAI Service. In addition, we’re excited to introduce Azure Machine Learning prompt flow to make it easier for developers to construct prompts while taking advantage of popular open-source prompt orchestration solutions like Semantic Kernel. In Azure OpenAI Service, which brings together advanced models including ChatGPT and GPT-4, with the enterprise capabilities of Azure, we’re announcing updates to enable developers to deploy the most cutting-edge AI models using their own data; a Provisioned Throughput SKU that offers dedicated capacity; and plugins that simplify integrating other external data sources into a customer’s use of Azure OpenAI Service. We now have more than 4,500 customers using Azure OpenAI Service. Building responsibly together At Microsoft, we’ve been committed to developing AI technology that has a beneficial impact and earns trust, while also sharing our own learnings and building new tools and innovations that help developers and businesses implement responsible AI practices in their own work and organizations. At Build, we’re introducing several new updates, including Azure AI Content Safety, a new Azure AI service to help businesses create safer online environments and communities. As part of Microsoft’s commitment to building responsible AI systems, Azure AI Content Safety will be integrated across Microsoft products, including Azure OpenAI Service and Azure Machine Learning. We’re also introducing new tools to Azure Machine Learning, including expanding Responsible AI dashboard support for text and image data, in preview, enabling users to evaluate large models built with unstructured data during the model building, training and/or evaluation stage. This helps users identify model errors, fairness issues and model explanations before models are deployed, for more performant and fair computer vision and natural language processing (NLP) models. And prompt flow, in preview soon, provides a streamlined experience for prompting, evaluating and tuning large language models. Users can quickly create prompt workflows that connect to various language models and data sources and assess the quality of their workflows with measurements such as groundedness to choose the best prompt for their use case. Prompt flow also integrates Azure AI Content Safety to help users detect and remove harmful content directly in their flow of work. In addition, Microsoft announced new media provenance capabilities coming to Microsoft Designer and Bing Image Creator in the coming months that will enable users to verify whether an image or video was generated by AI. The technology uses cryptographic methods to mark and sign AI-generated content with metadata about its origin. Introducing Microsoft Fabric, a new unified platform for analytics Today’s world is awash with data, constantly streaming from the devices we use, the applications we build and the interactions we have. And now, as we enter a new era defined by AI, this data is becoming even more important. Powering organization-specific AI experiences requires a constant supply of clean data from a well-managed and highly integrated analytics system. But most organizations’ analytics systems are a labyrinth of specialized and disconnected services. Microsoft Fabric is a unified platform for analytics that includes data engineering, data integration, data warehousing, data science, real-time analytics, applied observability and business intelligence, all connected to a single data repository called OneLake. It enables customers of all technical levels to experience capabilities in a single, unified experience. It is infused with Azure OpenAI Service at every layer to help customers unlock the full potential of their data, enabling developers to leverage the power of generative AI to find insights in their data. With Copilot in Microsoft Fabric in every data experience, customers can use conversational language to create dataflows and data pipelines, generate code and entire functions, build machine learning models or visualize results. Customers can even create their own conversational language experiences that combine Azure OpenAI Service models and their data and publish them as plugins. Accelerating an AI-powered future through partners Our customers benefit from our partner collaborations, such as with NVIDIA, that enable organizations to design, develop, deploy and manage applications with the scale and security of Azure. NVIDIA will accelerate enterprise-ready generative AI with NVIDIA AI Enterprise Integration with Azure Machine Learning. Omniverse Cloud, only available on Azure, enables organizations to aggregate data into massive, high-performance models, connect their domain-specific software tools and enable multi-user live collaboration across factory locations. NVIDIA GPUs leveraging ONNX Runtime & Olive toolchain will support the implementation of accelerating AI models without needing a deeper knowledge of the hardware. New capabilities for Microsoft Dev Box Microsoft Dev Box, an Azure service that gives developers access to ready-to-code, project-specific dev boxes that are preconfigured and centrally managed, is introducing several new capabilities to enhance the developer experience and boost productivity. While in preview, we’ve seen many customers experimenting with Dev Box, and we’ve migrated more than 9,000 developers internally to the service for day-to-day software development. Now, we’ve added additional features and capabilities, including customization using configuration-as-code and new starter developer images in Azure Marketplace that provide dev teams with ready-to-use images that can be customized further for specific dev team needs. Additionally, developers can now manage custom environments from a specialized developer portal, Azure Deployment Environments. Dev Box general availability will begin in July. Unveiling a new home for developers on Windows 11 with Dev Home  Dev Home will launch at Microsoft Build in preview as a new Windows experience developers can get from the Microsoft Store. Dev Home makes it easy to connect to GitHub and configure cloud development environments like Microsoft Dev Box and GitHub Codespaces. Dev Home is open source and fully extensible, enabling developers to enhance their experience with a customizable dashboard and the tools they need to be successful. Introducing Windows Copilot for Windows 11 Last fall at our Windows and Surface launch, Chief Product Officer Panos Panay talked about the power of AI to unlock new interaction models on the PC with Windows Studio Effects and DALL-E 2 in Microsoft Designer, and at CES he talked about how AI is going to reinvent the way people get things done on Windows. This brings us to Windows Copilot. Windows will be the first PC platform to centralize AI assistance with the introduction of Windows Copilot. Together, with Bing Chat and first- and third-party plugins, users can focus on bringing their ideas to life, completing complex projects and collaborating instead of spending energy finding, launching and working across multiple applications. This builds on the integration we released into Windows 11 back in February that brought the new AI-powered Bing to the taskbar. A preview of Windows Copilot will start to become available for Windows 11 in June. As you can see, it’s going to be a busy time at Microsoft Build. To give you a sense of what developers are going to experience at the event, we’re expecting approximately 200,000 registered attendees, with 350 sessions and more than 125 hours of content over two days. In total, we’ll announce more than 50 new products and features. For more information, make sure to watch keynotes on demand from Microsoft Chairman and CEO Satya Nadella, Kevin Scott and Scott Guthrie on Day 1. On Day 2, watch the keynotes anchored by Rajesh Jha and Panos Panay. Additionally, you can explore all the news and announcements in the Book of News and read more stories and news about products from Microsoft Build here: Watch Microsoft Build keynotes and view videos and photos Microsoft outlines framework for building AI apps and copilots; expands AI plugin ecosystem Bing at Microsoft Build 2023: Continuing the Transformation of Search Empowering every developer with plugins for Microsoft 365 Copilot Bringing the power of AI to Windows 11 – unlocking a new era of productivity for customers and developers with Windows Copilot and Dev Home Build next-generation, AI-powered applications on Microsoft Azure Introducing Microsoft Fabric: Data analytics for the era of AI Tags: AI, Azure AI Content Safety, Azure OpenAI Service, Bing, copilots, developers, Microsoft 365 Copilot, Microsoft Build, plugins, Windows 11 May 9, 2023 | Jared Spataro May 4, 2023 | Yusuf Mehdi Apr 24, 2023 | Judson Althoff Mar 28, 2023 | Vasu Jakkal Mar 21, 2023 | Yusuf Mehdi"
81,https://www.nytimes.com/2023/05/25/technology/microsoft-ai-rules-regulation.html,"Its president, Brad Smith, said companies needed to “step up” and governments needed to “move faster” as artificial intelligence progressed. Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By David McCabe David McCabe reports on tech policy from Washington. Microsoft endorsed a crop of regulations for artificial intelligence on Thursday, as the company navigates concerns from governments around the world about the risks of the rapidly evolving technology. Microsoft, which has promised to build artificial intelligence into many of its products, proposed regulations including a requirement that systems used in critical infrastructure can be fully turned off or slowed down, similar to an emergency braking system on a train. The company also called for laws to clarify when additional legal obligations apply to an A.I. system and for labels making it clear when an image or a video was produced by a computer. “Companies need to step up,” Brad Smith, Microsoft’s president, said in an interview about the push for regulations. “Government needs to move faster.” He laid out the proposals in front of an audience that included lawmakers at an event in downtown Washington on Thursday morning. The call for regulations punctuates a boom in A.I., with the release of the ChatGPT chatbot in November spawning a wave of interest. Companies including Microsoft and Google’s parent, Alphabet, have since raced to incorporate the technology into their products. That has stoked concerns that the companies are sacrificing safety to reach the next big thing before their competitors. Lawmakers have publicly expressed worries that such A.I. products, which can generate text and images on their own, will create a flood of disinformation, be used by criminals and put people out of work. Regulators in Washington have pledged to be vigilant for scammers using A.I. and instances in which the systems perpetuate discrimination or make decisions that violate the law. In response to that scrutiny, A.I. developers have increasingly called for shifting some of the burden of policing the technology onto government. Sam Altman, the chief executive of OpenAI, which makes ChatGPT and counts Microsoft as an investor, told a Senate subcommittee this month that government must regulate the technology. The maneuver echoes calls for new privacy or social media laws by internet companies like Google and Meta, Facebook’s parent. In the United States, lawmakers have moved slowly after such calls, with few new federal rules on privacy or social media in recent years. In the interview, Mr. Smith said Microsoft was not trying to slough off responsibility for managing the new technology, because it was offering specific ideas and pledging to carry out some of them regardless of whether government took action. “There is not an iota of abdication of responsibility,” he said. He endorsed the idea, supported by Mr. Altman during his congressional testimony, that a government agency should require companies to obtain licenses to deploy “highly capable” A.I. models. “That means you notify the government when you start testing,” Mr. Smith said. “You’ve got to share results with the government. Even when it’s licensed for deployment, you have a duty to continue to monitor it and report to the government if there are unexpected issues that arise.” Microsoft, which made more than $22 billion from its cloud computing business in the first quarter, also said those high-risk systems should be allowed to operate only in “licensed A.I. data centers.” Mr. Smith acknowledged that the company would not be “poorly positioned” to offer such services, but said many American competitors could also provide them. Microsoft added that governments should designate certain A.I. systems used in critical infrastructure as “high risk” and require them to have a “safety brake.” It compared that feature to “the braking systems engineers have long built into other technologies such as elevators, school buses and high-speed trains.” In some sensitive cases, Microsoft said, companies that provide A.I. systems should have to know certain information about their customers. To protect consumers from deception, content created by A.I. should be required to carry a special label, the company said. Mr. Smith said companies should bear the legal “responsibility” for harms associated with A.I. In some cases, he said, the liable party could be the developer of an application like Microsoft’s Bing search engine that uses someone else’s underlying A.I. technology. Cloud companies could be responsible for complying with security regulations and other rules, he added. “We don’t necessarily have the best information or the best answer, or we may not be the most credible speaker,” Mr. Smith said. “But, you know, right now, especially in Washington D.C., people are looking for ideas.” David McCabe covers tech policy. He joined The Times from Axios in 2019. Executives from leading A.I. companies, including OpenAI and Google, warned that the technology they were building might one day pose an existential threat to humanity. One of the most urgent warnings about the risks of A.I. has come from Geoffrey Hinton, whom many consider to be the godfather of artificial intelligence. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. The Age of A.I. In Silicon Valley’s hacker houses, young A.I. entrepreneurs are partying, innovating — and hoping not to get crushed by the big guys. A lawyer representing a man who sued an airline relied on ChatGPT to help prepare a court filing. It did not go well. How has the Shoggoth, an octopus-like creature from a science fiction story, come to symbolize the state of artificial intelligence? Kevin Roose explains. Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach."
82,https://www.nytimes.com/2023/05/25/technology/reid-hoffman-artificial-intelligence.html,"Few are as involved in so many different artificial intelligence efforts as Mr. Hoffman, a Silicon Valley investor who co-founded LinkedIn. “I’m beating the positive drum very loudly,” Reid Hoffman, a venture capitalist, said of A.I.Credit...Clara Mokri for The New York Times Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Erin Griffith Reporting from San Francisco. Reid Hoffman, billionaire entrepreneur and venture capital investor, is worried about artificial intelligence — but not for the doomsday reasons making headlines. Instead, he worries the doomsday headlines are too negative. So in recent months, Mr. Hoffman has engaged in an aggressive thought-leadership regimen to extol the virtues of A.I. He has done so in blog posts, television interviews and fireside chats. He has spoken to government officials around the world. He hosts three podcasts and a YouTube channel. And in March, he published a book, “Impromptu,” co-written with the A.I. tool GPT-4. It’s all part of the land grab for public opinion around A.I. in preparation for when the initial burst of fear and hype over the technology settles into a coherent debate. Sides will be chosen, regulation will be proposed, and tech tools will become politicized. For now, industry leaders like Mr. Hoffman are trying to nudge the terms of the discussion in their favor, even as public concerns only seem to grow. “I’m beating the positive drum very loudly, and I’m doing so deliberately,” he said. Few are as intertwined in so many facets of the fast-moving industry as Mr. Hoffman. The 55-year-old sits on the boards of 11 tech companies including Microsoft, which has gone all in on A.I., and eight nonprofits. His venture capital firm, Greylock Partners, has backed at least 37 A.I. companies. He was among the first investors in OpenAI, the most prominent A.I. start-up, and recently left its board. He also helped found Inflection AI, an A.I. chatbot start-up that has raised at least $225 million. And then there is his more abstract goal of “elevating humanity,” or helping people improve their circumstances, a concept he relays in an affable, matter-of-fact manner. Mr. Hoffman believes A.I. is critical to that mission and as examples points to its potential to transform areas like health care — “giving everyone a medical assistant”; and education — “giving everyone a tutor.” “That’s part of the responsibility that we should be thinking about here,” he said. Mr. Hoffman is among a small group of interconnected tech executives leading the A.I. charge, many of whom also led the last internet boom. He is a member of the “PayPal Mafia” of former PayPal executives that includes Elon Musk and Peter Thiel. The latter two backed DeepMind, an A.I. start-up that Google bought, and all three were early backers of OpenAI. Jessica Livingston, a founder of the start-up incubator Y Combinator, also put money into OpenAI; Sam Altman, OpenAI’s chief executive, was previously president of Y Combinator. Mr. Musk has now started his own A.I. company, X.AI. Mr. Thiel’s venture firm, Founders Fund, has backed more than 70 A.I. companies, including OpenAI, according to PitchBook, which tracks start-up investments. Mr. Altman has invested in several A.I. start-ups on top of running OpenAI, which itself has invested in seven A.I. start-ups through its start-up fund. And Y Combinator’s latest batch of start-ups included 78 focused on A.I., nearly double its last group. The tech leaders differ on A.I.’s risks and opportunities and have been loudly promoting their takes in the marketplace of ideas. Mr. Musk recently warned of A.I.’s dangers on Bill Maher’s show and in a sit-down with Senator Chuck Schumer, Democrat of New York. Mr. Hoffman has explained the technology’s potential to Vice President Kamala Harris, Commerce Secretary Gina Raimondo and Transportation Secretary Pete Buttigieg. Last week, Mr. Altman said in a congressional hearing that “the benefits of the tools we have deployed so far vastly outweigh the risks.” In Mr. Hoffman’s view, warnings about A.I.’s existential risk to humanity overstate what the technology can do. And he believes that other potential issues caused by A.I. — job loss, destruction of democracy, disruption of the economy — have an obvious fix: more technology. “The solutions live in the future, not by enshrining the past,” he said. That’s a tough pitch to a public that has seen tech’s harmful effects over the last decade, including social media misinformation and autonomous vehicle crashes. And this time, the risks are even larger, said Oded Netzer, a professor at Columbia Business School. “It’s not just the risks, it’s how fast they are moving,” Mr. Netzer said of tech companies’ handling of A.I. “I don’t think we can hope or trust that the industry will regulate itself.” Mr. Hoffman’s pro-A.I. campaign, he said, is meant to foster trust where it’s broken. “It’s not to say that there won’t be some harms in some areas,” he said. “The question is could we learn and iterate to a much better state?” Mr. Hoffman has been thinking about that question since he studied symbolic systems at Stanford University in the late 1980s. There, he imagined how A.I. would facilitate “our Promethean moment,” he said in a YouTube video from March. “We can make these new things and we can journey with them.” After working at PayPal and co-founding LinkedIn, the professional social network, in 2002, Mr. Hoffman began investing in start-ups including Nauto, Nuro and Aurora Innovation, all focused on applying A.I. tech to transportation. He also joined an A.I. ethics committee at DeepMind. Mustafa Suleyman, DeepMind’s co-founder, said Mr. Hoffman differed from other venture capitalists in that his primary motivation was doing good in the world. “How can we be in service of humanity? He asked that question all the time,” Mr. Suleyman said. When Mr. Suleyman began working on his latest start-up, Inflection AI, he found Mr. Hoffman’s strategic advice to be so useful that he asked him to help found the company. Greylock invested in the start-up last year. Mr. Hoffman was also there in OpenAI’s early days. At an Italian restaurant in San Jose, Calif., in 2015, he met with Mr. Musk and Mr. Altman to discuss the beginnings of the company, which has a mission of ensuring that the most powerful A.I. “benefits all of humanity.” Several years later, when OpenAI was thinking about corporate partnerships, Mr. Hoffman said he encouraged Mr. Altman to meet with Microsoft, which had bought LinkedIn in 2016. Mr. Altman said he was initially anxious that Microsoft, a behemoth with a duty to prioritize its shareholders, might not take seriously OpenAI’s mission and unusual structure of capping its profits. In any large, complicated deal, Mr. Altman said, “everyone’s anxious about, ‘How is this really going to work?’” Mr. Hoffman helped smooth things out. He talked Mr. Altman through various concerns while wearing metaphorical “hats” as an OpenAI board member, a Microsoft board member, and as himself. “You have to be really clear about which hat you’re talking with,” Mr. Hoffman said. Mr. Altman said Mr. Hoffman helped OpenAI “model Microsoft and think about what they’d care about, what they’d be good at, what they’d be bad at, and similar to them for us.” In 2019, OpenAI and Microsoft struck a $1 billion agreement, which has propelled them into a leading position today. (To avoid a conflict of interest, Mr. Hoffman was not part of the negotiations and abstained from voting to approve the deal on each board.) A little over a year ago, as Mr. Hoffman saw the progress OpenAI was making on its GPT-3 language model, he had another Promethean moment. He immediately flipped an A.I. switch on nearly everything he worked on, including Greylock’s new investments and existing start-ups as well as his podcast, book and discussions with government officials. “It was basically like, ‘If it’s not this, it better be something that’s absolutely critical for society,’” he said. OpenAI released a chatbot, ChatGPT, in November, which became a sensation. One Greylock investment, Tome, integrated OpenAI’s GPT-3 technology into its “storytelling” software immediately after. The number of Tome users skyrocketed to six million from a few thousand teams, said Keith Peiris, Tome’s chief executive. Mr. Hoffman said his approach was shaped, in part, by his access to “extremely high-quality information flows,” Some is through his business relationships with Microsoft, OpenAI and others. Some is through various philanthropies, like Stanford’s A.I. center. And some is through his political connections. He has poured millions of dollars into Democratic campaigns and political action committees. Barack Obama is a friend, he said. For now, he’s using his influence to paint a picture of A.I.-driven progress. Tech insiders cheer on his cheerleading. The rest of the world is more skeptical. A recent survey conducted by Reuters and Ipsos showed that 61 percent of Americans believe A.I. could be a threat to humanity. Mr. Hoffman brushes off those fears as overblown. He expects that the more tangible problems facing A.I., including its tendency to spit out incorrect information, will be worked out as tech companies upgrade their systems and deploy them to help. Looking ahead, he said, there will be more investments, more podcasts, more conversations with government officials and more work on Inflection AI. The way to navigate A.I.’s risks, he stressed, is by steering the world toward the positives. “I’m a tech optimist, not a tech utopian,” he said. Cade Metz contributed reporting. Erin Griffith reports on technology start-ups and venture capital from the San Francisco bureau. Before joining The Times she was a senior writer at Wired and Fortune. @eringriffith Executives from leading A.I. companies, including OpenAI and Google, warned that the technology they were building might one day pose an existential threat to humanity. One of the most urgent warnings about the risks of A.I. has come from Geoffrey Hinton, whom many consider to be the godfather of artificial intelligence. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. The Age of A.I. In Silicon Valley’s hacker houses, young A.I. entrepreneurs are partying, innovating — and hoping not to get crushed by the big guys. A lawyer representing a man who sued an airline relied on ChatGPT to help prepare a court filing. It did not go well. How has the Shoggoth, an octopus-like creature from a science fiction story, come to symbolize the state of artificial intelligence? Kevin Roose explains. Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach."
83,https://www.nytimes.com/2023/05/24/business/artificial-intelligence-regulation-openai.html,"By Gregory Schmidt Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. The leaders of OpenAI, the artificial intelligence research lab that developed the chatbot ChatGPT, have called for regulation of “superintelligent” A.I. technology, saying it “will be more powerful than other technologies humanity has had to contend with in the past.” To regulate the risks of A.I. systems, there should be an international watchdog, similar to the International Atomic Energy Agency, the organization that promotes the peaceful use of nuclear energy, OpenAI’s founders, Greg Brockman and Ilya Sutskever, and its chief executive, Sam Altman, wrote in a note posted Monday on the company’s website. “Given the possibility of existential risk, we can’t just be reactive,” they wrote. Mr. Altman appeared before Congress on May 16 to implore lawmakers to regulate artificial intelligence. Congressional leaders shared their worries about the threats that A.I. could pose, including the spread of misinformation and privacy violations. “I think if this technology goes wrong, it can go quite wrong. And we want to be vocal about that,” Mr. Altman said in his testimony before members of a Senate subcommittee. In March, more than 1,000 technology leaders and researchers, including Elon Musk, the chief executive of Tesla and owner of Twitter, called for a moratorium on the development of the most advanced A.I. systems, warning in an open letter that the tools presented “profound risks to society and humanity.” In their latest note, the OpenAI leaders said that “it’s conceivable that within the next 10 years, A.I. systems will exceed expert skill level in most domains, and carry out as much productive activity as one of today’s largest corporations.” The latest A.I. tools could upend the economics of the internet, turning today’s tech giants into has-beens and creating the industry’s next powerhouses. Tech companies have spent billions of dollars on A.I., amid the rising concerns about its potential to match human reasoning and destroy jobs. Goldman Sachs estimated recently that A.I. could expose 300 million full-time jobs to automation. BuzzFeed just introduced a chatbot that offers recipe recommendations. At last week’s hearing, Senator Richard Blumenthal, Democrat of Connecticut and chairman of the Senate panel, acknowledged that Congress had failed to keep up with new technologies. He added that the hearing was the first in a series to explore the potential of A.I. and eventually “write the rules” for it. “Our goal is to demystify and hold accountable those new technologies to avoid some of the mistakes of the past,” he said. But over the years, partisan squabbles and intense lobbying by the tech industry have stalled dozens of bills intended to strengthen privacy, speech and safety rules. Gregory Schmidt covers breaking news and real estate and is the editor of the Square Feet column. @GregoryNYC Executives from leading A.I. companies, including OpenAI and Google, warned that the technology they were building might one day pose an existential threat to humanity. One of the most urgent warnings about the risks of A.I. has come from Geoffrey Hinton, whom many consider to be the godfather of artificial intelligence. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. The Age of A.I. In Silicon Valley’s hacker houses, young A.I. entrepreneurs are partying, innovating — and hoping not to get crushed by the big guys. A lawyer representing a man who sued an airline relied on ChatGPT to help prepare a court filing. It did not go well. How has the Shoggoth, an octopus-like creature from a science fiction story, come to symbolize the state of artificial intelligence? Kevin Roose explains. Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach."
84,https://www.washingtonpost.com/business/interactive/2023/artificial-intelligence-tech-rapid-advances/,"Prompt: This small bird has a pink breast and crown, and black primaries and secondaries A cutting-edge model generates tiny images of birds and flowers. (Reed et al.) A new training method called diffusion helps create images with greater detail, but the perspective still looks flattened. (Stable Diffusion) Dall-E 2 incorporates more breakthroughs and data to produce an image with a shallow depth-of-field characteristic of many bird photos, but does not include a pink breast. (Dall-E 2) Artificial intelligence has become shockingly capable in the past year. The latest chatbots can conduct fluid conversations, craft poems, even write lines of computer code while the latest image-makers can create fake “photos” that are virtually indistinguishable from the real thing. It wasn’t always this way. As recently as two years ago, AI created robotic text riddled with errors. Images were tiny, pixelated and lacked artistic appeal. The mere suggestion that AI might one day rival human capability and talent drew ridicule from academics. A confluence of innovations has spurred growth. Breakthroughs in mathematical modeling, improvements in hardware and computing power, and the emergence of massive high-quality data sets have supercharged generative AI tools. [Quiz: Did AI make this? Test your knowledge.] While artificial intelligence is likely to improve even further, experts say the past two years have been uniquely fertile. Here’s how it all happened so fast. How three AI systems responded to the following text prompt A clever name for a play about dinosaurs is “to quote the most brilliant of examples : ‘who can say for sure they weren’t aliens or aliens or aliens? they had the appearance of aliens ; they wore black leather pants and they walked with much” This model does not follow the sentence structure. (OpenAI GPT) “Dino-Brawl. Dinosaur-Brawl — which can be viewed in the player’s perspective — starts with Mr. D, a dino-hunter, chasing down a lone” This model gave a name but followed it with a confusing sentence. (GPT-2) “Dino-Mite: The Mesozoic Extravaganza!” This model used a pun for the name and provided a subtitle. (Chat-GPT) Much of this recent growth stems from a new way of training AI, called the Transformers model. This method allows the technology to process large blocks of language quickly and to test the fluency of the outcome. It originated in a 2017 Google study that quickly became one of the field’s most influential pieces of research. To understand how the model works, consider a simple sentence: “The cat went to the litter box.” Previously, artificial intelligence models would analyze the sentence sequentially, processing the word “the” before moving onto “cat” and so on. This took time, and the software would often forget its earlier learning as it read new sentences, said Mark Riedl, a professor of computing at Georgia Tech. The transformers model immediately processes the relationships between words — a method called attention. New AI models can examine “cat” alongside “litter” and “box.” To make sure the AI performs correctly, the transformers model builds in a testing step. It masks a word in the sentence to see if the AI can predict what’s missing. Additionally, companies such as OpenAI have humans rate the quality of the response. For example, if the word “cat” is masked and the computer offers “the dog went to the litter box,” it’s likely to get a thumbs down. The model allows AI tools to ingest billions of sentences and quickly recognize patterns, resulting in more natural-sounding responses. Another new training method, called diffusion, has also improved AI image generators such as Dall-E and Midjourney, allowing nearly anyone to create hyper-realistic photos with simple, even nonsensical, text prompts, such as: “Draw me a picture of a rabbit in outer space.” Researchers feed these AI models billions of images, each paired with a text description, teaching the computer to identify relationships between images and words. The diffusion method then layers “noise” — visual clutter that looks like TV static — over the images. The AI system learns to recognize the noise and subtract it until the image is once again clear. [ AI can now create images out of thin air. See how it works.] This process of corrupting and regenerating images teaches the AI to remove imperfections, fine tuning each response until it is crisp and sharp. It also learns the relationship between neighboring pixels, making the generated image more realistic. Images that three AI systems generated from the following prompt A picture of a very clean living room This model generates an image so small the details are impossible to see. (Reed et al.) This model generates an image that resembles a living room, but the furniture and mirror are disfigured. (Stable Diffusion) This model generates an image with clean lines and reflections, but details like the coffee table’s legs are deformed. (Dall-E 2) These bigger, more complicated AI models require advanced computer systems. This hardware, too, has become much more powerful. Graphics processing units, or “GPUs” — circuits commonly used for video games — offer immense computing power and excel at running tasks simultaneously, a key requirement for training artificial intelligence models. Researchers can string thousands of GPUs together in super computers that operate in the cloud. This lets people access GPUs without building their own supercomputer — a system that makes high-powered computing cheaper and readily available to the masses. Over the past several years, the data sets used to train AI have gotten bigger and better, too. Previous data sets had lower quality content, Riedl said, like addresses or phone numbers, which aren’t helpful for learning natural speech. Now, data sets include more news articles and book transcripts — content that shows how humans write and talk — which helps teach the AI to “speak” more fluidly. As AI companies have trained models, they have gotten better at knowing how to create high-quality data, Riedl said. Entire organizations are now dedicated to building these data sets. [Inside the secret list of websites that make AI like ChatGPT sound smart] Scale AI, a San Francisco company, curates and labels training data sets for AI software. ImageNet, a data set started by Stanford University researchers, contains millions of labeled images. Common Crawl, a nonprofit that creates an open repository of web data, has developed text data sets from more than 12 years of internet scrapes. Some data sets have grown to contain billions of words. More data sets are multimodal, meaning they combine images with text, allowing artificial intelligence systems to understand the relationship between the two. There also has been a rise in medical imaging data sets to fuel AI’s ability to spot diseases and ailments. How three AI systems responded to the following text prompt This is a haiku about baseball: “a man who’s going to lose, a man who doesn’t need to lose. he wants the people to win by doing something he wants, rather than because they’re being manipulated. they’re playing the game” This model begins in a somewhat poetic place but quickly stops making sense. (OpenAI GPT) “Fantasy Baseball: In the late 1800s the Yankees and his friends had a tournament where pitcher Francisco Gohara came to San Francisco from Texas at the height of what was called the Great Depression.” This model invents a sentence about a fake baseball player, seemingly ignoring the word “haiku.” (GPT-2) “In summer’s embrace,Bats crack, balls soar through the air,Baseball’s timeless grace.” This model’s output follows the traditional 5-7-5 syllable style, though the first and third line rhyme. (Chat-GPT) Experts say it’s hard to predict how much better AI will get. Major obstacles stand in the way of further development. These models are expensive to run and exact a staggering environmental toll. They confidently churn out wrong, nonsensical and sometimes biased answers, while creating lifelike images that could sow confusion. As tech giants such as Google and Microsoft race to incorporate AI into their products, a slew of companies are trying to expand AI’s capabilities to generate video, music and create detection tools to screen artificially generated content. Most people are likely to interact with this new technology in the near future. But how useful it will be and what impact it will have on society remains to be seen. For each AI comparison graphic, we fed AI image and text generators the same prompt and used the first result. The 2016 image model was too old to run ourselves, so we used images from the Reed paper. The image models were: Reed et al. (2016); Stable Diffusion v1.4 (first released in late 2021 but published in 2022); and Dall-E 2 (first released in 2022 but used in 2023). The text models were OpenAI-GPT (2018); GPT-2 Large (2019); and ChatGPT (first released in 2022 but used in 2023). Editing by Alexis Sobel Fitts, Reuben Fischer-Baum, Karly Domb Sadof and Kate Rabinowitz."
85,https://www.foreignaffairs.com/china/china-flirting-ai-catastrophe,"Few early observers of the Cold War could have imagined that the worst nuclear catastrophe of the era would occur at an obscure power facility in Ukraine. The 1986 Chernobyl disaster was the result of a flawed nuclear reactor design and a series of mistakes made by the plant operators. The fact that the world’s superpowers were spiraling into an arms race of potentially world-ending magnitude tended to eclipse the less obvious dangers of what was, at the time, an experimental new technology. And yet despite hair-raising episodes such as the Cuban missile crisis of 1962, it was a failure of simple safety measures, exacerbated by authoritarian crisis bungling, that resulted in the uncontrolled release of 400 times the radiation emitted by the U.S. nuclear bomb dropped on Hiroshima in 1945. Estimates of the devastation from Chernobyl range from hundreds to tens of thousands of premature deaths from radiation—not to mention an “exclusion zone” that is twice the size of London and remains largely abandoned to this day. As the world settles into a new era of rivalry­—this time between China and the United States—competition over another revolutionary technology, artificial intelligence, has sparked a flurry of military and ethical concerns parallel to those initiated by the nuclear race. Those concerns are well worth the attention they are receiving, and more: a world of autonomous weapons and machine-speed war could have devastating consequences for humanity. Beijing’s use of AI tools to help fuel its crimes against humanity against the Uyghur people in Xinjiang already amounts to a catastrophe. But of equal concern should be the likelihood of AI engineers’ inadvertently causing accidents with tragic consequences. Although AI systems do not explode like nuclear reactors, their far-reaching potential for destruction includes everything from the development of deadly new pathogens to the hacking of critical systems such as electrical grids and oil pipelines. Due to Beijing’s lax approach toward technological hazards and its chronic mismanagement of crises, the danger of AI accidents is most severe in China. A clear-eyed assessment of these risks—and the potential for spillover well beyond China’s borders—should reshape how the AI sector considers the hazards of its work. Characterizing AI risk has been a matter of public debate in recent months, with some experts claiming that superhuman intelligence will someday pose an existential threat to humanity and others lambasting “AI doomers” for catastrophizing. But even putting aside the most extreme fears of an AI dystopia, previous incidents have provided plenty of reasons to worry about unintended large-scale calamities in the near term. For instance, compounding machine-speed interactions between AI systems in finance could inadvertently crash markets, as algorithmic trading did in the 2010 “flash crash,” which temporarily wiped out a trillion dollars’ worth of stocks in minutes. When drug researchers used AI to develop 40,000 potential biochemical weapons in less than six hours last year, they demonstrated how relatively simple AI systems can be easily adjusted to devastating effect. Sophisticated AI-powered cyberattacks could likewise go haywire, indiscriminately derailing critical systems that societies depend on, not unlike the infamous NotPetya attack, which Russia launched against Ukraine in 2017 but eventually infected computers across the globe. Despite these warning signs, AI technology continues to advance at breakneck speed, causing the safety risks to multiply faster than solutions can be created. Most Americans may not be well versed in the specifics of these risks but nonetheless recognize the dangers of building powerful new technologies into complex, consequential systems. According to an Ipsos survey published in 2022, only 35 percent of Americans believe that AI’s benefits outweigh its risks, making the United States among the most pessimistic countries in the world about the technology’s promise. Surveys of engineers in American AI labs suggest that they may be, if anything, more safety-conscious than the broader public. Geoffrey Hinton, known as the “godfather of AI,” and until recently a vice president at Google, has quit the industry to advocate that scientists refrain from scaling up AI technology “until they have understood whether they can control it.” China, by contrast, ranks as the most optimistic country in the world when it comes to AI, with nearly four out of five Chinese nationals professing faith in its benefits over its risks. Whereas the United States government and Silicon Valley are many years into a backlash against a “move fast and break things” mentality, China’s tech companies and government still pride themselves on embracing that ethos. Chinese technology leaders are enthusiastic about their government’s willingness to live with AI risks that, in the words of veteran AI expert and Chinese technology executive Kai-Fu Lee, would “scare away risk-sensitive American politicians.” The disparity between Chinese and American perceptions of the hazards of AI—and their respective tech sectors’ willingness to take risks—is no accident. It is a result of Chinese policies that systematically suppress citizens’ experience of disasters to protect the government from public criticism. In the United States, disasters tend to prompt an elevated public consciousness and enhanced safety measures as their heart-rending consequences ripple through the media and society—in machinery-intensive industries such as oil drilling, everyday food and drug production, and the processing of dangerous chemicals. Even now, legislators in Ohio are making progress on new safety regulations in the wake of a fiery train derailment in February that shot a plume of toxic chemicals above the town of East Palestine. But in China, these types of accidents rarely reverberate through the media as the state maintains a chokehold on information to promote a constant atmosphere of stability. The Chinese Communist Party smothers information when disaster responses are mismanaged and routinely falsifies death tolls. The government sometimes refuses to acknowledge, let alone report on, vast tragedies such as the mass radiation poisoning that resulted from at least 40 nuclear tests conducted between 1964 and 1996, which led to the premature deaths of nearly 200,000 citizens. The result is a culture of disaster amnesia in which it is often impossible for the public to demand change or for the government to be forced to learn from costly accidents. Little accountability for mistakes means that business owners tend to play fast and loose with safety, as evidenced by China’s grisly history of industrial accidents. Even the rare instances in which mishaps are publicly exposed lack the staying power that might result in serious reform. For example, the public outcry about mass-produced toxic toothpaste in 2007, poisoned infant milk formula in 2008, and the collision of high-speed trains near Wenzhou in 2011 prompted well-publicized displays of scapegoating and loudly proclaimed government reform plans but had limited impacts on public safety. The Chinese government often projects a facade of responsiveness but then buries information about the events, quite literally in the case of the now-underground remains of the Wenzhou train wreckage. Given that China has a far more restrictive media ecosystem under Xi Jinping than it did when these incidents occurred, public exposure is even less likely today. With the worst run-ins with emerging technologies routinely excised from public consciousness, Chinese society exhibits a seemingly boundless sense of techno-optimism, especially toward new technologies such as AI. Given that China’s historic ascent from poverty went hand in hand with high-speed technological advancement, accelerated scientific research is practically synonymous with national progress in the Chinese zeitgeist—viewed as having few, if any, downsides. To see this full-steam-ahead approach in action, look no further than He Jiankui, the Chinese scientist who shocked the world in 2018 by genetically modifying human embryos in secret to produce the world's first gene-edited babies. The doctor expected, and initially received, high praise in China for his feat, but the government clumsily pulled an about-face in response to international outrage over his unilateral decision to push humanity into uncharted territory. Unsurprisingly, further examination showed that He irreversibly botched his experiment, in what one geneticist called ""a graphic demonstration of attempted gene editing gone awry.” He not only likely failed to make the modified babies (and their potential offspring) HIV-resistant as intended, but also potentially increased their susceptibility to influenza, cancer, and other diseases. After a stint in prison, He was released and continues his research, alongside new Chinese legislation providing loopholes for similar ethically fraught and potentially lucrative genetic experimentation. Not only are experimental technologies seen as largely risk-free in China, but the country has also committed itself to a feverish sprint to become “the world’s premier artificial intelligence innovation center” by 2030. China’s efforts to overtake the United States in AI have been a priority for the Communist Party since at least 2015, when Xi announced his “Made in China 2025” strategy. This emphasis on AI has since been reiterated in various national documents and speeches. AI has become a linchpin of China’s military modernization strategy and is increasingly integral to the country’s system of state surveillance, repression, and control. With so much at stake, it is no surprise that China’s government has been investing tens of billions of dollars annually into its AI sector and leveraging its vast espionage network to try to steal foreign corporate technology secrets. China’s AI frenzy is paying off. The country produces more top-tier AI engineers than any other country—around 45 percent more than the United States, its closest competitor. It has also overtaken the United States in publishing high-quality AI research, accounting for nearly 30 percent of citations in AI journals globally in 2021, compared with 15 percent for the United States. This year, China is projected to overtake the United States in its share of the top one percent of the world’s most-cited AI papers. As the U.S. National Security Commission on Artificial Intelligence warned, “China possesses the might, talent, and ambition to surpass the United States as the world’s leader in AI in the next decade if current trends do not change.” Theorists have long worried that AI competition might initiate a race to the bottom on safety. But in competitions between major powers, established incumbents and ambitious challengers usually have vastly different levels of risk aversion, with the latter often demonstrating far more appetite for risk in a quest to rebalance perceived asymmetries. Today’s AI sprint would not be the first time Beijing’s desire to hasten progress invited disaster. The Chinese leader Mao Zedong’s attempt to collectivize farms, melt down agricultural tools to feed industrial development, and turbocharge steel production in his so-called Great Leap Forward plunged China into the worst famine in human history, with an estimated 30 million people starving to death between 1959 and 1961. Later, China’s attempt to slam the brakes on population growth through its 1979 one-child policy—adjusted to a two-child policy only in 2015—led to widespread forced abortions and infanticide, a population imbalance of roughly 33 million more males than females, and a severe demographic aging crisis across the country.  Less encompassing acceleration efforts, such as China’s 1990s rush to cash in on the commercial satellite launch industry, also catalyzed tragedy when a rocket blasted into a Chinese town in 1996, killing an unknown number of victims. Today, according to investigative reporting by The Wall Street Journal, hydroelectric plants, social housing complexes, and schools built around the world as part of China’s ambitious Belt and Road Initiative are literally falling apart, imposing vast costs on already impoverished countries. China’s drive to outdo the United States in AI capabilities has not yet produced any crises. But history suggests that if one occurred, Beijing’s response would be calamitous. Authoritarian states routinely mismanage emergencies, turning accidents into full-blown tragedies. Averting the worst outcomes depends on recognizing anomalies early on, especially those that might suggest bad news. But autocracies struggle to do that. There is no reason to expect anything different as the perils linked to AI take shape. When worrisome developments arise in China, party officials are incentivized to suppress troubling information rather than risk their positions by reporting bad news to their superiors, beginning a vicious cycle that tends toward catastrophe. The famine caused by Mao’s Great Leap Forward is a case in point. If farm collectivization and melting down tools set the stage for a crisis, it was the official cover-up of the early signs of danger that ultimately snowballed a bad harvest to mass starvation. The same pattern recurs in contemporary China with unnerving frequency. Consider, for example, the layers of government obfuscation that led at least one million Chinese men, women, and children to contract HIV by selling blood or receiving transfusions of contaminated blood in the 1990s. Despite regular early reports of the budding disaster, local officials aggressively suppressed evidence for years to protect their careers. Many of them were promoted even after the suppression and its effects were known. The highly lethal 2002 SARS outbreak in China was likewise covered up by the Chinese government for about four months, even as the deadly virus infected more than 8,000 people around the world and killed 774. And despite investing $850 million in public health mechanisms specifically designed to ensure that a SARS-like cover-up did not happen again, the government followed a similar path in its response to the COVID-19 pandemic. Critical weeks elapsed between the first recorded COVID case in Wuhan in December 2019 and China’s acknowledgment of the risk of human-to-human transmission on January 20, 2020. By that time, seven million individuals had freely traveled from Wuhan to other locales, spreading the virus across the country and beyond. During and after the COVID-19 outbreak, the Chinese government harassed and detained doctors and journalists who tried to bring life-saving information about the virus to light. At the same time, it lied to the World Health Organization, causing the WHO to fatally misadvise the rest of the world on the risks of COVID in the early weeks of the pandemic. To this day, the Chinese government refuses to offer the transparency needed to determine the origins of the virus, which may well have been the result of yet another Chinese high-tech accident. Those skeptical of the risks of a Chinese AI catastrophe might point to the government’s comparative willingness to regulate AI, or the fact that China still lags behind the United States in building the most sophisticated and therefore risky capabilities. But as with the rules governing the Chinese Internet, much of Beijing’s AI legislation aims only to ensure that companies remain subservient to the government and able to compete with their Western counterparts. These laws are also designed to suppress any information that threatens the regime. And although it may be true that the United States leads in cutting-edge AI technology, it is also clear that clever tinkering with others’ systems can bring out some of their most dangerous and unanticipated capabilities, such as combining models for enhanced capacity to plan and execute chemical experiments. It is all too easy to steal, copy, or clone advanced AI models and tweak them, potentially stripping them of safety features. And if China’s technology sector is good at anything, it is quickly adapting others’ creations for maximum impact, a strategy that has been the engine of its growth for decades. The potential for AI tragedy is hardly unique to China. As with any powerful new technology, disasters could strike anywhere, and smaller-scale incidents from accidents and misuse are already occurring regularly around the world with self-driving car crashes, AI voice-cloning scams, and the misidentification of people by law enforcement agencies using facial recognition, to name a few. Given its leading role in pioneering new AI capabilities, the United States in particular confronts a high risk of costly failures. American companies and the government would be wise to bolster their own AI safety efforts, as many lawmakers are now realizing. But from Chernobyl to COVID, history shows that the most acute risks of catastrophe stem from authoritarian states, which are far more prone to systemic missteps that exacerbate an initial mistake or accident. China’s blithe attitude toward technological risk, the government’s reckless ambition, and Beijing’s crisis mismanagement are all on a collision course with the escalating dangers of AI. A variety of U.S. policy measures could help mitigate these risks. Industry and government could double down on restricting the commercial flow of easily weaponized AI research to China in recognition of the serious threat it poses, including through technology transfer tactics that leverage joint ventures and Chinese investments. American diplomacy could champion the establishment of global AI safety standards. The United States, in coordination with the international community, could also monitor potential safety concerns in advanced AI labs around the world with an eye toward crafting contingency plans in the case of failures with spillover effects. There is precedent for doing so: the United States has been known to monitor the risks of accidents from dangerous behavior in Chinese biolabs, nuclear reactors, and space operations. But the first step is rightly prioritizing the threat. As in the Cold War, weapons races and technological competition may attract a great deal of attention, but safety risks are equally worthy of concern, especially in authoritarian states. And to avoid another Chernobyl-like calamity, addressing China’s risks of an AI catastrophe should be at the top of the agenda. Washington Needs an Endgame in Ukraine Why the Financial Crisis Took Economists By Surprise Iran’s Proxies Have Seized Power in Baghdad—and Are Gutting the State The War in Ukraine Has Become a Battle for the Russian Psyche How Beijing Threatens U.S. Dominance How AI Distorts Decision-Making and Makes Dictators More Dangerous Get the Magazine From the publishers of Foreign Affairs Published by the Council on Foreign Relations ©2023 Council on Foreign Relations, Inc. All Rights Reserved."
86,https://www.nytimes.com/2023/05/25/technology/ai-hiring-law-new-york.html,"New York City’s pioneering, focused approach sets rules on how companies use the technology in work force decisions. “Until you try to regulate, you won’t learn how,” said Julia Stoyanovich, director of the Center for Responsible A.I. at New York University.Credit...Mary Inhea Kang for The New York Times Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Steve Lohr Steve Lohr, a technology reporter, has reported on artificial intelligence for more than two decades. European lawmakers are finishing work on an A.I. act. The Biden administration and leaders in Congress have their plans for reining in artificial intelligence. Sam Altman, the chief executive of OpenAI, maker of the A.I. sensation ChatGPT, recommended the creation of a federal agency with oversight and licensing authority in Senate testimony last week. And the topic came up at the Group of 7 summit in Japan. Amid the sweeping plans and pledges, New York City has emerged as a modest pioneer in A.I. regulation. The city government passed a law in 2021 and adopted specific rules last month for one high-stakes application of the technology: hiring and promotion decisions. Enforcement begins in July. The city’s law requires companies using A.I. software in hiring to notify candidates that an automated system is being used. It also requires companies to have independent auditors check the technology annually for bias. Candidates can request and be told what data is being collected and analyzed. Companies will be fined for violations. New York City’s focused approach represents an important front in A.I. regulation. At some point, the broad-stroke principles developed by governments and international organizations, experts say, must be translated into details and definitions. Who is being affected by the technology? What are the benefits and harms? Who can intervene, and how? “Without a concrete use case, you are not in a position to answer those questions,” said Julia Stoyanovich, an associate professor at New York University and director of its Center for Responsible A.I. But even before it takes effect, the New York City law has been a magnet for criticism. Public interest advocates say it doesn’t go far enough, while business groups say it is impractical. The complaints from both camps point to the challenge of regulating A.I., which is advancing at a torrid pace with unknown consequences, stirring enthusiasm and anxiety. Uneasy compromises are inevitable. Ms. Stoyanovich is concerned that the city law has loopholes that may weaken it. “But it’s much better than not having a law,” she said. “And until you try to regulate, you won’t learn how.” The law applies to companies with workers in New York City, but labor experts expect it to influence practices nationally. At least four states — California, New Jersey, New York and Vermont — and the District of Columbia are also working on laws to regulate A.I. in hiring. And Illinois and Maryland have enacted laws limiting the use of specific A.I. technologies, often for workplace surveillance and the screening of job candidates. The New York City law emerged from a clash of sharply conflicting viewpoints. The City Council passed it during the final days of the administration of Mayor Bill de Blasio. Rounds of hearings and public comments, more than 100,000 words, came later — overseen by the city’s Department of Consumer and Worker Protection, the rule-making agency. The result, some critics say, is overly sympathetic to business interests. “What could have been a landmark law was watered down to lose effectiveness,” said Alexandra Givens, president of the Center for Democracy & Technology, a policy and civil rights organization. That’s because the law defines an “automated employment decision tool” as technology used “to substantially assist or replace discretionary decision making,” she said. The rules adopted by the city appear to interpret that phrasing narrowly so that A.I. software will require an audit only if it is the lone or primary factor in a hiring decision or is used to overrule a human, Ms. Givens said. That leaves out the main way the automated software is used, she said, with a hiring manager invariably making the final choice. The potential for A.I.-driven discrimination, she said, typically comes in screening hundreds or thousands of candidates down to a handful or in targeted online recruiting to generate a pool of candidates. Ms. Givens also criticized the law for limiting the kinds of groups measured for unfair treatment. It covers bias by sex, race and ethnicity, but not discrimination against older workers or those with disabilities. “My biggest concern is that this becomes the template nationally when we should be asking much more of our policymakers,” Ms. Givens said. The law was narrowed to sharpen it and make sure it was focused and enforceable, city officials said. The Council and the worker protection agency heard from many voices, including public-interest activists and software companies. Its goal was to weigh trade-offs between innovation and potential harm, officials said. “This is a significant regulatory success toward ensuring that A.I. technology is used ethically and responsibly,” said Robert Holden, who was the chair of the Council committee on technology when the law was passed and remains a committee member. New York City is trying to address new technology in the context of federal workplace laws with guidelines on hiring that date to the 1970s. The main Equal Employment Opportunity Commission rule states that no practice or method of selection used by employers should have a “disparate impact” on a legally protected group like women or minorities. Businesses have criticized the law. In a filing this year, the Software Alliance, a trade group that includes Microsoft, SAP and Workday, said the requirement for independent audits of A.I. was “not feasible” because “the auditing landscape is nascent,” lacking standards and professional oversight bodies. But a nascent field is a market opportunity. The A.I. audit business, experts say, is only going to grow. It is already attracting law firms, consultants and start-ups. Companies that sell A.I. software to assist in hiring and promotion decisions have generally come to embrace regulation. Some have already undergone outside audits. They see the requirement as a potential competitive advantage, providing proof that their technology expands the pool of job candidates for companies and increases opportunity for workers. “We believe we can meet the law and show what good A.I. looks like,” said Roy Wang, general counsel of Eightfold AI, a Silicon Valley start-up that produces software used to assist hiring managers. The New York City law also takes an approach to regulating A.I. that may become the norm. The law’s key measurement is an “impact ratio,” or a calculation of the effect of using the software on a protected group of job candidates. It does not delve into how an algorithm makes decisions, a concept known as “explainability.” In life-affecting applications like hiring, critics say, people have a right to an explanation of how a decision was made. But A.I. like ChatGPT-style software is becoming more complex, perhaps putting the goal of explainable A.I. out of reach, some experts say. “The focus becomes the output of the algorithm, not the working of the algorithm,” said Ashley Casovan, executive director of the Responsible AI Institute, which is developing certifications for the safe use of A.I. applications in the workplace, health care and finance. Steve Lohr covers technology, economics and work force issues. He was part of the team awarded the Pulitzer Prize for explanatory reporting in 2013. @SteveLohr Executives from leading A.I. companies, including OpenAI and Google, warned that the technology they were building might one day pose an existential threat to humanity. One of the most urgent warnings about the risks of A.I. has come from Geoffrey Hinton, whom many consider to be the godfather of artificial intelligence. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. The Age of A.I. In Silicon Valley’s hacker houses, young A.I. entrepreneurs are partying, innovating — and hoping not to get crushed by the big guys. A lawyer representing a man who sued an airline relied on ChatGPT to help prepare a court filing. It did not go well. How has the Shoggoth, an octopus-like creature from a science fiction story, come to symbolize the state of artificial intelligence? Kevin Roose explains. Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach."
87,https://www.europarl.europa.eu/news/en/press-room/20230505IPR84904/ai-act-a-step-closer-to-the-first-rules-on-artificial-intelligence,"To ensure a human-centric and ethical development of Artificial Intelligence (AI) in Europe, MEPs endorsed new transparency and risk-management rules for AI systems. On Thursday, the Internal Market Committee and the Civil Liberties Committee adopted a draft negotiating mandate on the first ever rules for Artificial Intelligence with 84 votes in favour, 7 against and 12 abstentions. In their amendments to the Commission’s proposal, MEPs aim to ensure that AI systems are overseen by people, are safe, transparent, traceable, non-discriminatory, and environmentally friendly. They also want to have a uniform definition for AI designed to be technology-neutral, so that it can apply to the AI systems of today and tomorrow. Risk based approach to AI - Prohibited AI practices The rules follow a risk-based approach and establish obligations for providers and users depending on the level of risk the AI can generate. AI systems with an unacceptable level of risk to people’s safety would be strictly prohibited, including systems that deploy subliminal or purposefully manipulative techniques, exploit people’s vulnerabilities or are used for social scoring (classifying people based on their social behaviour, socio-economic status, personal characteristics). MEPs substantially amended the list to include bans on intrusive and discriminatory uses of AI systems such as: MEPs expanded the classification of high-risk areas to include harm to people’s health, safety, fundamental rights or the environment. They also added AI systems to influence voters in political campaigns and in recommender systems used by social media platforms (with more than 45 million users under the Digital Services Act) to the high-risk list. General-purpose AI - transparency measures MEPs included obligations for providers of foundation models - a new and fast evolving development in the field of AI - who would have to guarantee robust protection of fundamental rights, health and safety and the environment, democracy and rule of law. They would need to assess and mitigate risks, comply with design, information and environmental requirements and register in the EU database. Generative foundation models, like GPT, would have to comply with additional transparency requirements, like disclosing that the content was generated by AI, designing the model to prevent it from generating illegal content and publishing summaries of copyrighted data used for training. Supporting innovation and protecting citizens' rights To boost AI innovation, MEPs added exemptions to these rules for research activities and AI components provided under open-source licenses. The new law promotes regulatory sandboxes, or controlled environments, established by public authorities to test AI before its deployment. MEPs want to boost citizens’ right to file complaints about AI systems and receive explanations of decisions based on high-risk AI systems that significantly impact their rights. MEPs also reformed the role of the EU AI Office, which would be tasked with monitoring how the AI rulebook is implemented. After the vote, co-rapporteur Brando Benifei (S&D, Italy) said: “We are on the verge of putting in place landmark legislation that must resist the challenge of time. It is crucial to build citizens’ trust in the development of AI, to set the European way for dealing with the extraordinary changes that are already happening, as well as to steer the political debate on AI at the global level. We are confident our text balances the protection of fundamental rights with the need to provide legal certainty to businesses and stimulate innovation in Europe”. Co-rapporteur Dragos Tudorache (Renew, Romania) said: “Given the profound transformative impact AI will have on our societies and economies, the AI Act is very likely the most important piece of legislation in this mandate. It’s the first piece of legislation of this kind worldwide, which means that the EU can lead the way in making AI human-centric, trustworthy and safe. We have worked to support AI innovation in Europe and to give start-ups, SMEs and industry space to grow and innovate, while protecting fundamental rights, strengthening democratic oversight and ensuring a mature system of AI governance and enforcement."" Before negotiations with the Council on the final form of the law can begin, this draft negotiating mandate needs to be endorsed by the whole Parliament, with the vote expected during the 12-15 June session."
